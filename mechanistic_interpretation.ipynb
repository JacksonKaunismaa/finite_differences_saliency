{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "import color_regions, network, visualizations, utils\n",
    "from color_regions import *\n",
    "from network import *\n",
    "from visualizations import *\n",
    "from utils import *\n",
    "from hooks import *\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcabd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up autoreloading of shared code\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport color_regions,network,visualizations,utils,hooks\n",
    "%aimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4eadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "     [transforms.ToTensor()])#,\n",
    "    #transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "batch_size = 512 # seems to be the fastest batch size\n",
    "train_indices = (0, 250_000) # size of training set\n",
    "valid_indices = (1_250_000, 1_270_000)\n",
    "test_indices = (2_260_000, 2_270_000)\n",
    "\n",
    "def color_classifier(color):  \n",
    "    if color <= 30:  # => 3 classes\n",
    "        return 0\n",
    "    if 30 < color <= 60:  # => 90/255 is 0, 90/255 is 1, 75/255 is 2\n",
    "        return 1\n",
    "    if 60 < color <= 90:\n",
    "        return 2\n",
    "    if 90 < color <= 120:\n",
    "        return 1\n",
    "    if 120 < color <= 150:\n",
    "        return 0\n",
    "    if 150 < color <= 180:\n",
    "        return 1\n",
    "    if 180 < color <= 210:\n",
    "        return 2\n",
    "    if 210 < color <= 240:\n",
    "        return 0\n",
    "    if 240 < color:\n",
    "        return 2\n",
    "critical_color_values = list(range(0,241,30))\n",
    "\n",
    "def set_loader_helper(indices):\n",
    "    data_set = ColorDatasetGenerator(color_classifier=color_classifier,\n",
    "                                    image_indices=indices,\n",
    "                                    transform=transform,\n",
    "                                    color_range=(5, 255),\n",
    "                                    noise_size=(1,9),\n",
    "                                    num_classes=3,\n",
    "                                    size=128,\n",
    "                                    radius=(128//6, 128//3))\n",
    "    loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size, \n",
    "                                          shuffle=True, num_workers=6, pin_memory=True)\n",
    "    return data_set, loader\n",
    "train_set, train_loader = set_loader_helper(train_indices)\n",
    "valid_set, valid_loader = set_loader_helper(valid_indices)\n",
    "test_set, test_loader = set_loader_helper(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bbf7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"hard\" task\n",
    "color_probe = np.linspace(0, 255, 255)\n",
    "color_class = [color_classifier(x) for x in color_probe]\n",
    "plt.plot(color_probe, color_class)\n",
    "plt.xlabel(\"Color\")\n",
    "plt.yticks([0, 1, 2])\n",
    "plt.ylabel(\"Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7aebda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_net = ResNet([[2, 3, 4],  # num_channels (input and output), kernel_size, stride\n",
    "                   #[4, 3, 2],\n",
    "                   [6, 3, 4]], 3, [128, 128, 1], \n",
    "                   \"tiny_net_noise_hard_grey.dict\", fc_layers=[]).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "tiny_optim = torch.optim.Adam(tiny_net.parameters())\n",
    "print(tiny_net.num_params())\n",
    "tiny_net.load_model_state_dict(optim=tiny_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba6c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train(tiny_net, tiny_optim, loss_func, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da112e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_net = AllActivations(tiny_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7210c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5_123_456)\n",
    "test_img, lbl, color, size, pos  = valid_set.generate_one()\n",
    "print(color)\n",
    "plt.imshow(test_img, cmap=\"gray\")\n",
    "tensor_test_img = tensorize(test_img, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9785a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_net.eval()\n",
    "interp_net(tensor_test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7d2fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_conv = interp_net._features[\"conv_layers1_0\"].detach().cpu().numpy().squeeze()\n",
    "first_conv_weights = dict(tiny_net.named_modules())[\"conv_layers1.0\"].weight.detach().cpu().numpy().squeeze()\n",
    "print(dict(tiny_net.named_modules())[\"conv_layers1.0\"].bias)\n",
    "fig = plt.figure(figsize=(4*2, 5*2))\n",
    "plt.subplot(3,2,1)\n",
    "imshow_centered_colorbar(test_img, \"bwr\", \"original_image\")\n",
    "plt.subplot(3,2,3)\n",
    "imshow_centered_colorbar(first_conv[0], \"bwr\", \"output conv1_0.0\")\n",
    "plt.subplot(3,2,4)\n",
    "imshow_centered_colorbar(first_conv[1], \"bwr\", \"output conv1_0.1\")\n",
    "plt.subplot(3,2,5)\n",
    "imshow_centered_colorbar(first_conv_weights[0], \"bwr\", \"weights of conv1_0.0\")\n",
    "plt.subplot(3,2,6)\n",
    "imshow_centered_colorbar(first_conv_weights[1], \"bwr\", \"weights of conv1_0.1\")\n",
    "# => first layer basically just computes a compressed version of original, twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cc9966",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn1_params.running_mean, bn1_params.running_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e446cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn1_params = dict(tiny_net.named_modules())[\"batch_norms1.0\"]\n",
    "print(bn1_params.weight, bn1_params.bias)\n",
    "first_batchnorms = interp_net._features[\"batch_norms1_0\"].detach().cpu().numpy().squeeze()\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "plt.subplot(2,2,1)\n",
    "imshow_centered_colorbar(first_conv[0], \"bwr\", \"output conv1_0.0\")\n",
    "plt.subplot(2,2,3)  # conv{1,2}_{layer_num}.{channel_index}\n",
    "imshow_centered_colorbar(first_conv[1], \"bwr\", \"output of conv1_0.1\")\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "imshow_centered_colorbar(first_batchnorms[0], \"bwr\", \"output batchnorm1_0.0\")\n",
    "plt.subplot(2,2,4)\n",
    "imshow_centered_colorbar(first_batchnorms[1], \"bwr\", \"output batchnorm1_0.1\")\n",
    "# conv of circle (which we just preserve its shape with our conv1) must exceed\n",
    "# the bias else it gets zero-ed out => gives us 1 boundary on the color. \n",
    "# eg. look at channel 1. we multiply the raw value by 7.5, and then subtract 270\n",
    "# (note that the bias on channel 1 is basically 0), and divide by 553\n",
    "# then multiply by 1, and subtract 0.8176 => any color value above -23 will be > 0\n",
    "# for channel 0, it turns out any color value above +25 will be > 0 => already\n",
    "# separating on that first non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8fa12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_conv = interp_net._features[\"conv_layers2_0\"].detach().cpu().numpy().squeeze()\n",
    "second_conv_weights = dict(tiny_net.named_modules())[\"conv_layers2.0\"].weight.detach().cpu().numpy().squeeze()\n",
    "fig = plt.figure(figsize=(6, 7))\n",
    "\n",
    "for m in range(2):\n",
    "    plt.subplot(3,2,m+1)\n",
    "    imshow_centered_colorbar(second_conv[m], \"bwr\", f\"out conv2_1.{m}\")\n",
    "    plt.subplot(3,2,m+3)\n",
    "    imshow_centered_colorbar(second_conv_weights[m][0], \"bwr\", f\"w 2_0.0->{m}\")\n",
    "    plt.subplot(3,2,m+5)\n",
    "    imshow_centered_colorbar(second_conv_weights[m][1], \"bwr\", f\"w 2_0.1->{m}\")\n",
    "# both paths have a \"just recompute/compress the image (identity mapping learned?)\", though\n",
    "# 1 shifts it up a bit (not sure how relevant this is, but you can actually see it in the image)\n",
    "# very curve detector-like filters as well in both paths\n",
    "# so channel 0 is upper-right curves, unsure what the bright pixel in lower left of w_2_0.0 is\n",
    "# but the other path doesn't have it, so maybe not important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb36757",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchnorms_2 = dict(tiny_net.named_modules())[\"batch_norms2.0\"]\n",
    "print(batchnorms_2.weight, batchnorms_2.bias)\n",
    "print(batchnorms_2.running_mean, batchnorms_2.running_var) \n",
    "# take a look at channel 0 (which separated at +25 before), the equation is now\n",
    "# ([(x*6.9-bn1.bias0)/bn1.var0*bn1.scale0+bn1.shift0]*4-1.2290)/sqrt(5.97)*0.7737-0.7180 +\n",
    "# ([(x*7.5-bn1.bias1)/bn1.var0*bn2.scale1+bn1.sfiht1]*1-1.1275)/sqrt(3.9812)*0.8932+1.1430 = 0\n",
    "# after rearranging, 0.028507*x-1.9267244 => has its 0 at 67, (so any color > 67)\n",
    "# will leave channel 0 here with activation > 0 (post-ReLU), which isn't particularly\n",
    "# close to any critical value, but I guess it just approximates the boundaries with a\n",
    "# bunch of piecewise linear functions like this, so you get the idea "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a128434",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_net.conv_blocks[block].conv2.weight[c, :].max(axis=-1).values.max(axis=-1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "block = 1\n",
    "\n",
    "#uniform_inpt = torch.full((1,16,32,32), 100.0).to(device)\n",
    "#plt.imshow(tiny_net.conv_blocks[0].conv2.weight[c, in_c].detach().cpu().numpy(), cmap=\"bwr\")\n",
    "conv_maps = tiny_net.conv_blocks[block].conv2.weight[c, :]\n",
    "#imshow_centered_colorbar(conv_maps[7].detach().cpu().numpy(), cmap=\"bwr\")\n",
    "conv_scale = conv_maps.max(axis=-1).values.max(axis=-1).values\n",
    "conv_shift = tiny_net.conv_blocks[block].conv2.bias[c]\n",
    "bn_scale = tiny_net.conv_blocks[block].batch_norm2.weight[c]\n",
    "bn_shift = tiny_net.conv_blocks[block].batch_norm2.bias[c]\n",
    "bn_var = tiny_net.conv_blocks[block].batch_norm2.running_var[c]\n",
    "bn_mean = tiny_net.conv_blocks[block].batch_norm2.running_mean[c]\n",
    "print(conv_shift, bn_scale, bn_shift, bn_var, bn_mean)\n",
    "#(c*conv_scale + conv_shift - bn_mean) / torch.sqrt(bn_var) * bn_scale + bn_shift\n",
    "slope = (conv_scale/torch.sqrt(bn_var)*bn_scale).detach().cpu().numpy()\n",
    "bias = ((conv_shift - bn_mean)/torch.sqrt(bn_var)*bn_scale + bn_shift).detach().cpu().numpy()\n",
    "\n",
    "lines = np.asarray([profile_plots[f\"conv_blocks.{block}.act_func1_{x}\"][0] for x in range(6)])\n",
    "\n",
    "uniform_scaling = slope.dot(lines) + bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09b7313",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.maximum(uniform_scaling, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4287d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_net.eval()\n",
    "profile_plots,_ = activation_color_profile(AllActivations(tiny_net), valid_loader, valid_set, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657cba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_profile_plot(profile_plots[\"conv_blocks.1.act_func2_4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff1606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets attempt to somewhat automate this process\n",
    "def fetch_layer_params(layer_idx, one_or_two):\n",
    "    conv_param = dict(tiny_net.named_modules())[f\"conv_layers{one_or_two}.{layer_idx}\"]\n",
    "    batchnorm_param = dict(tiny_net.named_modules())[f\"batch_norms{one_or_two}.{layer_idx}\"]\n",
    "    return conv_param, batchnorm_param\n",
    "\n",
    "def recurse_build_func():\n",
    "    conv, bn = fetch_layer_params(layer_idx, one_or_two)\n",
    "    for conv_map in conv.weight[channel]:\n",
    "        sorted_map = torch.sort(conv_map)\n",
    "        first_diff = sorted_map[0] - sorted_map[1]\n",
    "        if first_diff > 0.8:\n",
    "            last_diff = None\n",
    "            for i,j in zip(range(1,9), range(2,9)):\n",
    "                diff = sorted_map[i] - sorted_map[j]\n",
    "                if diff < first_diff and (last_diff is None or abs(diff - last_diff) < 0.1):\n",
    "                    last_diff = diff\n",
    "                else:\n",
    "                    break\n",
    "    else:  # TODO: finish this (done with color_profile_plots instead)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_weights(interp_net, \"conv_blocks.1.act_func1\", color_profile=profile_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95846d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_weights(interp_net, \"conv_blocks.1.act_func2\", color_profile=profile_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09141873",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(interp_net, \"conv_blocks.1.act_func2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c7b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_conv = interp_net._features[\"conv_layers1_1\"].detach().cpu().numpy().squeeze()\n",
    "second_conv_weights = dict(tiny_net.named_modules())[\"conv_layers1.1\"].weight.detach().cpu().numpy().squeeze()\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "\n",
    "for m in range(6):\n",
    "    plt.subplot(3,6,m+1)\n",
    "    imshow_centered_colorbar(second_conv[m], \"bwr\", f\"out conv1_1.{m}\")\n",
    "    plt.subplot(3,6,m+7)\n",
    "    imshow_centered_colorbar(second_conv_weights[m][0], \"bwr\", f\"w 1_1.0->{m}\")\n",
    "    plt.subplot(3,6,m+13)\n",
    "    imshow_centered_colorbar(second_conv_weights[m][1], \"bwr\", f\"w 1_1.1->{m}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cda2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn3_params = dict(tiny_net.named_modules())[\"batch_norms1.1\"]\n",
    "print(bn3_params.weight, bn3_params.bias)\n",
    "print(bn3_params.running_mean, bn3_params.running_var)\n",
    "third_batchnorms = interp_net._features[\"batch_norms1_1\"].detach().cpu().numpy().squeeze()\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "for m in range(6):\n",
    "    plt.subplot(2,6,m+1)\n",
    "    imshow_centered_colorbar(second_conv[m], \"bwr\", f\"output conv1_1.{m}\")\n",
    "    plt.subplot(2,6,m+7)\n",
    "    imshow_centered_colorbar(third_batchnorms[m], \"bwr\", f\"output batchnorm1_1.{m}\")\n",
    "# it appears the only important channel at this point is 2. channels 0,1 looks like it was\n",
    "# close to being important, but failed some color check. channel 5 I don't really\n",
    "# understand since it appears to have picked up some signal that wasnt there before?\n",
    "# (I suppose the mean is negative, and the scale is larger than 1 so it would expand any\n",
    "# slight differences that existed but weren't visible?). Channel 4 I think is also trying\n",
    "# to be a circle finder (upper right?), but failed color check as well. Channel 3 is also\n",
    "# looking like it just barely failed the color check. Actually, looking at channel 1 again, \n",
    "# its output after a ReLU I expect would look exactly like channel 4 right now, so\n",
    "# channel 4 is definitely a \"failed color check\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b0304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_conv = interp_net._features[\"conv_layers2_1\"].detach().cpu().numpy().squeeze()\n",
    "second_conv_weights = dict(tiny_net.named_modules())[\"conv_layers2.1\"].weight.detach().cpu().numpy().squeeze()\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "for m in range(6):\n",
    "    plt.subplot(7,6,m+1)\n",
    "    imshow_centered_colorbar(second_conv[m], \"bwr\", f\"out conv2_1.{m}\")\n",
    "    for k in range(6):\n",
    "        plt.subplot(7,6,m+1+(k+1)*6)\n",
    "        imshow_centered_colorbar(second_conv_weights[m][k], \"bwr\", f\"w 2_1.{k}->{m}\",\n",
    "                                colorbar=True)\n",
    "    # Channel 0 is basically saying \"cancel out everything except for channel 4 in prev layer\"\n",
    "    # So it should basically copy its value (which it does). Channel 2 is similar, though it \n",
    "    # appears to copy from channel 1, and 4 a bit. At the end of it, channel 4 ends\n",
    "    # up being the most active, since it has that strong positive edge detector with \n",
    "    # channel 2 in the previous layer. Channel 1 also does decently well, but its circle\n",
    "    # has been thoroughly zeroed out, and only an \"artifact-like\" row of brightness \n",
    "    # remains at the top edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e98608",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn4_params = dict(tiny_net.named_modules())[\"batch_norms2.1\"]\n",
    "print(bn4_params.weight, bn4_params.bias)\n",
    "print(bn4_params.running_mean, bn4_params.running_var)\n",
    "fourth_batchnorms = interp_net._features[\"batch_norms2_1\"].detach().cpu().numpy().squeeze()\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "for m in range(6):\n",
    "    plt.subplot(2,6,m+1)\n",
    "    imshow_centered_colorbar(second_conv[m], \"bwr\", f\"output conv2_1.{m}\")\n",
    "    plt.subplot(2,6,m+7)\n",
    "    imshow_centered_colorbar(fourth_batchnorms[m], \"bwr\", f\"output batchnorm2_1.{m}\")\n",
    "# so again, we see somewhat of a \"direction reversal\" in channel5 (pretty much because of \n",
    "# the positive bias (compared to the other biases, which are all negative), but channel 4 mostly\n",
    "# seems to be the winner here. The \"artifact\" bright top row of channel 1 is mostly negated, \n",
    "# (we actually see those weird rows in multiple conv maps here, could be an artifact\n",
    "# of the padding/striding method maybe?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d9ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_weights = dict(tiny_net.named_modules())[\"fully_connected.0\"].weight.detach().cpu().numpy().squeeze()\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "\n",
    "for m in range(6):\n",
    "    plt.subplot(4,6,m+1)\n",
    "    imshow_centered_colorbar(fourth_batchnorms[m], \"bwr\", f\"out batchnorm2_1.{m}\") # no ReLU\n",
    "    for fc_m in range(3):\n",
    "        fc_shaped = fc_weights[fc_m].reshape(6,8,8)[m]\n",
    "        result = (np.where(fourth_batchnorms[m]>0, fourth_batchnorms[m], 0)*fc_shaped).sum()\n",
    "        plt.subplot(4,6,m+1+(fc_m+1)*6)\n",
    "        imshow_centered_colorbar(fc_shaped, \"bwr\", f\"{result:.2f}\")\n",
    "    # the stupid edge lines actually seem to be getting used somehow (see bottom row, which\n",
    "    # is used to predict class 2). Some of these maps are just \"find a circle-ish thing in \n",
    "    # the center\". Probably makes sense that the \"best\" place to put your circle checker is \n",
    "    # right in the middle, because most circles are at least overlapping the middle, due\n",
    "    # to the data generation process. Some of these maps appear to do nothing, eg.\n",
    "    # the map for predicting class 1 ignores channel 4. Although maybe there is some\n",
    "    # \"antipodal\" symmetry between class 1 channel 4 and class 0 channel 4 => channel 4 \n",
    "    # gives a lot of info for class 1??, though im not sure why you would only highlight\n",
    "    # one pixel inside them (we see the same pattern used in channel 2, and actually in a lot of\n",
    "    # the channels) => channel 0 is like \"positive evidence for class 1, negative evidence for\n",
    "    # class 0\"\n",
    "    \n",
    "    # note that its actually the same classes that are in superpositon:\n",
    "    # eg. for class 0,1 we have superposition in channel 0, channel 4\n",
    "    #     for class 0,2 we have superpositon in channel 1,2,3,5\n",
    "    \n",
    "    # also we arguably have a \"1-map\" type thing occuring in many of the channels. For example,\n",
    "    # in channel 4, it sort of looks like that for class 0 and class 2, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32957346",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_fc_conv(interp_net, color_profile=profile_plots, fixed_height=True, full_gridspec=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc0b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_fc_conv(interp_net, color_profile=profile_plots, fixed_height=False, full_gridspec=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e26be1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_net.fully_connected[0].fully_connected.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f54cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlab.init_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa020739",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlab.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a599b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlab_fc_conv_feature_angles(tiny_net, \n",
    "                            \"fully_connected.0.act_func\", num_embed=3, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab19196",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "%matplotlib notebook\n",
    "feature_gram, projected_weights = visualizations.fc_conv_feature_angles(tiny_net, \n",
    "                            \"fully_connected.0.act_func\", num_embed=3, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e760669",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlab_fc_conv_feature_angles(tiny_net, \"fully_connected.0.act_func\", num_embed=3, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1053e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_gram # normalized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f38c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = mlab.test_contour3d()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab634599",
   "metadata": {},
   "outputs": [],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4862b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_gram  # unnormalized version\n",
    "# in terms of visualization, should compress the number down to d=n/2\n",
    "# d = num dimensions to embed into, n = num features or classes\n",
    "# since num_equations is num in strict upper right triangle of gram matrix\n",
    "# ie. (n-1)(n-2)/2\n",
    "# and num unknowns is (d-1)*(n-1), since d-1 angles to choose per point, and get to pick\n",
    "# angles for n-1 points (the system is rotation invariant so position of first one doesn't add\n",
    "# any dof)\n",
    "# thus d-1=(n-2)/2 => d = n/2 => 2 should be fine for this case????????\n",
    "# but this feels wrong because first dot limits us to 2 locations for second feature\n",
    "# and 2 locations for 3rd feature, but 2nd feature 3rd feature dot is unlikely\n",
    "# to overlap with these 2 locations for the 2????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5c8f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_fc(interp_net, \"fully_connected.0.act_func\", size_mul=(8,25), color_profile=profile_plots)\n",
    "# this was subsampled by taking (arbitrarily) the first 32 weights of each 384 weight vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a12f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(interp_net, \"conv_blocks.1.act_func2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
