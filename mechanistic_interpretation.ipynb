{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from color_regions import ColorDatasetGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f19fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "prev_time = 0\n",
    "gamma = 0.99\n",
    "stats = {}  # tracks ewma running average\n",
    "def benchmark(point=None, profile=True, verbose=True, cuda=True): # not thread safe at all\n",
    "    global prev_time\n",
    "    if not profile:\n",
    "        return\n",
    "    if cuda:\n",
    "        torch.cuda.synchronize()\n",
    "    time_now = time.perf_counter()\n",
    "    if point is not None:\n",
    "        point = f\"{sys._getframe().f_back.f_code.co_name}-{point}\"\n",
    "        time_taken = time_now - prev_time\n",
    "        if point not in stats:\n",
    "            stats[point] = time_taken\n",
    "        stats[point] = stats[point]*gamma + time_taken*(1-gamma)\n",
    "        if verbose:\n",
    "            print(f\"took {time_taken} to reach {point}, ewma={stats[point]}\")\n",
    "    prev_time = time_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4eadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "     [transforms.ToTensor()])#,\n",
    "    #transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "batch_size = 512 # seems to be the fastest batch size\n",
    "train_indices = (0, 250_000) # size of training set\n",
    "valid_indices = (1_250_000, 1_270_000)\n",
    "test_indices = (2_260_000, 2_270_000)\n",
    "\n",
    "def color_classifier(color):  \n",
    "    if color <= 30:  # => 3 classes\n",
    "        return 0\n",
    "    if 30 < color <= 60:  # => 90/255 is 0, 90/255 is 1, 75/255 is 2\n",
    "        return 1\n",
    "    if 60 < color <= 90:\n",
    "        return 2\n",
    "    if 90 < color <= 120:\n",
    "        return 1\n",
    "    if 120 < color <= 150:\n",
    "        return 0\n",
    "    if 150 < color <= 180:\n",
    "        return 1\n",
    "    if 180 < color <= 210:\n",
    "        return 2\n",
    "    if 210 < color <= 240:\n",
    "        return 0\n",
    "    if 240 < color:\n",
    "        return 2\n",
    "critical_color_values = list(range(0,241,30))\n",
    "\n",
    "def set_loader_helper(indices):\n",
    "    data_set = ColorDatasetGenerator(color_classifier=color_classifier,\n",
    "                                    image_indices=indices,\n",
    "                                    transform=transform,\n",
    "                                    color_range=(5, 255),\n",
    "                                    noise_size=(1,9),\n",
    "                                    num_classes=3,\n",
    "                                    size=128,\n",
    "                                    radius=(128//6, 128//3))\n",
    "    loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size, \n",
    "                                          shuffle=True, num_workers=6, pin_memory=True)\n",
    "    return data_set, loader\n",
    "train_set, train_loader = set_loader_helper(train_indices)\n",
    "valid_set, valid_loader = set_loader_helper(valid_indices)\n",
    "test_set, test_loader = set_loader_helper(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bbf7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"hard\" task\n",
    "color_probe = np.linspace(0, 255, 255)\n",
    "color_class = [color_classifier(x) for x in color_probe]\n",
    "plt.plot(color_probe, color_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bd6ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, conv_layers, num_classes, img_shape, path, fc_layers=[1000], groups=1):\n",
    "        super().__init__()\n",
    "        self.conv_layers1 = []  # entry into residual block \n",
    "        self.conv_layers2 = []  # https://arxiv.org/pdf/1512.03385.pdf Figure 3\n",
    "        self.batch_norms1 = []\n",
    "        self.batch_norms2 = []\n",
    "        self.is_resid = []\n",
    "        self.path = path\n",
    "        self.num_classes = num_classes\n",
    "        channels = img_shape[-1]\n",
    "        img_size = img_shape[0]\n",
    "        for l in conv_layers:  # (out_channels, kernel_size, stride) is each l\n",
    "            if l[2] > 1: # stride\n",
    "                pad_type = \"valid\"\n",
    "                img_size = (img_size-l[1])//l[2] + 1 # https://arxiv.org/pdf/1603.07285.pdf\n",
    "            else:\n",
    "                pad_type = \"same\"\n",
    "            if isinstance(l[0], float):\n",
    "                l[0] = int(l[0])\n",
    "                l[0] -= l[0] % groups # ensure divisble by groups\n",
    "            self.is_resid.append(l[2] == 1 and channels == l[0])\n",
    "            self.conv_layers1.append(nn.Conv2d(channels, l[0], l[1], stride=l[2], padding=pad_type, groups=groups))\n",
    "            channels = l[0]\n",
    "            self.final_num_logits = channels * img_size * img_size \n",
    "            self.batch_norms1.append(nn.BatchNorm2d(channels)) # cant use track_running_stats=False since\n",
    "            self.batch_norms2.append(nn.BatchNorm2d(channels)) # it causes poor performance for inference with batch size=1 (or probably with the same image repeated a bunch of times)\n",
    "            self.conv_layers2.append(nn.Conv2d(channels, channels, l[1], stride=1, padding=\"same\", groups=groups))\n",
    "        self.conv_layers1 = nn.ModuleList(self.conv_layers1)\n",
    "        self.conv_layers2 = nn.ModuleList(self.conv_layers2)\n",
    "        self.batch_norms1 = nn.ModuleList(self.batch_norms1)\n",
    "        self.batch_norms2 = nn.ModuleList(self.batch_norms2)\n",
    "\n",
    "        fully_connected = []\n",
    "        fc_layers.insert(0, self.final_num_logits)\n",
    "        fc_layers.append(num_classes)\n",
    "        for fc_prev, fc_next in zip(fc_layers, fc_layers[1:]):\n",
    "            fully_connected.append(nn.Linear(fc_prev, fc_next))\n",
    "        self.fully_connected = nn.ModuleList(fully_connected)\n",
    "\n",
    "    def forward(self, x, logits=False):\n",
    "        network_iter = zip(self.conv_layers1, self.conv_layers2, self.batch_norms1, self.batch_norms2, self.is_resid)\n",
    "        for i, (conv1, conv2, batch_norm1, batch_norm2, is_resid) in enumerate(network_iter):                \n",
    "            x_conv1 = F.relu(batch_norm1(conv1(x)))\n",
    "            x_conv2 = F.relu(batch_norm2(conv2(x_conv1)))\n",
    "            if is_resid:\n",
    "                x = x + x_conv2  # residual block\n",
    "            else:\n",
    "                x = x_conv2  # dimension increasing block            \n",
    "        x = torch.flatten(x, 1)\n",
    "        for i, fc_layer in enumerate(self.fully_connected):\n",
    "            x = fc_layer(x)\n",
    "            if i != len(self.fully_connected) - 1: # dont ReLU the last one\n",
    "                x = F.relu(x)            \n",
    "        if self.num_classes == 1 and not logits:  # always allow returning logits\n",
    "            x = torch.sigmoid(x)\n",
    "        return x    \n",
    "\n",
    "    def num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    def save_model_state_dict(self, path=None, optim=None):\n",
    "        if path is None:\n",
    "            path = self.path\n",
    "        if optim is not None:\n",
    "            save_dict = {}\n",
    "            save_dict[\"model\"] = self.state_dict()\n",
    "            save_dict[\"optim\"] = optim.state_dict()\n",
    "        else:\n",
    "            save_dict = self.state_dict()\n",
    "        torch.save(save_dict, path)\n",
    "    \n",
    "    def load_model_state_dict(self, path=None, optim=None):\n",
    "        if path is None:\n",
    "            path = self.path\n",
    "        if not os.path.exists(path):\n",
    "            return\n",
    "        load_dict = torch.load(path)\n",
    "        if \"model\" in load_dict:\n",
    "            if optim is not None:\n",
    "                optim.load_state_dict(load_dict[\"optim\"]) \n",
    "            self.load_state_dict(load_dict[\"model\"])\n",
    "        else:\n",
    "            self.load_state_dict(load_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a139b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HookAdder(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.handles = []\n",
    "        self.hooks_exist = False\n",
    "        \n",
    "    def setup_hooks(self):\n",
    "        if self.hooks_exist:\n",
    "            return\n",
    "        def recurse_modules(mod, curr_name=\"\"):\n",
    "            has_sub_mods = False\n",
    "            for name, sub_mod in mod.named_children():\n",
    "                recurse_modules(sub_mod, f\"{curr_name}{name}.\")\n",
    "                has_sub_mods = True\n",
    "            if not has_sub_mods: # if no submodules, it is an actual operation\n",
    "                for hook_type, hook_func in zip(self.hook_types, self.hook_funcs):\n",
    "                    mod_hook = getattr(mod, hook_type)\n",
    "                    generated_hook = getattr(self, hook_func)(curr_name[:-1])\n",
    "                    self.handles.append(mod_hook(generated_hook))\n",
    "        recurse_modules(self.model)\n",
    "        self.hooks_exist = True\n",
    "    \n",
    "    def clean_up(self):\n",
    "        for handle in self.handles:\n",
    "            handle.remove()\n",
    "        del handles\n",
    "        self.hooks_exist = False\n",
    "\n",
    "    def forward(self, *args, preserve_hooks=False):\n",
    "        self.setup_hooks()\n",
    "        result = self.model(*args)\n",
    "        if not preserve_hooks:\n",
    "            self.clean_up()\n",
    "        return result\n",
    "\n",
    "class ProfileExecution(HookAdder):\n",
    "    def __init__(self, model):\n",
    "        self.hook_types = [\"register_forward_hook\"]\n",
    "        self.hook_funcs = [\"benchmark_hook\"]\n",
    "        super().__init__(model)\n",
    "    \n",
    "    def benchmark_hook(self, name):\n",
    "        def fn(layer, inpt, outpt):\n",
    "            benchmark(name, verbose=False)\n",
    "        return fn\n",
    "\n",
    "class AllActivations(HookAdder):\n",
    "    def __init__(self, model):\n",
    "        self._features = {}\n",
    "        self.hook_types = [\"register_forward_hook\"]\n",
    "        self.hook_funcs = [\"save_activations_hook\"]\n",
    "        super().__init__(model)\n",
    "            \n",
    "    def save_activations_hook(self, name):\n",
    "        def fn(layer, inpt, output):\n",
    "            self._features[name] = output\n",
    "        return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067e476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(pred_logits, labels):\n",
    "    if labels.shape[1] != 1:\n",
    "        pred_probabilities = F.softmax(pred_logits, dim=1)\n",
    "        classifications = torch.argmax(pred_probabilities, dim=1)\n",
    "        labels_argmax = torch.argmax(labels, dim=1)\n",
    "    else:\n",
    "        classifications = pred_logits.int()\n",
    "        labels_argmax = labels\n",
    "    correct = (labels_argmax == classifications)\n",
    "    return correct\n",
    "\n",
    "def train(net, optimizer, loss, epochs):\n",
    "    va_losses = []\n",
    "    tr_losses = []\n",
    "    va_accuracies = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_tr_loss = 0.0\n",
    "        net.train()\n",
    "        for i, sample in tqdm(enumerate(train_loader)):\n",
    "            imgs = sample[\"image\"].to(device, non_blocking=False).float()\n",
    "            labels = sample[\"label\"].to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(imgs)\n",
    "            batch_loss = loss(outputs, labels)\n",
    "            epoch_tr_loss += batch_loss.item()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_va_loss = 0.0\n",
    "        epoch_va_correct = 0\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, sample in enumerate(valid_loader):\n",
    "                imgs = sample[\"image\"].to(device).float()\n",
    "                labels = sample[\"label\"].to(device).float()\n",
    "                outputs = net(imgs)\n",
    "                epoch_va_loss += loss(outputs, labels).item()\n",
    "                epoch_va_correct += correct(outputs, labels).sum().item()\n",
    "        epoch_va_accuracy = epoch_va_correct/(valid_indices[1] - valid_indices[0])\n",
    "        print(f'Epoch {epoch + 1}: va_loss: {epoch_va_loss}, va_accuracy: {epoch_va_accuracy}, tr_loss: {epoch_tr_loss}')\n",
    "        if not va_losses or epoch_va_loss < min(va_losses):\n",
    "            net.save_model_state_dict(optim=optimizer)\n",
    "        va_losses.append(epoch_va_loss)\n",
    "        tr_losses.append(epoch_tr_loss)\n",
    "        va_accuracies.append(epoch_va_accuracy)\n",
    "    return va_losses, va_accuracies, tr_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7aebda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_net = ResNet([[2, 3, 4],  # num_channels (input and output), kernel_size, stride\n",
    "                   #[4, 3, 2],\n",
    "                   [6, 3, 4]], 3, [128, 128, 1], \n",
    "                   \"tiny_net_noise_hard_grey.dict\", fc_layers=[]).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "tiny_optim = torch.optim.Adam(tiny_net.parameters())\n",
    "print(tiny_net.num_params())\n",
    "tiny_net.load_model_state_dict(optim=tiny_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba6c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train(tiny_net, tiny_optim, loss_func, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da112e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_net = AllActivations(tiny_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7210c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5_123_456)\n",
    "test_img, lbl, color, size, pos  = valid_set.generate_one()\n",
    "print(color)\n",
    "plt.imshow(test_img, cmap=\"gray\")\n",
    "tensor_test_img = torch.tensor(test_img.transpose(2,0,1)).to(device).unsqueeze(0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9785a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_net.eval()\n",
    "interp_net(tensor_test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7323d1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_centered_colorbar(img, cmap, title, colorbar=True):\n",
    "    heat_max = np.max(abs(img))\n",
    "    plt.imshow(img, cmap=cmap, vmin=-heat_max, vmax=heat_max)\n",
    "    if colorbar:\n",
    "        plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7d2fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_conv = interp_net._features[\"conv_layers1_0\"].detach().cpu().numpy().squeeze()\n",
    "first_conv_weights = dict(tiny_net.named_modules())[\"conv_layers1.0\"].weight.detach().cpu().numpy().squeeze()\n",
    "print(dict(tiny_net.named_modules())[\"conv_layers1.0\"].bias)\n",
    "fig = plt.figure(figsize=(4*2, 5*2))\n",
    "plt.subplot(3,2,1)\n",
    "imshow_centered_colorbar(test_img, \"bwr\", \"original_image\")\n",
    "plt.subplot(3,2,3)\n",
    "imshow_centered_colorbar(first_conv[0], \"bwr\", \"output conv1_0.0\")\n",
    "plt.subplot(3,2,4)\n",
    "imshow_centered_colorbar(first_conv[1], \"bwr\", \"output conv1_0.1\")\n",
    "plt.subplot(3,2,5)\n",
    "imshow_centered_colorbar(first_conv_weights[0], \"bwr\", \"weights of conv1_0.0\")\n",
    "plt.subplot(3,2,6)\n",
    "imshow_centered_colorbar(first_conv_weights[1], \"bwr\", \"weights of conv1_0.1\")\n",
    "# => first layer basically just computes a compressed version of original, twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cc9966",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn1_params.running_mean, bn1_params.running_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e446cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn1_params = dict(tiny_net.named_modules())[\"batch_norms1.0\"]\n",
    "print(bn1_params.weight, bn1_params.bias)\n",
    "first_batchnorms = interp_net._features[\"batch_norms1_0\"].detach().cpu().numpy().squeeze()\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "plt.subplot(2,2,1)\n",
    "imshow_centered_colorbar(first_conv[0], \"bwr\", \"output conv1_0.0\")\n",
    "plt.subplot(2,2,3)  # conv{1,2}_{layer_num}.{channel_index}\n",
    "imshow_centered_colorbar(first_conv[1], \"bwr\", \"output of conv1_0.1\")\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "imshow_centered_colorbar(first_batchnorms[0], \"bwr\", \"output batchnorm1_0.0\")\n",
    "plt.subplot(2,2,4)\n",
    "imshow_centered_colorbar(first_batchnorms[1], \"bwr\", \"output batchnorm1_0.1\")\n",
    "# conv of circle (which we just preserve its shape with our conv1) must exceed\n",
    "# the bias else it gets zero-ed out => gives us 1 boundary on the color. \n",
    "# eg. look at channel 1. we multiply the raw value by 7.5, and then subtract 270\n",
    "# (note that the bias on channel 1 is basically 0), and divide by 553\n",
    "# then multiply by 1, and subtract 0.8176 => any color value above -23 will be > 0\n",
    "# for channel 0, it turns out any color value above +25 will be > 0 => already\n",
    "# separating on that first non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8fa12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_conv = interp_net._features[\"conv_layers2_0\"].detach().cpu().numpy().squeeze()\n",
    "second_conv_weights = dict(tiny_net.named_modules())[\"conv_layers2.0\"].weight.detach().cpu().numpy().squeeze()\n",
    "fig = plt.figure(figsize=(6, 7))\n",
    "\n",
    "for m in range(2):\n",
    "    plt.subplot(3,2,m+1)\n",
    "    imshow_centered_colorbar(second_conv[m], \"bwr\", f\"out conv2_1.{m}\")\n",
    "    plt.subplot(3,2,m+3)\n",
    "    imshow_centered_colorbar(second_conv_weights[m][0], \"bwr\", f\"w 2_0.0->{m}\")\n",
    "    plt.subplot(3,2,m+5)\n",
    "    imshow_centered_colorbar(second_conv_weights[m][1], \"bwr\", f\"w 2_0.1->{m}\")\n",
    "# both paths have a \"just recompute/compress the image (identity mapping learned?)\", though\n",
    "# 1 shifts it up a bit (not sure how relevant this is, but you can actually see it in the image)\n",
    "# very curve detector-like filters as well in both paths\n",
    "# so channel 0 is upper-right curves, unsure what the bright pixel in lower left of w_2_0.0 is\n",
    "# but the other path doesn't have it, so maybe not important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb36757",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchnorms_2 = dict(tiny_net.named_modules())[\"batch_norms2.0\"]\n",
    "print(batchnorms_2.weight, batchnorms_2.bias)\n",
    "print(batchnorms_2.running_mean, batchnorms_2.running_var) \n",
    "# take a look at channel 0 (which separated at +25 before), the equation is now\n",
    "# ([(x*6.9-bn1.bias0)/bn1.var0*bn1.scale0+bn1.shift0]*4-1.2290)/sqrt(5.97)*0.7737-0.7180 +\n",
    "# ([(x*7.5-bn1.bias1)/bn1.var0*bn2.scale1+bn1.sfiht1]*1-1.1275)/sqrt(3.9812)*0.8932+1.1430 = 0\n",
    "# after rearranging, 0.028507*x-1.9267244 => has its 0 at 67, (so any color > 67)\n",
    "# will leave channel 0 here with activation > 0 (post-ReLU), which isn't particularly\n",
    "# close to any critical value, but I guess it just approximates the boundaries with a\n",
    "# bunch of piecewise linear functions like this, so you get the idea "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aff1606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets attempt to somewhat automate this process\n",
    "def fetch_layer_params(layer_idx, one_or_two):\n",
    "    conv_param = dict(tiny_net.named_modules())[f\"conv_layers{one_or_two}.{layer_idx}\"]\n",
    "    batchnorm_param = dict(tiny_net.named_modules())[f\"batch_norms{one_or_two}.{layer_idx}\"]\n",
    "    return conv_param, batchnorm_param\n",
    "\n",
    "def recurse_build_func():\n",
    "    conv, bn = fetch_layer_params(layer_idx, one_or_two)\n",
    "    for conv_map in conv.weight[channel]:\n",
    "        sorted_map = torch.sort(conv_map)\n",
    "        first_diff = sorted_map[0] - sorted_map[1]\n",
    "        if first_diff > 0.8:\n",
    "            last_diff = None\n",
    "            for i,j in zip(range(1,9), range(2,9)):\n",
    "                diff = sorted_map[i] - sorted_map[j]\n",
    "                if diff < first_diff and (last_diff is None or abs(diff - last_diff) < 0.1):\n",
    "                    last_diff = diff\n",
    "                else:\n",
    "                    break\n",
    "    else:  # TODO: finish this\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c7b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_conv = interp_net._features[\"conv_layers1_1\"].detach().cpu().numpy().squeeze()\n",
    "second_conv_weights = dict(tiny_net.named_modules())[\"conv_layers1.1\"].weight.detach().cpu().numpy().squeeze()\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "\n",
    "for m in range(6):\n",
    "    plt.subplot(3,6,m+1)\n",
    "    imshow_centered_colorbar(second_conv[m], \"bwr\", f\"out conv1_1.{m}\")\n",
    "    plt.subplot(3,6,m+7)\n",
    "    imshow_centered_colorbar(second_conv_weights[m][0], \"bwr\", f\"w 1_1.0->{m}\")\n",
    "    plt.subplot(3,6,m+13)\n",
    "    imshow_centered_colorbar(second_conv_weights[m][1], \"bwr\", f\"w 1_1.1->{m}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cda2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn3_params = dict(tiny_net.named_modules())[\"batch_norms1.1\"]\n",
    "print(bn3_params.weight, bn3_params.bias)\n",
    "print(bn3_params.running_mean, bn3_params.running_var)\n",
    "third_batchnorms = interp_net._features[\"batch_norms1_1\"].detach().cpu().numpy().squeeze()\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "for m in range(6):\n",
    "    plt.subplot(2,6,m+1)\n",
    "    imshow_centered_colorbar(second_conv[m], \"bwr\", f\"output conv1_1.{m}\")\n",
    "    plt.subplot(2,6,m+7)\n",
    "    imshow_centered_colorbar(third_batchnorms[m], \"bwr\", f\"output batchnorm1_1.{m}\")\n",
    "# it appears the only important channel at this point is 2. channels 0,1 looks like it was\n",
    "# close to being important, but failed some color check. channel 5 I don't really\n",
    "# understand since it appears to have picked up some signal that wasnt there before?\n",
    "# (I suppose the mean is negative, and the scale is larger than 1 so it would expand any\n",
    "# slight differences that existed but weren't visible?). Channel 4 I think is also trying\n",
    "# to be a circle finder (upper right?), but failed color check as well. Channel 3 is also\n",
    "# looking like it just barely failed the color check. Actually, looking at channel 1 again, \n",
    "# its output after a ReLU I expect would look exactly like channel 4 right now, so\n",
    "# channel 4 is definitely a \"failed color check\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b0304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_conv = interp_net._features[\"conv_layers2_1\"].detach().cpu().numpy().squeeze()\n",
    "second_conv_weights = dict(tiny_net.named_modules())[\"conv_layers2.1\"].weight.detach().cpu().numpy().squeeze()\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "for m in range(6):\n",
    "    plt.subplot(7,6,m+1)\n",
    "    imshow_centered_colorbar(second_conv[m], \"bwr\", f\"out conv2_1.{m}\")\n",
    "    for k in range(6):\n",
    "        plt.subplot(7,6,m+1+(k+1)*6)\n",
    "        imshow_centered_colorbar(second_conv_weights[m][k], \"bwr\", f\"w 2_1.{k}->{m}\",\n",
    "                                colorbar=True)\n",
    "    # Channel 0 is basically saying \"cancel out everything except for channel 4 in prev layer\"\n",
    "    # So it should basically copy its value (which it does). Channel 2 is similar, though it \n",
    "    # appears to copy from channel 1, and 4 a bit. At the end of it, channel 4 ends\n",
    "    # up being the most active, since it has that strong positive edge detector with \n",
    "    # channel 2 in the previous layer. Channel 1 also does decently well, but its circle\n",
    "    # has been thoroughly zeroed out, and only an \"artifact-like\" row of brightness \n",
    "    # remains at the top edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e98608",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn4_params = dict(tiny_net.named_modules())[\"batch_norms2.1\"]\n",
    "print(bn4_params.weight, bn4_params.bias)\n",
    "print(bn4_params.running_mean, bn4_params.running_var)\n",
    "fourth_batchnorms = interp_net._features[\"batch_norms2_1\"].detach().cpu().numpy().squeeze()\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "for m in range(6):\n",
    "    plt.subplot(2,6,m+1)\n",
    "    imshow_centered_colorbar(second_conv[m], \"bwr\", f\"output conv2_1.{m}\")\n",
    "    plt.subplot(2,6,m+7)\n",
    "    imshow_centered_colorbar(fourth_batchnorms[m], \"bwr\", f\"output batchnorm2_1.{m}\")\n",
    "# so again, we see somewhat of a \"direction reversal\" in channel5 (pretty much because of \n",
    "# the positive bias (compared to the other biases, which are all negative), but channel 4 mostly\n",
    "# seems to be the winner here. The \"artifact\" bright top row of channel 1 is mostly negated, \n",
    "# (we actually see those weird rows in multiple conv maps here, could be an artifact\n",
    "# of the padding/striding method maybe?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ba3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_maps = np.arange(64*6).reshape(6,8,8)\n",
    "#print(out_maps.flatten())\n",
    "flattened = out_maps.flatten()\n",
    "#print(flattened)\n",
    "flattened.reshape(6,8,8)  # => should just work if we reshape the fc layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d9ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_weights = dict(tiny_net.named_modules())[\"fully_connected.0\"].weight.detach().cpu().numpy().squeeze()\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "\n",
    "for m in range(6):\n",
    "    plt.subplot(4,6,m+1)\n",
    "    imshow_centered_colorbar(fourth_batchnorms[m], \"bwr\", f\"out batchnorm2_1.{m}\") # no ReLU\n",
    "    for fc_m in range(3):\n",
    "        fc_shaped = fc_weights[fc_m].reshape(6,8,8)[m]\n",
    "        result = (np.where(fourth_batchnorms[m]>0, fourth_batchnorms[m], 0)*fc_shaped).sum()\n",
    "        plt.subplot(4,6,m+1+(fc_m+1)*6)\n",
    "        imshow_centered_colorbar(fc_shaped, \"bwr\", f\"{result:.2f}\")\n",
    "    # the stupid edge lines actually seem to be getting used somehow (see bottom row, which\n",
    "    # is used to predict class 2). Some of these maps are just \"find a circle-ish thing in \n",
    "    # the center\". Probably makes sense that the \"best\" place to put your circle checker is \n",
    "    # right in the middle, because most circles are at least overlapping the middle, due\n",
    "    # to the data generation process. Some of these maps appear to do nothing, eg.\n",
    "    # the map for predicting class 1 ignores channel 4. Although maybe there is some\n",
    "    # \"antipodal\" symmetry between class 1 channel 4 and class 0 channel 4 => channel 4 \n",
    "    # gives a lot of info for class 1??, though im not sure why you would only highlight\n",
    "    # one pixel inside them (we see the same pattern used in channel 2, and actually in a lot of\n",
    "    # the channels) => channel 0 is like \"positive evidence for class 1, negative evidence for\n",
    "    # class 0\"\n",
    "    \n",
    "    # note that its actually the same classes that are in superpositon:\n",
    "    # eg. for class 0,1 we have superposition in channel 0, channel 4\n",
    "    #     for class 0,2 we have superpositon in channel 1,2,3,5\n",
    "    \n",
    "    # also we arguably have a \"1-map\" type thing occuring in many of the channels. For example,\n",
    "    # in channel 4, it sort of looks like that for class 0 and class 2, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc6ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_net._features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82dfbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def rate_distribution(net, loader, dataset, buckets=100):\n",
    "    net.eval()\n",
    "    total = np.zeros((buckets))\n",
    "    num_correct = np.zeros((buckets))\n",
    "    num_possible_colors = dataset.color_range[1] - dataset.color_range[0]\n",
    "    for sample in tqdm(loader):\n",
    "        imgs = sample[\"image\"].to(device).float()\n",
    "        labels = sample[\"label\"].to(device).float()\n",
    "        actual_colors = sample[\"color\"]\n",
    "        color_indices = (buckets * (actual_colors - dataset.color_range[0]) / num_possible_colors).int().numpy()\n",
    "        outputs = net(imgs)\n",
    "        correct_preds = correct(outputs, labels).cpu().numpy()\n",
    "        for i, color_idx in enumerate(color_indices):\n",
    "            total[color_idx] += 1  \n",
    "            num_correct[color_idx] += correct_preds[i]\n",
    "    return num_correct, total\n",
    "_num_correct, _total = rate_distribution(tiny_net, valid_loader, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d70fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(num_correct, total, dataset, critical_values=[150-0.5], buckets=100):\n",
    "    num_wrong = total - num_correct\n",
    "    width = 0.4\n",
    "    labels = [int(x) for i, x in enumerate(np.linspace(*dataset.color_range, buckets))]\n",
    "    plt.bar(labels, num_correct, width, label=\"correct amount\")\n",
    "    plt.bar(labels, num_wrong, width, bottom=num_correct, label=\"wrong amount\")\n",
    "    plt.vlines(critical_values, np.max(total), 0,\n",
    "               linewidth=0.8, colors=\"r\", label=\"decision boundary\",\n",
    "               linestyles=\"dashed\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Color value\")\n",
    "    plt.show()\n",
    "make_graph(_num_correct, _total, valid_set,   # with .eval(), looks good\n",
    "           critical_values=[x-0.5 for x in critical_color_values]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_net.eval() # very important!\n",
    "with torch.no_grad():\n",
    "    test_index = 987_652  # results seem pretty dependent on image, especially in small-color regimes\n",
    "    counterfactual_color_values = np.linspace(0, 255, 255) # probably because bad batchnorm estimates\n",
    "    responses = []\n",
    "    for color in counterfactual_color_values:\n",
    "        np.random.seed(test_index)\n",
    "        generated_img, lbl, *__ = valid_set.generate_one(set_color=color)\n",
    "        generated_img = np.expand_dims(generated_img, 0).transpose(0, 3, 1, 2)\n",
    "        generated_img = torch.tensor(generated_img).to(device).float()\n",
    "        response = tiny_net(torch.tensor(generated_img).to(device).float(), logits=True).cpu().numpy()\n",
    "        responses.append(np.squeeze(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e16a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_responses(resp, colors, title):\n",
    "    resp = np.arcsinh(np.array(resp))\n",
    "    for output_logit in range(resp.shape[1]):\n",
    "        plt.plot(colors, resp[:, output_logit], label=f\"class {output_logit}\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Color value\")\n",
    "    plt.ylabel(\"Network output logit\")\n",
    "    plt.title(title)\n",
    "    plt.vlines(critical_color_values, np.min(resp), np.max(resp), linewidth=0.8,\n",
    "               colors=\"r\", label=\"decision boundary\",\n",
    "               linestyles=\"dashed\") # with .eval() works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17f7269",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_responses(responses, counterfactual_color_values, \"\")\n",
    "#i ncreas esize of training set and roughness (mean squared grad across img or pixelvalues) should go down, meght be overfitting (double deep descent)\n",
    "#  -> could be causing decrease in quality of FD maps\n",
    "# to be expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def finite_differences(model, dataset, target_class, stacked_img, locations, channel, unfairness, values_prior):\n",
    "    num_iters = 20 # sample 20 values evenly spaced\n",
    "    cuda_stacked_img = torch.tensor(stacked_img).to(device)\n",
    "    if dataset.num_classes == 2:\n",
    "        class_multiplier = 1 if target_class == 1 else -1 \n",
    "        baseline_activations = class_multiplier*model(cuda_stacked_img, logits=True)\n",
    "    else:\n",
    "        baseline_activations = model(cuda_stacked_img)[:, target_class]\n",
    "    largest_slope = np.zeros(stacked_img.shape[0])  # directional finite difference?\n",
    "    slices = np.index_exp[np.arange(stacked_img.shape[0]), channel, locations[:, 0], locations[:, 1]]\n",
    "    if values_prior is None:\n",
    "        values_prior = np.linspace(5, 250, stacked_img.shape[0]) # uniform distribution assumption\n",
    "    elif isinstance(values_prior, list):\n",
    "        values_prior = np.expand_dims(np.asarray(values_prior), 1)\n",
    "    num_loops = 1 if unfairness == \"very unfair\" else len(values_prior)\n",
    "    for i in range(num_loops):\n",
    "        shift_img = stacked_img.copy()\n",
    "        # shifting method\n",
    "        if unfairness in [\"fair\", \"unfair\"]:\n",
    "            shift_img[slices] = values_prior[i]+0.01  # add tiny offset to \"guarantee\" non-zero shift\n",
    "        elif unfairness in [\"very unfair\"]:\n",
    "            critical_value_dists = shift_img[slices] - values_prior\n",
    "            closest = np.argmin(abs(critical_value_dists), axis=0) # find closest class boundary\n",
    "            shift_img[slices] = 0.01 + np.choose(closest, values_prior) - 10*np.sign(np.choose(closest, critical_value_dists))\n",
    "        \n",
    "        actual_diffs = shift_img[slices] - stacked_img[slices]  \n",
    "        img_norm = torch.tensor(shift_img).to(device) # best is no normalization anyway\n",
    "        if dataset.num_classes == 2:\n",
    "            activations = class_multiplier*model(img_norm, logits=True)\n",
    "        else:\n",
    "            activations = model(img_norm)[:, target_class]\n",
    "        activation_diff = (activations - baseline_activations).cpu().numpy().squeeze()\n",
    "        finite_difference = np.clip(activation_diff/actual_diffs, -30, 30) # take absolute slope\n",
    "        largest_slope = np.where(abs(finite_difference) > abs(largest_slope), finite_difference, largest_slope)\n",
    "    return largest_slope      \n",
    "\n",
    "def finite_differences_map(model, dataset, target_class, img, unfairness=\"fair\", values_prior=None):\n",
    "    # generate a saliency map using finite differences method (iterate over colors)\n",
    "    model.eval()\n",
    "    batch_size = 32  # check batch_size num pixel positions in parallel\n",
    "    im_size = dataset.size\n",
    "    #img = img.astype(np.float32)/255. # normalization handled later\n",
    "    values_x = np.repeat(np.arange(im_size), im_size)\n",
    "    values_y = np.tile(np.arange(im_size), im_size)\n",
    "    indices = np.stack((values_x, values_y), axis=1)\n",
    "    stacked_img = np.repeat(np.expand_dims(img, 0), batch_size, axis=0)\n",
    "    stacked_img = np.transpose(stacked_img, (0, 3, 1, 2)).astype(np.float32) # NCHW format\n",
    "    img_heat_map = np.zeros_like(img)\n",
    "    for channel in range(dataset.channels):\n",
    "        for k in tqdm(range(0, im_size*im_size, batch_size)):\n",
    "            actual_batch_size = min(batch_size, im_size*im_size-k+batch_size)\n",
    "            locations = indices[k:k+batch_size]\n",
    "            largest_slopes = finite_differences(model, dataset, target_class, stacked_img, locations, channel, unfairness, values_prior)\n",
    "            img_heat_map[locations[:,0], locations[:,1], channel] = largest_slopes\n",
    "    return img_heat_map#.sum(axis=2)  # linear approximation aggregation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d84dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(500_011)\n",
    "explain_img, label, *_ = valid_set.generate_one()\n",
    "heat_map = finite_differences_map(tiny_net, valid_set, label.argmax(), explain_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d95ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_grid_figure(grid, titles=None, colorbar=True, cmap=None, transpose=False, hspace=-0.4):      \n",
    "    np_grid = np.array(grid).squeeze()\n",
    "    if len(np_grid.shape) != 4:\n",
    "        np_grid = np.expand_dims(np_grid, 0)\n",
    "    if transpose:\n",
    "        np_grid = np_grid.transpose(1,0,2,3)\n",
    "        \n",
    "    if cmap is None:\n",
    "        cmap = \"bwr\"\n",
    "    nrows, ncols = np_grid.shape[0], np_grid.shape[1]\n",
    "    im_size = np_grid.shape[2]\n",
    "    print(np_grid.shape, nrows, ncols)\n",
    "    fig = plt.figure(figsize=(4/128*im_size*ncols, 5/128*im_size*nrows))\n",
    "    gridspec = fig.add_gridspec(nrows, ncols, hspace=hspace)\n",
    "    axes = gridspec.subplots(sharex=\"col\", sharey=\"row\")\n",
    "    if len(axes.shape) == 1:\n",
    "        axes = np.expand_dims(axes, 0)\n",
    "    for i, row in enumerate(np_grid):\n",
    "        for j, img in enumerate(row):\n",
    "            if j == 0: # assume explain_img is the first thing\n",
    "                im = axes[i,j].imshow(img, cmap=\"gray\")\n",
    "            else:\n",
    "                img_max = np.max(abs(img))\n",
    "                if cmap != \"gray\":\n",
    "                    im = axes[i,j].imshow(img, cmap=cmap, interpolation=\"nearest\", vmax=img_max, vmin=-img_max)\n",
    "                else:\n",
    "                    axes[i,j].imshow(img, cmap=cmap)\n",
    "                if colorbar:\n",
    "                    plt.colorbar(im, pad=0, fraction=0.048)\n",
    "            if titles and i == 0:\n",
    "                axes[i,j].set_title(titles[j])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725c5459",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_grid_figure([explain_img, heat_map], titles=[\"Image\", \"FD Map\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9c62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = [20_000, 25_000, 30_000, 600_000, 600_001]\n",
    "heat_maps = []\n",
    "explain_imgs = []\n",
    "for i, image_id in enumerate(image_ids):\n",
    "    np.random.seed(image_id)\n",
    "    explain_img_i, target_i, *__ = valid_set.generate_one()\n",
    "    heat_map_i = finite_differences_map(tiny_net, valid_set, target_i.argmax(), explain_img_i)\n",
    "    heat_maps.append(heat_map_i)\n",
    "    explain_imgs.append(explain_img_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3081340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_grid_figure([explain_imgs, heat_maps], transpose=True, colorbar=True) \n",
    "# probably caused by stride of 4 in first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcda00b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_pixels_response(num_pixels, one_class=True):\n",
    "    tiny_net.eval() # very important!\n",
    "    test_index = 987_652  # results seem pretty dependent on image, especially in small-color regimes\n",
    "    np.random.seed(test_index)\n",
    "    generated_img, lbl, color, size, pos  = valid_set.generate_one()\n",
    "    generated_img = np.expand_dims(generated_img, 0).transpose(0, 3, 1, 2)\n",
    "    \n",
    "    np.random.seed(int(time.time()/np.pi))\n",
    "    selected_pixels = np.random.randint(0, valid_set.size, (num_pixels, 2))\n",
    "    num_inside = 0\n",
    "    for p in selected_pixels:\n",
    "        if np.linalg.norm(p-pos) < size:\n",
    "            num_inside += 1\n",
    "    print(f\"Percent of random inside circle: {num_inside/num_pixels*100.}\")\n",
    "    \n",
    "    counterfactual_color_values = np.linspace(0, 255, 255) # probably because bad batchnorm estimates\n",
    "    responses = []\n",
    "    with torch.no_grad():\n",
    "        for color in counterfactual_color_values:\n",
    "            generated_img[0, 0, selected_pixels[:,0], selected_pixels[:,1]] = color\n",
    "            tensor_img = torch.tensor(generated_img).to(device).float()\n",
    "            response = tiny_net(torch.tensor(generated_img).to(device).float(), logits=True).cpu().numpy()\n",
    "            if one_class:\n",
    "                responses.append(np.expand_dims(np.squeeze(response[:,lbl.argmax()]), 0))\n",
    "            else:\n",
    "                responses.append(np.squeeze(response))\n",
    "    plot_responses(responses, counterfactual_color_values, \"Randomly selected pixels\")\n",
    "    \n",
    "def circle_pixels_response(num_pixels, one_class=True):\n",
    "    tiny_net.eval() # very important!\n",
    "    test_index = 987_652 \n",
    "    np.random.seed(test_index)  # generate image\n",
    "    generated_img, lbl, color, size, pos = valid_set.generate_one()\n",
    "    generated_img = np.expand_dims(generated_img, 0).transpose(0, 3, 1, 2)\n",
    "    \n",
    "    np.random.seed(int(time.time()/np.pi))\n",
    "    angle = np.random.uniform(0,2*np.pi, num_pixels)\n",
    "    radii = np.random.uniform(0, size, num_pixels)\n",
    "    selected_pixels = np.zeros((num_pixels, 2))\n",
    "    selected_pixels[:,0] = pos[0][0] + np.cos(angle)*radii\n",
    "    selected_pixels[:,1] = pos[0][1] + np.sin(angle)*radii\n",
    "    selected_pixels = np.round(selected_pixels).astype(np.int64)\n",
    "    \n",
    "    counterfactual_color_values = np.linspace(0, 255, 255) # probably because bad batchnorm estimates\n",
    "    responses = []\n",
    "    with torch.no_grad():\n",
    "        for color in counterfactual_color_values:\n",
    "            generated_img[0, 0, selected_pixels[:,0], selected_pixels[:,1]] = color\n",
    "            tensor_img = torch.tensor(generated_img).to(device).float()\n",
    "            response = tiny_net(torch.tensor(generated_img).to(device).float(), logits=True).cpu().numpy()\n",
    "            if one_class:\n",
    "                responses.append(np.expand_dims(np.squeeze(response[:,lbl.argmax()]), 0))\n",
    "            else:\n",
    "                responses.append(np.squeeze(response))\n",
    "    plot_responses(responses, counterfactual_color_values, \"Pixels inside circle\")\n",
    "    \n",
    "def both_pixels_response(num_pixels, one_class):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    circle_pixels_response(num_pixels, one_class=one_class)\n",
    "    plt.subplot(1,2,2)\n",
    "    random_pixels_response(num_pixels, one_class=one_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4e5772",
   "metadata": {},
   "outputs": [],
   "source": [
    "both_pixels_response(100, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e7586d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38f8060b",
   "metadata": {},
   "source": [
    "What if we run the same experiment, but cheat with a prior on pixel values that we know *should* be informative to the output logit, namely values closest to the decision boundary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d72b360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, conv_layers, img_shape, path, fc_layers=[], embed_size=128):\n",
    "        super().__init__()\n",
    "        self.path = path  # for saving and loading\n",
    "        \n",
    "        enc_layers1 = []  \n",
    "        enc_layers2 = []\n",
    "        enc_maxpools = []\n",
    "        enc_batchnorms1 = []\n",
    "        enc_batchnorms2 = []\n",
    "        \n",
    "        dec_layers1 = []  \n",
    "        dec_layers2 = []\n",
    "        dec_maxpools = []  # should be maxunpools\n",
    "        dec_batchnorms1 = []\n",
    "        dec_batchnorms2 = []\n",
    "        \n",
    "        is_resid = []        \n",
    "        channels = img_shape[-1]\n",
    "        img_size = img_shape[0]\n",
    "        for l in conv_layers:  # (out_channels, kernel_size, stride) is each l\n",
    "            is_resid.append(l[2] == 1 and channels == l[0])\n",
    "            \n",
    "            if l[2] > 1:\n",
    "                img_size = (img_size-l[2])//l[2]+1\n",
    "                print(img_size)\n",
    "                enc_maxpools.append(nn.MaxPool2d(l[2], return_indices=True))\n",
    "                dec_maxpools.insert(0, nn.MaxUnpool2d(l[2]))\n",
    "            else:\n",
    "                enc_maxpools.append(None)\n",
    "                dec_maxpools.insert(0, None)\n",
    "                \n",
    "            enc_layers1.append(nn.Conv2d(channels, l[0], l[1], padding=\"same\"))\n",
    "            enc_batchnorms1.append(nn.BatchNorm2d(l[0])) \n",
    "\n",
    "            dec_layers2.insert(0,nn.ConvTranspose2d(l[0], channels, l[1], padding=l[1]//2))\n",
    "            dec_batchnorms2.insert(0,nn.BatchNorm2d(channels)) \n",
    "\n",
    "            channels = l[0]\n",
    "            \n",
    "            enc_layers2.append(nn.Conv2d(channels, channels, l[1], padding=\"same\"))\n",
    "            enc_batchnorms2.append(nn.BatchNorm2d(channels))\n",
    "\n",
    "            dec_layers1.insert(0,nn.ConvTranspose2d(channels, channels, l[1], padding=l[1]//2))\n",
    "            dec_batchnorms1.insert(0,nn.BatchNorm2d(channels)) \n",
    "            \n",
    "        self.final_flat_shape = channels*img_size*img_size\n",
    "        self.final_img_shape = [channels, img_size, img_size]\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        enc_fully_connected = []\n",
    "        dec_fully_connected = []\n",
    "        extended_fc_layers = fc_layers.copy()\n",
    "        extended_fc_layers.insert(0, self.final_flat_shape)\n",
    "        extended_fc_layers.append(embed_size)\n",
    "        for fc_prev, fc_next in zip(extended_fc_layers, extended_fc_layers[1:]):\n",
    "            enc_fully_connected.append(nn.Linear(fc_prev, fc_next))\n",
    "            dec_fully_connected.insert(0,nn.Linear(fc_next, fc_prev))\n",
    "            \n",
    "        self.enc_layers1 = nn.ModuleList(enc_layers1)\n",
    "        self.enc_layers2 = nn.ModuleList(enc_layers2)\n",
    "        self.enc_maxpools = nn.ModuleList(enc_maxpools)\n",
    "        self.enc_batchnorms1 = nn.ModuleList(enc_batchnorms1)\n",
    "        self.enc_batchnorms2 = nn.ModuleList(enc_batchnorms2)\n",
    "        self.enc_fully_connected = nn.ModuleList(enc_fully_connected)\n",
    "        \n",
    "        self.dec_layers1 = nn.ModuleList(dec_layers1)\n",
    "        self.dec_layers2 = nn.ModuleList(dec_layers2)\n",
    "        self.dec_maxpools = nn.ModuleList(dec_maxpools)\n",
    "        self.dec_batchnorms1 = nn.ModuleList(dec_batchnorms1)\n",
    "        self.dec_batchnorms2 = nn.ModuleList(dec_batchnorms2)\n",
    "        self.dec_fully_connected = nn.ModuleList(dec_fully_connected)\n",
    "\n",
    "        self.enc_is_resid = is_resid\n",
    "        self.dec_is_resid = reversed(is_resid)\n",
    "        \n",
    "        iter_names = [\"layers1\", \"layers2\", \"maxpools\", \"batchnorms1\", \"batchnorms2\", \"is_resid\"]\n",
    "        self.enc_iter = list(zip(*[getattr(self, \"enc_\"+name) for name in iter_names]))\n",
    "        self.dec_iter = list(zip(*[getattr(self, \"dec_\"+name) for name in iter_names]))\n",
    "        \n",
    "    def net_block(self, x, indices, block_name):\n",
    "        conv_iter = getattr(self, block_name + \"_iter\")\n",
    "        fc_iter = getattr(self, block_name + \"_fully_connected\")\n",
    "\n",
    "        if block_name == \"dec\":\n",
    "            for i, fc_layer in enumerate(fc_iter):\n",
    "                x = F.relu(fc_layer(x))\n",
    "            x = torch.reshape(x, (-1,*self.final_img_shape))\n",
    "#         print(x.shape, block_name, \"pre-conv\", type(conv_iter), next(conv_iter))\n",
    "        #print([t.shape if t is not None else t for t in indices])\n",
    "        for i, (conv1, conv2, maxpool, batch_norm1, batch_norm2, is_resid) in enumerate(conv_iter):\n",
    "            x_conv1 = F.relu(batch_norm1(conv1(x)))\n",
    "#             if block_name == \"dec\":\n",
    "#                 print(\"DEC\", i, type(indices[i]), maxpool)\n",
    "            if maxpool is not None:\n",
    "                if block_name == \"enc\":\n",
    "                    x_conv1, indices_layer = maxpool(x_conv1)\n",
    "                    #print(x.shape, conv1, indices_layer.shape)\n",
    "                    indices.insert(0,indices_layer)\n",
    "                else:\n",
    "                    #print(x.shape, maxpool, i, type(indices[i-1]), type(indices[i]), type(indices[i+1]))\n",
    "                    x_conv1 = maxpool(x_conv1, indices[i])\n",
    "            else:\n",
    "                if block_name == \"enc\":\n",
    "                    indices.insert(0,None)\n",
    "                    #print(x.shape, conv1)\n",
    "                    \n",
    "            if block_name == \"dec\" and i == len(conv_iter)-1:\n",
    "                x_conv2 = F.relu(conv2(x_conv1))\n",
    "            else:\n",
    "                x_conv2 = F.relu(batch_norm2(conv2(x_conv1)))\n",
    "                \n",
    "            if is_resid:\n",
    "                x = x + x_conv2  # residual block\n",
    "            else:\n",
    "                x = x_conv2  # dimension increasing block\n",
    "          #  print(x.shape, block_name, \"conv_mid\", i)\n",
    "#         print(x.shape, block_name, \"post_conv\")\n",
    "        if block_name == \"enc\":\n",
    "            x = torch.flatten(x, 1)\n",
    "            for i, fc_layer in enumerate(fc_iter):\n",
    "                x = fc_layer(x)\n",
    "                if i != len(fc_iter) - 1:\n",
    "                    x = F.relu(x)\n",
    "        return x    \n",
    "    \n",
    "    def encode(self, x, indices): # due to the symmetry of decoding/encoding, we can do this nicely\n",
    "        return self.net_block(x, indices, \"enc\")\n",
    "    \n",
    "    def decode(self, z, indices): # => this implementation means the output is ReLU'd\n",
    "        return self.net_block(z, indices, \"dec\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        save_indices = []\n",
    "        #print(save_indices, x.shape, \"pre anything\")\n",
    "        z = self.encode(x, save_indices)  # write to save_indices\n",
    "        return self.decode(z, save_indices) # read from save_indices\n",
    "    \n",
    "    def num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    def save_model_state_dict(self, path=None, optim=None):\n",
    "        if path is None:\n",
    "            path = self.path\n",
    "        if optim is not None:\n",
    "            save_dict = {}\n",
    "            save_dict[\"model\"] = self.state_dict()\n",
    "            save_dict[\"optim\"] = optim.state_dict()\n",
    "        else:\n",
    "            save_dict = self.state_dict()\n",
    "        torch.save(save_dict, path)\n",
    "    \n",
    "    def load_model_state_dict(self, path=None, optim=None):\n",
    "        if path is None:\n",
    "            path = self.path\n",
    "        if not os.path.exists(path):\n",
    "            return\n",
    "        load_dict = torch.load(path)\n",
    "        if \"model\" in load_dict:\n",
    "            if optim is not None:\n",
    "                optim.load_state_dict(load_dict[\"optim\"]) \n",
    "            self.load_state_dict(load_dict[\"model\"])\n",
    "        else:\n",
    "            self.load_state_dict(load_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85239a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoenc_train(net, optimizer, loss, epochs):\n",
    "    va_losses = []\n",
    "    tr_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_tr_loss = 0.0\n",
    "        net.train()\n",
    "        for i, sample in tqdm(enumerate(train_loader)):\n",
    "            imgs = sample[\"image\"].to(device, non_blocking=False).float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(imgs)  # should be close to the image\n",
    "            batch_loss = loss(outputs, imgs)  # L2 loss of reconstruction\n",
    "            epoch_tr_loss += batch_loss.item()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_va_loss = 0.0\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, sample in enumerate(valid_loader):\n",
    "                imgs = sample[\"image\"].to(device).float()\n",
    "                outputs = net(imgs)\n",
    "                epoch_va_loss += loss(outputs, imgs).item()\n",
    "        print(f'Epoch {epoch + 1}: va_loss: {epoch_va_loss}, tr_loss: {epoch_tr_loss}')\n",
    "        if not va_losses or epoch_va_loss < min(va_losses):\n",
    "            net.save_model_state_dict(optim=optimizer)\n",
    "        va_losses.append(epoch_va_loss)\n",
    "        tr_losses.append(epoch_tr_loss)\n",
    "    return va_losses, tr_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a37661",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_enc = AutoEncoder([[16, 7, 1],  # num_channels (input and output), kernel_size, max_pool kernel\n",
    "                        [32, 3, 2],  # make sure to change the batch size before working with this\n",
    "                        [32, 3, 2],\n",
    "                        [64, 3, 2],\n",
    "                        [64, 3, 2],\n",
    "                        [128, 3, 2],\n",
    "                        [128, 3, 1]], [128, 128, 1], \"auto_enc_greyscale_no_norm.dict\",\n",
    "                        fc_layers=[], embed_size=256).to(device)\n",
    "auto_enc_loss = nn.MSELoss()\n",
    "auto_enc_optim = torch.optim.Adam(auto_enc.parameters())\n",
    "print(auto_enc.num_params())\n",
    "auto_enc.load_model_state_dict(optim=auto_enc_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136c7b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = autoenc_train(auto_enc, auto_enc_optim, auto_enc_loss, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca4c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple sanity check\n",
    "auto_enc.eval()\n",
    "generated_img = valid_set.generate_one()[0]\n",
    "tensor_img = torch.tensor(np.expand_dims(generated_img,0).transpose(0,3,1,2)).to(device).float()\n",
    "reconstruction = np.expand_dims(auto_enc(tensor_img).detach().cpu().numpy().squeeze(),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reconstruction.mean(), generated_img.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2887066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_grid_figure([generated_img.squeeze(), reconstruction.squeeze()], colorbar=False, cmap=\"gray\", transpose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a22fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_indicator(outpts, inputs):  # assume values are equally spaced\n",
    "    outpt_diffs = outpts[:-1] - outpts[1:]\n",
    "    input_diffs = inputs[:-1] - inputs[1:]\n",
    "    deriv = outpt_diffs/input_diffs\n",
    "    avg_abs_grad = np.where(abs(deriv) < 1e-3, 1, 0).mean()  # for linear funcs, output_range = m*x_range\n",
    "    grad_range = np.max(abs(deriv)) - np.min(abs(deriv))\n",
    "    output_range = np.max(outpts) - np.min(outpts) # avg_square_grad = m**2/x_range\n",
    "    return output_range, avg_abs_grad, grad_range/output_range\n",
    "    \n",
    "def random_polynomial(num_pts, degree, pts_range, inpts):\n",
    "    input_pts = np.random.uniform(inpts.min(), inpts.max(), num_pts)\n",
    "    input_pts = np.concatenate((input_pts, [0,255]))\n",
    "    output_pts = np.random.uniform(*pts_range, num_pts)\n",
    "    output_pts = np.concatenate((output_pts, np.random.uniform(*pts_range, 2)))\n",
    "    return np.poly1d(np.polyfit(input_pts, output_pts, degree))(inpts)\n",
    "\n",
    "def random_indicator(jumps_range, inputs):\n",
    "    num_jumps = np.random.randint(*jumps_range)\n",
    "    jumps = np.sort(np.random.uniform(inputs.min(), inputs.max(), num_jumps))\n",
    "    noise = np.random.normal(loc=0, scale=0.01, size=(len(inputs)))\n",
    "    return np.digitize(inputs, jumps) % 2 + noise\n",
    "\n",
    "input_range = np.linspace(0,255,255)\n",
    "plt.subplot(1,2,1)\n",
    "poly_results = random_polynomial(4, 3, (-4, 4), input_range)\n",
    "plt.plot(input_range, poly_results)\n",
    "plt.subplot(1,2,2)\n",
    "indic_results = random_indicator((1,5), input_range)\n",
    "plt.plot(input_range, indic_results)\n",
    "print(is_indicator(poly_results, input_range))\n",
    "print(is_indicator(indic_results, input_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70b9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_image(encoder_net, img, dataset, target_class, sample_size=256):\n",
    "    sample = []\n",
    "    model.eval()\n",
    "    while len(sample) != sample_size:\n",
    "        sampled_img, sampled_label, *_ =  dataset.generate_one()\n",
    "        if sampled_label.argmax() != target_class:  # only consider sample from other classes\n",
    "            sample.append(sampled_img)\n",
    "    im_size = img.shape[0]\n",
    "    sample = np.array(sample).squeeze()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86a46e",
   "metadata": {},
   "source": [
    "# Model Optimization Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a9a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_net.eval()\n",
    "generated_img = torch.tensor(valid_set.generate_one()[0].transpose(2,0,1)).unsqueeze(0).to(device).float()\n",
    "profile_model = ProfileExecution(small_net)\n",
    "for _ in tqdm(range(1000)):\n",
    "    profile_model.forward(generated_img)\n",
    "profile_model.clean_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd852210",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum(stats.values())  # --> gave 3x speed! (Fast and Accurate Model scaling?)\n",
    "for k,v in sorted(stats.items(), key=lambda x: x[0]):    # --> the 3x speedup caused underfitting though, so switched to 2x\n",
    "    print(k,(100.*v/total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
