{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "import color_regions, network, visualizations, utils\n",
    "from color_regions import *\n",
    "from network import *\n",
    "from visualizations import *\n",
    "from utils import *\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7625d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up autoreloading of shared code\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport color_regions,network,visualizations,utils\n",
    "%aimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f19fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "prev_time = 0\n",
    "gamma = 0.99\n",
    "stats = {}  # tracks ewma running average\n",
    "def benchmark(point=None, profile=True, verbose=True, cuda=True): # not thread safe at all\n",
    "    global prev_time\n",
    "    if not profile:\n",
    "        return\n",
    "    if cuda:\n",
    "        torch.cuda.synchronize()\n",
    "    time_now = time.perf_counter()\n",
    "    if point is not None:\n",
    "        point = f\"{sys._getframe().f_back.f_code.co_name}-{point}\"\n",
    "        time_taken = time_now - prev_time\n",
    "        if point not in stats:\n",
    "            stats[point] = time_taken\n",
    "        stats[point] = stats[point]*gamma + time_taken*(1-gamma)\n",
    "        if verbose:\n",
    "            print(f\"took {time_taken} to reach {point}, ewma={stats[point]}\")\n",
    "    prev_time = time_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce8b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data_set = TextureDatasetGenerator(\"./data/dtd\")  # -> do this so we only load once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4eadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "     [transforms.ToTensor()])\n",
    "      #transforms.RandomRotation(90)])\n",
    "\n",
    "batch_size = 128  # seems to be the fastest batch size\n",
    "train_indices = (0, 500_000) # size of training set\n",
    "valid_indices = (1_250_000, 1_260_000)\n",
    "test_indices = (2_260_000, 2_270_000)\n",
    "\n",
    "def set_loader_helper(indices):\n",
    "    data_set = TextureDatasetGenerator(main_data_set,\n",
    "                                       transform=transform,\n",
    "                                       noise_size=(5,15),\n",
    "                                       size=128,\n",
    "                                       radius_frac=(1/3, 1/2.1),\n",
    "                                       image_indices=indices)\n",
    "    loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size, \n",
    "                                          shuffle=True, num_workers=4, pin_memory=True)\n",
    "    return data_set, loader\n",
    "train_set, train_loader = set_loader_helper(train_indices)\n",
    "valid_set, valid_loader = set_loader_helper(valid_indices)\n",
    "test_set, test_loader = set_loader_helper(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f349501",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_net = ResNet([[32, 7, 2],  # num_channels (input and output), kernel_size, stride\n",
    "                  [64, 3, 1],\n",
    "                  [64, 3, 1],\n",
    "                  [128, 3, 2],\n",
    "                  [128, 3, 1],\n",
    "                  [128, 3, 1],\n",
    "                  [256, 3, 2],\n",
    "                  [256, 3, 1],\n",
    "                  [512, 3, 2],\n",
    "                  [512, 3, 1]], valid_set.num_classes, [128, 128, 3], \n",
    "                   \"texture_net.dict\", fc_layers=[160]).to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(res_net.parameters())\n",
    "print(res_net.num_params())\n",
    "res_net.load_model_state_dict(optim=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e6e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train(res_net, optim, loss_func, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc44053",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(500_001)\n",
    "explain_img, explain_target_logit, *__ = valid_set.generate_one()\n",
    "heat_map = finite_differences_map(res_net, valid_set, explain_target_logit.argmax(), explain_img, device=device, batch_size=127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725c5459",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(explain_img/255.)\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow_centered_colorbar(heat_map.sum(axis=2), cmap=\"bwr\", title=\"FD Map\")\n",
    "print(valid_set.idx_to_texname[explain_target_logit.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8303659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_scales = [3,5,7,9,13,15]\n",
    "if 1: \n",
    "    %store -r pca_directions_1_stride pca_directions_s_stride\n",
    "else:\n",
    "    pca_directions_1_stride = find_pca_directions(valid_set, 16384, default_scales, 1)\n",
    "    pca_directions_s_stride = find_pca_directions(valid_set, 16384, default_scales, default_scales)\n",
    "    %store pca_directions_1_stride pca_directions_s_stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa30368",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca_directions(pca_directions_1_stride, \"Strides=1\", default_scales, component=0, lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c26fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca_directions(pca_directions_1_stride, \"Strides=1\", default_scales, component=1, lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8499944",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca_directions(pca_directions_1_stride, \"Strides=1\", default_scales, component=2, lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962dbd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca_directions(pca_directions_1_stride, \"Strides=1\", default_scales, component=3, lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60033135",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca_directions(pca_directions_s_stride, \"Strides=scales\", default_scales, component=0, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ae987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca_directions(pca_directions_s_stride, \"Strides=scales\", default_scales, component=1, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939de1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca_directions(pca_directions_s_stride, \"Strides=scales\", default_scales, component=2, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9b36ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca_directions(pca_directions_s_stride, \"Strides=scales\", default_scales, component=3, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(500_001)\n",
    "explain_img, explain_target_logit, *__ = valid_set.generate_one()\n",
    "result = pca_direction_grids(res_net, valid_set, explain_target_logit.argmax(), explain_img, default_scales, \n",
    "                    pca_directions_s_stride, device=device, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c4e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,4,1)\n",
    "plt.imshow(explain_img.squeeze())\n",
    "for c in range(3):\n",
    "    plt.subplot(1,4,c+2)\n",
    "    imshow_centered_colorbar(result[...,c], cmap=\"bwr\", title=f\"Channel {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9508fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1_2123, 1_40_124, 1_508_559, 1_5_019_258, 1_2_429_852, 9032, 5832, 12, 5014, 92, 42, 52, \n",
    "         52_934, 935_152, 1_000_000, 1_000_001, 27, 24, 512, 999_105]  # 20 \n",
    "def generate_many_pca(net, component=0):\n",
    "    _pca_map_s_strides = []\n",
    "    _pca_map_1_strides = []\n",
    "    _grad_maps = []\n",
    "    _explain_imgs = []\n",
    "    for seed in seeds:\n",
    "        np.random.seed(seed)\n",
    "        generated_img, label, *__ = valid_set.generate_one()\n",
    "        tensored_img = tensorize(generated_img, device=device, requires_grad=True)\n",
    "        grad_map = torch.autograd.grad(net(tensored_img)[0,label.argmax()], tensored_img)[0]\n",
    "        pca_map_strided = pca_direction_grids(net, valid_set, label.argmax(), generated_img, \n",
    "                                              default_scales, pca_directions_s_stride, \n",
    "                                              device=device, batch_size=128, component=component)\n",
    "        pca_map_1_stride = pca_direction_grids(net, valid_set, label.argmax(), generated_img, \n",
    "                                              default_scales, pca_directions_1_stride, component=component \n",
    "                                              device=device, batch_size=128, strides=1)\n",
    "        _explain_imgs.append(generated_img)\n",
    "        _grad_maps.append(grad_map.detach().cpu().squeeze(0).numpy().transpose(1,2,0))\n",
    "        _pca_map_s_strides.append(pca_map_strided.copy())\n",
    "        _pca_map_1_strides.append(pca_map_1_stride.copy())\n",
    "    return _pca_map_s_strides, _pca_map_1_strides, _grad_maps, _explain_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229b7a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_map_s_strides, pca_map_1_strides, grad_maps, explain_imgs = generate_many_pca(res_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b28aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "texture_pca_map_1_strides_2023_01_29 = pca_map_1_strides\n",
    "%store texture_pca_map_1_strides_2023_01_29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab5c5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_figure_list = []\n",
    "for i, img in enumerate(pca_map_1_strides):\n",
    "    grid_figure_row = [explain_imgs[i]]\n",
    "    grid_figure_row += [x for x in img.transpose(2,0,1)]\n",
    "    grid_figure_row += [b for b in grad_maps[i].transpose(2,0,1)]\n",
    "    grid_figure_list.append(grid_figure_row)\n",
    "plt_grid_figure(grid_figure_list, titles=[\"Image\", \n",
    "                                          \"Channel 0 PCA (strides=1)\",\n",
    "                                          \"Channel 1 PCA (strides=1)\",\n",
    "                                          \"Channel 2 PCA (strides=1)\",\n",
    "                                          \"Channel 0 Gradient Map\",\n",
    "                                          \"Channel 1 Gradient Map\",\n",
    "                                          \"Channel 2 Gradient Map\"], cmap=\"bwr\", transpose=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f717bf",
   "metadata": {},
   "source": [
    "# PCA Direction convergence experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca_directions(pca_directions_1_stride, \"Strides=1\", default_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c39a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca_directions(pca_directions_s_stride, \"Strides=scales\", default_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e6036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_scales = [3,5,7,9,13,15]\n",
    "small_pca_directions_1_stride = find_pca_directions(valid_set, 512, default_scales, 1)\n",
    "small_pca_directions_s_stride = find_pca_directions(valid_set, 512, default_scales, default_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfff2255",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca_directions(small_pca_directions_1_stride, \"Strides=1\", default_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d468101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca_directions(small_pca_directions_s_stride, \"Strides=scales\", default_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbd5858",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(510)\n",
    "test_directions = find_pca_directions(valid_set, 8192*4, default_scales, default_scales, component=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9981afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca_directions(test_directions, \"Strides=scales\", default_scales) \n",
    "# component 0\n",
    "# seed 510 gargantuan (32768) sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772f5f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca_directions(test_directions, \"Strides=scales\", default_scales)\n",
    "# component 1\n",
    "# seed 510 gargantuan (32768) sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfcf3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca_directions(test_directions, \"Strides=scales\", default_scales)\n",
    "# component 1\n",
    "# seed 507 gargantuan (32768) sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c966780",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca_directions(test_directions, \"Strides=scales\", default_scales)\n",
    "# component 1\n",
    "# seed 507, huge (8192) sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3653a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca_directions(test_directions, \"Strides=scales\", default_scales) \n",
    "# component 1\n",
    "# seed 507, large (2048) sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f76dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca_directions(test_directions, \"Strides=scales\", default_scales) \n",
    "# component 1\n",
    "# seed 508, large (2048) sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621e6101",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca_directions(test_directions, \"Strides=scales\", default_scales)\n",
    "# component 1\n",
    "# seed 508, small sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c954c967",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca_directions(test_directions, \"Strides=scales\", default_scales) \n",
    "# component 1\n",
    "# seed 507, small sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f8060b",
   "metadata": {},
   "source": [
    "What if we run the same experiment, but cheat with a prior on pixel values that we know *should* be informative to the output logit, namely values closest to the decision boundary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86a46e",
   "metadata": {},
   "source": [
    "# Model Optimization Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f3a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_net.save_model_state_dict(optim=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a9a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    res_net.forward(generated_img, profile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd852210",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum(stats.values())  # --> gave 3x speed! (Fast and Accurate Model scaling?)\n",
    "for k,v in stats.items():    # --> the 3x speedup caused underfitting though, so switched to 2x\n",
    "    print(k,(100.*v/total))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
