{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "from color_regions import *\n",
    "from network import *\n",
    "from visualizations import *\n",
    "from utils import *\n",
    "from hooks import *\n",
    "from config_objects import *\n",
    "from training import *\n",
    "\n",
    "# set up autoreloading of shared code\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport color_regions,network,visualizations,utils,hooks,config_objects,training\n",
    "%aimport\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd95c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pickle\n",
    "import dataclasses\n",
    "\n",
    "\n",
    "with open(\"./full_random_noisy/log.pkl\", \"rb\") as p:\n",
    "    data = pickle.load(p)\n",
    "\n",
    "    layer_sizes=dict(medium_size=[[16, 3, 1], [32, 3, 1]],\n",
    "                    tiny_size=[[2, 3, 4], [6, 3, 4]],\n",
    "                    large_size=[[16, 3, 1], [32, 3, 2], [32, 3, 2], [64, 3, 2]],\n",
    "                    huge_size=[[16, 3, 1], [32, 3, 2], [32, 3, 2], [64, 3, 2], [64, 3, 1], [128, 3, 1]])\n",
    "    \n",
    "    inv_layer_sizes = {str(v): k for k,v in layer_sizes.items()}\n",
    "    \n",
    "for k,v in data[\"train_results\"].items():\n",
    "    conf = dataclasses.asdict(k)\n",
    "    conf[\"layer_sizes\"] = inv_layer_sizes[str(conf[\"layer_sizes\"])]\n",
    "    wandb.init(project='project-apfij50gijdpoaij', config=conf)\n",
    "    for va_loss, va_acc, tr_loss in zip(*v[:3]):\n",
    "        wandb.log({\"va_loss\": va_loss, \"va_acc\": va_acc, \"tr_loss\": tr_loss})\n",
    "    \n",
    "    #print(data[\"test_results\"][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ba02f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pickle\n",
    "import dataclasses\n",
    "\n",
    "\n",
    "with open(\"./full_random_noisy/log.pkl\", \"rb\") as p:\n",
    "    data = pickle.load(p)\n",
    "\n",
    "    layer_sizes=dict(medium_size=[[16, 3, 1], [32, 3, 1]],\n",
    "                    tiny_size=[[2, 3, 4], [6, 3, 4]],\n",
    "                    large_size=[[16, 3, 1], [32, 3, 2], [32, 3, 2], [64, 3, 2]],\n",
    "                    huge_size=[[16, 3, 1], [32, 3, 2], [32, 3, 2], [64, 3, 2], [64, 3, 1], [128, 3, 1]])\n",
    "    \n",
    "    inv_layer_sizes = {str(v): k for k,v in layer_sizes.items()}\n",
    "\n",
    "best_run = {model_size:None for model_size in layer_sizes}\n",
    "best_acc = {model_size:-float(\"inf\") for model_size in layer_sizes}\n",
    "for k,v in data[\"train_results\"].items():\n",
    "    conf = dataclasses.asdict(k)\n",
    "    #conf[\"layer_sizes\"] = inv_layer_sizes[str(conf[\"layer_sizes\"])]\n",
    "    size_type = inv_layer_sizes[str(conf[\"layer_sizes\"])]\n",
    "    if data[\"test_results\"][k][1] > best_acc[size_type]:\n",
    "        best_acc[size_type] = data[\"test_results\"][k][1]\n",
    "        best_run[size_type] = k\n",
    "    #wandb.init(project='project-apfij50gijdpoaij', config=conf)\n",
    "        #wandb.log({\"va_loss\": va_loss, \"va_acc\": va_acc, \"tr_loss\": tr_loss})\n",
    "    \n",
    "    #print(data[\"test_results\"][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e921f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_uploaded = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208d97b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc\n",
    "# {'medium_size': 0.97576,\n",
    "#  'tiny_size': 0.84508,\n",
    "#  'large_size': 0.96756,\n",
    "#  'huge_size': 0.97484}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fccb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run\n",
    "# huge is worth training more (radiant-flower 23)\n",
    "# large maybe worth (quiet-grass 58)\n",
    "# medium not worth (rare-bee 3)\n",
    "# tiny worth training (helpful-cloud 43)\n",
    "# {'medium_size': ExperimentConfig(layer_sizes=[[16, 3, 1], [32, 3, 1]], fc_layers=[], groups=1, global_avg_pooling=True, learn_rate=0.01, weight_decay=1e-07, gain=0.05, epochs=50),\n",
    "#  'tiny_size': ExperimentConfig(layer_sizes=[[2, 3, 4], [6, 3, 4]], fc_layers=[], groups=1, global_avg_pooling=False, learn_rate=0.001, weight_decay=3.7926901907322535e-06, gain=0.2, epochs=30),\n",
    "#  'large_size': ExperimentConfig(layer_sizes=[[16, 3, 1], [32, 3, 2], [32, 3, 2], [64, 3, 2]], fc_layers=[], groups=1, global_avg_pooling=False, learn_rate=0.001, weight_decay=0.0026366508987303553, gain=0.05, epochs=30),\n",
    "#  'huge_size': ExperimentConfig(layer_sizes=[[16, 3, 1], [32, 3, 2], [32, 3, 2], [64, 3, 2], [64, 3, 1], [128, 3, 1]], fc_layers=[], groups=1, global_avg_pooling=True, learn_rate=0.01, weight_decay=2.06913808111479e-07, gain=0.1, epochs=30)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4eadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = (0, 250_000) # size of training set\n",
    "valid_indices = (1_250_000, 1_275_000)\n",
    "test_indices = (3_260_000, 3_560_000)\n",
    "\n",
    "critical_color_values = list(range(0,241,30))\n",
    "\n",
    "dset_config = ColorDatasetConfig(task_difficulty=\"hard\",\n",
    "                                 noise_size=(1,9),\n",
    "                                 num_classes=3,\n",
    "                                 num_objects=0,  # => permuted\n",
    "                                 radius=(1/8., 1/7.),\n",
    "                                 device=device,\n",
    "                                 batch_size=128)\n",
    "\n",
    "# copies the config each time\n",
    "train_set = ColorDatasetGenerator(train_indices, dset_config)\n",
    "valid_set = ColorDatasetGenerator(valid_indices, dset_config)\n",
    "test_set = ColorDatasetGenerator(test_indices, dset_config)\n",
    "# train_set.cfg.infinite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bbf7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"hard\" task\n",
    "critical_color_values = list(range(0,241,30))\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "color_probe = np.linspace(0, 255, 255)\n",
    "color_class = [hard_color_classifier(x) for x in color_probe]\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(color_probe, color_class)\n",
    "plt.xticks(critical_color_values)\n",
    "plt.yticks([0, 1, 2])\n",
    "plt.ylabel(\"Class\")\n",
    "\n",
    "med_color_class = [medium_color_classifier(x) for x in color_probe]\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(color_probe, med_color_class)\n",
    "plt.xlabel(\"Image Intensity\")\n",
    "plt.xticks([100, 150, 200])\n",
    "plt.yticks([0, 1, 2])\n",
    "plt.ylabel(\"Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ae9cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_x = 4\n",
    "num_y = 4\n",
    "plt.figure(figsize=(3*num_x, 3*num_y))\n",
    "# back_probs = [0.25]\n",
    "#valid_set.back_p = 0.25\n",
    "for i in range(num_x*num_y):\n",
    "\n",
    "    #valid_set.back_p = back_probs[i % 3]\n",
    "    while not (80 < (img_gen := valid_set.generate_one())[2] < 150): # only do ones with target color >= 40\n",
    "        pass\n",
    "    plt.subplot(num_y, num_x, i+1)\n",
    "#     if i // num_x == 0:\n",
    "#         plt.title(f\"p={valid_set.back_p}\")\n",
    "    imshow_centered_colorbar(img_gen[0], cmap=\"gray\", colorbar=False)\n",
    "#     plt.subplot(num_x, num_y, i*2+2)\n",
    "#     plot_color_classes(valid_set, (0, 128), alpha=1.0)\n",
    "#     plt.vlines([clr], 0, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdc898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can specify a probability via the second value in the tuple for each entry\n",
    "hyperparameters=dict(learn_rate=[1e-4, 1e-3, 1e-2],\n",
    "                     weight_decay=10**np.linspace(-7, -1, 20),\n",
    "                     global_avg_pooling=[True, False],\n",
    "                     layer_sizes=dict(medium_size=[[16, 3, 1], [32, 3, 1]],\n",
    "                                      tiny_size=[[2, 3, 4], [6, 3, 4]],\n",
    "                                      large_size=[[16, 3, 1], [32, 3, 2], [32, 3, 2], [64, 3, 2]],\n",
    "                                      huge_size=[[16, 3, 1], [32, 3, 2], [32, 3, 2], [64, 3, 2], [64, 3, 1], [128, 3, 1]],\n",
    "                                     ),\n",
    "                    gain=[0, 0.05, 0.1, 0.2, 0.3])\n",
    "prob_dists = dict(layer_sizes=[0.4, 0.4, 0.1, 0.1])\n",
    "run_experiments(train_set, valid_set, \"./full_random_noisy\", hyperparameters, \n",
    "                search_type=\"random\", prob_dists=prob_dists, num_rand=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528ba560",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_arr = np.zeros((2,12))\n",
    "loss_arr = np.zeros((2,12))\n",
    "\n",
    "for i, learn_rate in enumerate([1e-4, 1e-3, 1e-2]):\n",
    "        for j, weight_decay in enumerate(10**np.linspace(-7, -3, 12)):\n",
    "            acc_arr[i,j] = results[(learn_rate, weight_decay, )]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb96c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model from the sample (note that none actually used gain or weight decay, due to a mistake)\n",
    "large_noise_net = ResNet([[16, 3, 1], [32, 3, 1]], valid_set.num_classes, [128, 128, 1], \n",
    "                   \"decay_noise/large_size_0.2_0.0000351.dict\", global_avg_pooling=True, \n",
    "                   fc_layers=[]).to(device)\n",
    "print(large_noise_net.num_params())\n",
    "large_noise_net.load_model_state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a4e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_net = ResNet([[2, 3, 4], [6, 3, 4]], valid_set.num_classes, [128, 128, 1], \n",
    "                   \"permuted_hard_tiny.dict\", global_avg_pooling=True,\n",
    "                   fc_layers=[]).to(device)\n",
    "print(noise_net.num_params())\n",
    "noise_net.load_model_state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b8231",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(noise_net, nn.CrossEntropyLoss(), test_loader, device=device)\n",
    "# tiny, 84.4% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650ee0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(large_noise_net, nn.CrossEntropyLoss(), test_loader, device=device)\n",
    "# large, 97.6% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cbd1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_net.eval()\n",
    "avg_img = np.ones((valid_set.size, valid_set.size))\n",
    "tensor_avg_img = tensorize(avg_img, device=device)\n",
    "responses = []\n",
    "for color in np.arange(255):\n",
    "    tensor_avg_img[...] = color\n",
    "    responses.append(permuted_net(tensor_avg_img).detach().cpu().numpy())\n",
    "responses = np.asarray(responses).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24262db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    plt.plot(np.arange(255), responses[:,i], label=f\"logit {i}\")\n",
    "plt.legend()\n",
    "plot_color_classes(valid_set, (responses.min(), responses.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da72635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def averaging_test(dataset, sample, edge_width=10):\n",
    "    avg_area = np.pi/3*(dataset.radius[1]**2+dataset.radius[0]**2+dataset.radius[0]*dataset.radius[1])\n",
    "    pct_area = avg_area / (dataset.size**2)\n",
    "    print(f\"Targets are on average {pct_area:.1%} of the image\")\n",
    "    other_points = []\n",
    "    \n",
    "    total_answered = 0\n",
    "    right_calibrated = 0\n",
    "    right_naive = 0\n",
    "    right_color_set = 0\n",
    "    right_base = 0\n",
    "    right_edge_set = 0\n",
    "    right_background_set = 0\n",
    "    \n",
    "    avg_img = np.ones((dataset.size, dataset.size))\n",
    "    tensor_avg_img = tensorize(avg_img, device=device)\n",
    "    for _ in tqdm(range(sample)):\n",
    "        img_gen, lbl, color, *_ = dataset.generate_one()\n",
    "        color = color[0]\n",
    "        foreground_mask = np.where(img_gen>2)\n",
    "        other_space = img_gen[(img_gen > 2) & (img_gen != color)].sum() / foreground_mask[0].shape[0]\n",
    "        \n",
    "        prediction = (img_gen[foreground_mask].mean() - 36.9)/(1-36.9/128)\n",
    "        if np.isnan(prediction) or np.isnan(other_space):\n",
    "            continue\n",
    "        tensor_avg_img[...] = color  # color setting\n",
    "        color_set_classif = permuted_net(tensor_avg_img).argmax()\n",
    "        \n",
    "        tensor_avg_img[...] = img_gen.mean()  # naive averaging\n",
    "        naive_classif = permuted_net(tensor_avg_img).argmax()\n",
    "        \n",
    "        tensor_avg_img[...] = prediction  # calibrated averaging\n",
    "        calibrated_classif = permuted_net(tensor_avg_img).argmax()\n",
    "        \n",
    "        tensor_img_gen = tensorize(img_gen, device=device)\n",
    "        base_classif = permuted_net(tensor_img_gen).argmax() # regular classification\n",
    "        \n",
    "        tensor_img_gen[tensor_img_gen == 0] = (color + 30) % 255 # set background to a different class\n",
    "        background_set_classif = permuted_net(tensor_img_gen).argmax()\n",
    "        \n",
    "        # edge set test (set to color since thats the best results)\n",
    "        tensor_avg_img[...] = 0\n",
    "        tensor_avg_img[0,0, 0:edge_width] = color\n",
    "        tensor_avg_img[0,0, -edge_width:] = color\n",
    "        tensor_avg_img[0,0,:, 0:edge_width] = color\n",
    "        tensor_avg_img[0,0,:, -edge_width:] = color\n",
    "        edge_set_classif = permuted_net(tensor_avg_img).argmax()\n",
    "        \n",
    "        total_answered += 1\n",
    "        right_base += lbl.argmax() == base_classif\n",
    "        right_background_set += lbl.argmax() == background_set_classif\n",
    "        right_edge_set += lbl.argmax() == edge_set_classif\n",
    "        right_calibrated += lbl.argmax() == calibrated_classif\n",
    "        right_naive += lbl.argmax() == naive_classif\n",
    "        right_color_set += lbl.argmax() == color_set_classif\n",
    "    print(f\"Calibrated got {right_calibrated/total_answered:.2%} correct\")\n",
    "    print(f\"Naive got {right_naive/total_answered:.2%} correct\")\n",
    "    print(f\"Color setting got {right_color_set/total_answered:.2%} correct\")\n",
    "    print(f\"Edge setting got {right_edge_set/total_answered:.2%} correct\")\n",
    "    print(f\"Background setting got {right_background_set/total_answered:.2%} correct\")\n",
    "    print(f\"Base got {right_base/total_answered:.2%} correct\")\n",
    "    \n",
    "result = averaging_test(valid_set, 100_000)\n",
    "# PCA map to see edge behaviour (average a bunch of them?)\n",
    "# color set edge test\n",
    "# background only set test? (do it maliciously) (see how badly it hurts performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0790b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_by_color(dataset, sample=100_000):\n",
    "    points = []\n",
    "    avg_area = np.pi/3*(dataset.radius[1]**2+dataset.radius[0]**2+dataset.radius[0]*dataset.radius[1])\n",
    "    pct_area = avg_area / (dataset.size**2)\n",
    "    print(f\"Targets are on average {pct_area:.1%} of the image\")\n",
    "    other_points = []\n",
    "    total_answered = 0\n",
    "    right_calibrated = 0\n",
    "    right_naive = 0\n",
    "    right_really_naive = 0\n",
    "    for _ in tqdm(range(sample)):\n",
    "        img_gen, lbl, color, *_ = dataset.generate_one()\n",
    "        #prediction = np.minimum(img_gen/pct_area, 255)\n",
    "        foreground_mask = np.where(img_gen>2)\n",
    "        other_space = img_gen[(img_gen > 2) & (img_gen != color)].sum() / foreground_mask[0].shape[0]\n",
    "        #print(len(foreground_mask[0]), img_gen[(img_gen > 2) & (img_gen != color)].size)\n",
    "        # model: avg = color*(1-pct) + 128*pct\n",
    "        # calculate pct by figuring out the average sum of non-target non-background pixels\n",
    "        # divided by the size of the non-background area => gives you 128*pct\n",
    "        \n",
    "        prediction = (img_gen[foreground_mask].mean() - 36.9)/(1-36.9/128)\n",
    "        if np.isnan(prediction) or np.isnan(other_space):\n",
    "            continue\n",
    "        total_answered += 1\n",
    "        right_calibrated += lbl.argmax() == color_classifier(prediction)\n",
    "        right_naive += lbl.argmax() == color_classifier(img_gen[foreground_mask].mean())\n",
    "        right_really_naive += lbl.argmax() == color_classifier(img_gen.mean()/pct_area)\n",
    "        points.append((color, prediction))\n",
    "        other_points.append((color, other_space))\n",
    "    print(f\"Calibrated got {right_calibrated/total_answered:.2%} correct\")\n",
    "    print(f\"Naive got {right_naive/total_answered:.2%} correct\")\n",
    "    print(f\"Really naive got {right_really_naive/total_answered:.2%} correct\")\n",
    "\n",
    "    return np.asarray(points), np.asarray(other_points)\n",
    "result = error_by_color(valid_set, sample=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cacc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(result[1][:,0], result[1][:,1], s=0.05)\n",
    "plt.plot(np.arange(255), c=\"r\")\n",
    "result[1][:,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff6084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(result[0][:,0], result[0][:,1], s=0.05)\n",
    "plt.plot(np.arange(255), c=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9785a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5_123_456)\n",
    "test_img, lbl, color, size, *_  = valid_set.generate_one()\n",
    "print(color)\n",
    "plt.imshow(test_img, cmap=\"gray\")\n",
    "tensor_test_img = tensorize(test_img, device=device)\n",
    "\n",
    "interp_net = AllActivations(noise_net)\n",
    "interp_net.eval()\n",
    "interp_net(tensor_test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "block = 1\n",
    "\n",
    "#uniform_inpt = torch.full((1,16,32,32), 100.0).to(device)\n",
    "#plt.imshow(tiny_net.conv_blocks[0].conv2.weight[c, in_c].detach().cpu().numpy(), cmap=\"bwr\")\n",
    "conv_maps = tiny_net.conv_blocks[block].conv2.weight[c, :]\n",
    "#imshow_centered_colorbar(conv_maps[7].detach().cpu().numpy(), cmap=\"bwr\")\n",
    "conv_scale = conv_maps.max(axis=-1).values.max(axis=-1).values\n",
    "conv_shift = tiny_net.conv_blocks[block].conv2.bias[c]\n",
    "bn_scale = tiny_net.conv_blocks[block].batch_norm2.weight[c]\n",
    "bn_shift = tiny_net.conv_blocks[block].batch_norm2.bias[c]\n",
    "bn_var = tiny_net.conv_blocks[block].batch_norm2.running_var[c]\n",
    "bn_mean = tiny_net.conv_blocks[block].batch_norm2.running_mean[c]\n",
    "print(conv_shift, bn_scale, bn_shift, bn_var, bn_mean)\n",
    "#(c*conv_scale + conv_shift - bn_mean) / torch.sqrt(bn_var) * bn_scale + bn_shift\n",
    "slope = (conv_scale/torch.sqrt(bn_var)*bn_scale).detach().cpu().numpy()\n",
    "bias = ((conv_shift - bn_mean)/torch.sqrt(bn_var)*bn_scale + bn_shift).detach().cpu().numpy()\n",
    "\n",
    "lines = np.asarray([profile_plots[f\"conv_blocks.{block}.act_func1_{x}\"][0] for x in range(6)])\n",
    "\n",
    "uniform_scaling = slope.dot(lines) + bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09b7313",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.maximum(uniform_scaling, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab19196",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "feature_gram, projected_weights = visualizations.fc_conv_feature_angles(noise_net, \n",
    "                            \"fully_connected.0.act_func\", num_embed=3, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5824af",
   "metadata": {},
   "source": [
    "# Small Network Weight Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4287d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_net.eval()\n",
    "profile_plots,_ = activation_color_profile(AllActivations(noise_net), valid_loader, valid_set, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a3e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5_13_46)\n",
    "test_img, lbl, color, size, pos, noise, orig_img  = valid_set.generate_one()\n",
    "print(color)\n",
    "\n",
    "plt.figure(figsize=(12,16))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_img, cmap=\"gray\")\n",
    "plt.subplot(1,2,2)\n",
    "denoised_img = np.where(test_img == color, color, 0)\n",
    "plt.imshow(denoised_img, cmap=\"gray\")\n",
    "\n",
    "tensor_test_img = tensorize(test_img, device=device)\n",
    "denoised_tensor_img = tensorize(denoised_img, device=device)\n",
    "\n",
    "interp_net = AllActivations(noise_net)\n",
    "interp_net.eval()\n",
    "print(interp_net(tensor_test_img))\n",
    "\n",
    "de_interp_net = AllActivations(noise_net)\n",
    "de_interp_net.eval()\n",
    "print(de_interp_net(denoised_tensor_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b05a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_weights(interp_net, \"conv_blocks.0.act_func1\", color_profile=profile_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21100dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(interp_net, \"conv_blocks.0.act_func1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8383a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_weights(interp_net, \"conv_blocks.0.act_func2\", color_profile=profile_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8496bee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(interp_net, \"conv_blocks.0.act_func2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(de_interp_net, \"conv_blocks.0.act_func2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95391ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(interp_net.model.conv_blocks[1].conv1.bias)\n",
    "show_conv_weights(interp_net, \"conv_blocks.1.act_func1\", color_profile=profile_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7de39ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(interp_net, \"conv_blocks.1.act_func1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5f0912",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(de_interp_net, \"conv_blocks.1.act_func1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140be7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_weights(interp_net, \"conv_blocks.1.act_func2\", color_profile=profile_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24567e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(interp_net, \"conv_blocks.1.act_func2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5587be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(de_interp_net, \"conv_blocks.1.act_func2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421bcda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_fc_conv(interp_net, color_profile=profile_plots, fixed_height=True, full_gridspec=True)\n",
    "# no longer have this since its GAP\n",
    "show_fc(interp_net, \"fully_connected.0.act_func\", color_profile=profile_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e881748",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(interp_net, \"conv_blocks.1.act_func1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a54c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(interp_net, \"conv_blocks.1.act_func2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c97db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(de_interp_net, \"conv_blocks.1.act_func2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e21661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_mapper = get_weight(interp_net, \"fully_connected.0.fully_connected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ed68e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_large_net.final_img_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483523e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_weights(interp_net, \"conv_blocks.0.act_func2\", color_profile=permuted_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c1d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(interp_net, \"conv_blocks.0.act_func2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c4db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_weights(interp_net, \"conv_blocks.1.act_func1\", color_profile=permuted_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fa4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(interp_net, \"conv_blocks.1.act_func1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5310ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_weights(interp_net, \"conv_blocks.1.act_func2\", color_profile=permuted_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36fa0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(interp_net, \"conv_blocks.1.act_func2\")\n",
    "# uniform image of average and pass into network\n",
    "# pasting images onto each other\n",
    "# send in images that have only target pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7456f481",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_fc_conv(interp_net, color_profile=permuted_plots, fixed_height=True, full_gridspec=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe3a100",
   "metadata": {},
   "source": [
    "# PCA Direction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015647ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_scales = [3,5,7,9,13,15]\n",
    "if 0: \n",
    "    %store -r noise_back_pca_directions_1_stride noise_back_pca_directions_s_stride\n",
    "else:\n",
    "    noise_back_pca_directions_1_stride = find_pca_directions(valid_set, 4096, default_scales, 1)\n",
    "    noise_back_pca_directions_s_stride = find_pca_directions(valid_set, 4096, default_scales, default_scales)\n",
    "    %store noise_back_pca_directions_1_stride noise_back_pca_directions_s_stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9758c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca_directions(noise_back_pca_directions_s_stride, \"Strides=scales\", default_scales, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef2b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1_2123, 1_40_124, 1_508_559, 1_5_019_258, 1_2_429_852, 9032, 5832, 12, 5014, 92, 42, 52, \n",
    "         52_934, 935_152, 1_000_000, 1_000_001, 27, 24, 512, 999_105]  # 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5312b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_map_s_strides, _, grad_maps, explain_imgs = generate_many_pca(permuted_net, seeds, \n",
    "                noise_back_pca_directions_1_stride, default_scales, valid_set, component=0, \n",
    "                batch_size=512, strides=3, skip_1_stride=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90178548",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_grid_figure([explain_imgs, pca_map_s_strides, grad_maps], transpose=True, titles=[\"Image\", \"Strides=3\", \"Gradient\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
