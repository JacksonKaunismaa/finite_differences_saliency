{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from collections import defaultdict\n",
    "\n",
    "import color_regions, network, visualizations, utils\n",
    "from color_regions import *\n",
    "from network import *\n",
    "from visualizations import *\n",
    "from utils import *\n",
    "from hooks import *\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcabd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up autoreloading of shared code\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport color_regions,network,visualizations,utils,hooks\n",
    "%aimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4eadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "     [transforms.ToTensor()])#,\n",
    "    #transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "batch_size = 128 # seems to be the fastest batch size\n",
    "train_indices = (0, 250_000) # size of training set\n",
    "valid_indices = (1_250_000, 1_270_000)\n",
    "test_indices = (2_260_000, 2_560_000)\n",
    "\n",
    "def color_classifier(color):  \n",
    "    if color <= 30:  # => 3 classes\n",
    "        return 0\n",
    "    if 30 < color <= 60:  # => 90/255 is 0, 90/255 is 1, 75/255 is 2\n",
    "        return 1\n",
    "    if 60 < color <= 90:\n",
    "        return 2\n",
    "    if 90 < color <= 120:\n",
    "        return 1\n",
    "    if 120 < color <= 150:\n",
    "        return 0\n",
    "    if 150 < color <= 180:\n",
    "        return 1\n",
    "    if 180 < color <= 210:\n",
    "        return 2\n",
    "    if 210 < color <= 240:\n",
    "        return 0\n",
    "    if 240 < color:\n",
    "        return 2\n",
    "critical_color_values = list(range(0,241,30))\n",
    "\n",
    "def set_loader_helper(indices, infinite=False):\n",
    "    data_set = ColorDatasetGenerator(color_classifier=color_classifier,\n",
    "                                    image_indices=indices,\n",
    "                                    transform=transform,\n",
    "                                    color_range=(5, 255),\n",
    "                                    noise_size=(1,9),\n",
    "                                    num_classes=3,\n",
    "                                    infinite=infinite,\n",
    "                                    size=128,\n",
    "                                    num_objects=0,\n",
    "                                    radius=(128//8, 128//7))\n",
    "    loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size, \n",
    "                                          shuffle=True, num_workers=6, pin_memory=True)\n",
    "    return data_set, loader\n",
    "train_set, train_loader = set_loader_helper(train_indices, infinite=False)\n",
    "valid_set, valid_loader = set_loader_helper(valid_indices)\n",
    "test_set, test_loader = set_loader_helper(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bbf7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"hard\" task\n",
    "plt.figure(figsize=(6,6))\n",
    "color_probe = np.linspace(0, 255, 255)\n",
    "color_class = [color_classifier(x) for x in color_probe]\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(color_probe, color_class)\n",
    "plt.xticks(critical_color_values)\n",
    "plt.yticks([0, 1, 2])\n",
    "plt.ylabel(\"Class\")\n",
    "def medium_color_classifier(color):\n",
    "    if color <= 100:  \n",
    "        return 0\n",
    "    if 100 < color <= 150:\n",
    "        return 1\n",
    "    if 150 < color <= 200: \n",
    "        return 2\n",
    "    if 200 < color:\n",
    "        return 1\n",
    "med_color_class = [medium_color_classifier(x) for x in color_probe]\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(color_probe, med_color_class)\n",
    "plt.xlabel(\"Color\")\n",
    "plt.xticks([100, 150, 200])\n",
    "plt.yticks([0, 1, 2])\n",
    "plt.ylabel(\"Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dataset to uniformly change background (noise)\n",
    "# evidence that we care about color classification\n",
    "# color vs shape \n",
    "# GuidedBackprop shows that conv nets rely mostly on edges\n",
    "# What happens when the network cannot rely only on the edges\n",
    "# practical examples of beach orientation detection, shadow xray?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ae9cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_x = 4\n",
    "num_y = 4\n",
    "plt.figure(figsize=(3*num_x, 3*num_y))\n",
    "# back_probs = [0.25]\n",
    "valid_set.back_p = 0.25\n",
    "for i in range(num_x*num_y):\n",
    "\n",
    "    #valid_set.back_p = back_probs[i % 3]\n",
    "    while not (80 < (img_gen := valid_set.generate_one())[2] < 150): # only do ones with target color >= 40\n",
    "        pass\n",
    "    plt.subplot(num_y, num_x, i+1)\n",
    "#     if i // num_x == 0:\n",
    "#         plt.title(f\"p={valid_set.back_p}\")\n",
    "    imshow_centered_colorbar(img_gen[0], cmap=\"gray\", colorbar=False)\n",
    "#     plt.subplot(num_x, num_y, i*2+2)\n",
    "#     plot_color_classes(valid_set, (0, 128), alpha=1.0)\n",
    "#     plt.vlines([clr], 0, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30255ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.mgrid[:128, :128].reshape(-1, 2)\n",
    "np.random.shuffle(idxs)\n",
    "idxs = idxs.reshape(128, 128, 2)\n",
    "print(idxs.shape)\n",
    "plt.imshow(img_gen[0][idxs[...,0], idxs[...,1]], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7aebda",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_net = ResNet([[2, 3, 4],  # num_channels (input and output), kernel_size, stride\n",
    "                   [6, 3, 4]], 3, [128, 128, 1], \n",
    "                   \"noise_net_hard_tiny.dict\", fc_layers=[]).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "noise_optim = torch.optim.Adam(noise_net.parameters())\n",
    "print(noise_net.num_params())\n",
    "noise_net.load_model_state_dict(optim=noise_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3083fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_net = ResNet([[2, 3, 4],  # num_channels (input and output), kernel_size, stride\n",
    "                   [6, 3, 4]], 3, [128, 128, 1], \n",
    "                   \"permuted_hard_tiny.dict\", fc_layers=[]).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "permuted_optim = torch.optim.Adam(permuted_net.parameters())\n",
    "print(permuted_net.num_params())\n",
    "permuted_net.load_model_state_dict(optim=permuted_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088e7f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_large_net = ResNet([[16, 3, 1],  # num_channels (input and output), kernel_size, stride\n",
    "                      [32, 3, 1]], 3, [128, 128, 1], \n",
    "                   \"permuted_hard_large.dict\", fc_layers=[]).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()  # dont start from same initialization\n",
    "permuted_large_optim = torch.optim.Adam(permuted_large_net.parameters())\n",
    "print(permuted_large_net.num_params())\n",
    "permuted_large_net.load_model_state_dict(optim=permuted_large_optim)\n",
    "#set_initializers(permuted_large_net, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d1924",
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_large_net2 = ResNet([[16, 3, 1],  # num_channels (input and output), kernel_size, stride\n",
    "                      [32, 3, 1]], 3, [128, 128, 1], \n",
    "                   \"permuted_hard_large2.dict\", fc_layers=[]).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()  # dont start from same initialization\n",
    "permuted_large_optim2 = torch.optim.Adam(permuted_large_net2.parameters())\n",
    "permuted_large_net2.load_model_state_dict(optim=permuted_large_optim2)\n",
    "#set_initializers(permuted_large_net2, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc95a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_large_net3 = ResNet([[16, 3, 1],  # num_channels (input and output), kernel_size, stride\n",
    "                      [32, 3, 1]], 3, [128, 128, 1], \n",
    "                   \"permuted_hard_large3.dict\", fc_layers=[]).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()  # dont start from same initialization\n",
    "permuted_large_optim3 = torch.optim.Adam(permuted_large_net3.parameters())\n",
    "permuted_large_net3.load_model_state_dict(optim=permuted_large_optim3)\n",
    "#set_initializers(permuted_large_net2, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e05bda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_noise_net = ResNet([[2, 3, 4],  # num_channels (input and output), kernel_size, stride\n",
    "                   [6, 3, 4]], 3, [128, 128, 1], \n",
    "                   \"noise_net_hard_tiny_low_p.dict\", fc_layers=[]).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "low_noise_optim = torch.optim.Adam(low_noise_net.parameters())\n",
    "print(low_noise_net.num_params())\n",
    "low_noise_net.load_model_state_dict(optim=low_noise_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1317dc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_noise_net = ResNet([[2, 3, 4],  # num_channels (input and output), kernel_size, stride\n",
    "                   [6, 3, 4]], 3, [128, 128, 1], # p = 0.25\n",
    "                   \"noise_net_hard_tiny_tiny_p.dict\", fc_layers=[]).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "tiny_noise_optim = torch.optim.Adam(tiny_noise_net.parameters())\n",
    "print(tiny_noise_net.num_params())\n",
    "tiny_noise_net.load_model_state_dict(optim=tiny_noise_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d515e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(tiny_noise_net, loss_func, valid_loader, device=device)\n",
    "# with squares intact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0096fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(permuted_net, loss_func, valid_loader, device=device) # sample size 20k\n",
    "# small net, small sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26570b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(permuted_net, loss_func, test_loader, device=device) # larger sample size (300k)\n",
    "# small net, large sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c22b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(permuted_large_net, loss_func, test_loader, device=device) \n",
    "# finite dataset, gain 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9af97f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(permuted_large_net2, loss_func, test_loader, device=device) \n",
    "# infinite dataset, gain 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c27f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(permuted_large_net3, loss_func, test_loader, device=device) \n",
    "# finite dataset, gain 0.01 (weights are saved as permuted_large4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d96cb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(permuted_large_net3, loss_func, test_loader, device=device) \n",
    "# finite dataset, gain 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7859e3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(permuted_large_net2, permuted_large_optim2, loss_func, 1000, train_loader, valid_loader, device=device)\n",
    "# infinite data, gain 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d48a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train(permuted_large_net, permuted_large_optim, loss_func, 1000, train_loader, valid_loader, device=device)\n",
    "# finite data test, gain 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d0a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(permuted_large_net3, permuted_large_optim3, loss_func, 1000, train_loader, valid_loader, device=device)\n",
    "# finite dataset, gain 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f2556",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(permuted_large_net3, permuted_large_optim3, loss_func, 1000, train_loader, valid_loader, device=device)\n",
    "# finite dataset, gain 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b95436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train(permuted_large_net, permuted_large_optim, loss_func, 1000, train_loader, valid_loader, device=device)\n",
    "# infinite data test (no initializiation changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b66256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train(permuted_net, permuted_optim, loss_func, 1000, train_loader, valid_loader, device=device)\n",
    "# small net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba6c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train(noise_net, noise_optim, loss_func, 1000, train_loader, valid_loader, device=device)\n",
    "# trained with squares still visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc2374",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train(low_noise_net, low_noise_optim, loss_func, 1000, train_loader, valid_loader, device=device)\n",
    "# trained with squares still visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b822eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train(tiny_noise_net, tiny_noise_optim, loss_func, 1000, train_loader, valid_loader, device=device)\n",
    "# trained with squares still visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cbd1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_net.eval()\n",
    "avg_img = np.ones((valid_set.size, valid_set.size))\n",
    "tensor_avg_img = tensorize(avg_img, device=device)\n",
    "responses = []\n",
    "for color in np.arange(255):\n",
    "    tensor_avg_img[...] = color\n",
    "    responses.append(permuted_net(tensor_avg_img).detach().cpu().numpy())\n",
    "responses = np.asarray(responses).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24262db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    plt.plot(np.arange(255), responses[:,i], label=f\"logit {i}\")\n",
    "plt.legend()\n",
    "plot_color_classes(valid_set, (responses.min(), responses.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da72635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def averaging_test(dataset, sample, edge_width=10):\n",
    "    avg_area = np.pi/3*(dataset.radius[1]**2+dataset.radius[0]**2+dataset.radius[0]*dataset.radius[1])\n",
    "    pct_area = avg_area / (dataset.size**2)\n",
    "    print(f\"Targets are on average {pct_area:.1%} of the image\")\n",
    "    other_points = []\n",
    "    \n",
    "    total_answered = 0\n",
    "    right_calibrated = 0\n",
    "    right_naive = 0\n",
    "    right_color_set = 0\n",
    "    right_base = 0\n",
    "    right_edge_set = 0\n",
    "    right_background_set = 0\n",
    "    \n",
    "    avg_img = np.ones((dataset.size, dataset.size))\n",
    "    tensor_avg_img = tensorize(avg_img, device=device)\n",
    "    for _ in tqdm(range(sample)):\n",
    "        img_gen, lbl, color, *_ = dataset.generate_one()\n",
    "        color = color[0]\n",
    "        foreground_mask = np.where(img_gen>2)\n",
    "        other_space = img_gen[(img_gen > 2) & (img_gen != color)].sum() / foreground_mask[0].shape[0]\n",
    "        \n",
    "        prediction = (img_gen[foreground_mask].mean() - 36.9)/(1-36.9/128)\n",
    "        if np.isnan(prediction) or np.isnan(other_space):\n",
    "            continue\n",
    "        tensor_avg_img[...] = color  # color setting\n",
    "        color_set_classif = permuted_net(tensor_avg_img).argmax()\n",
    "        \n",
    "        tensor_avg_img[...] = img_gen.mean()  # naive averaging\n",
    "        naive_classif = permuted_net(tensor_avg_img).argmax()\n",
    "        \n",
    "        tensor_avg_img[...] = prediction  # calibrated averaging\n",
    "        calibrated_classif = permuted_net(tensor_avg_img).argmax()\n",
    "        \n",
    "        tensor_img_gen = tensorize(img_gen, device=device)\n",
    "        base_classif = permuted_net(tensor_img_gen).argmax() # regular classification\n",
    "        \n",
    "        tensor_img_gen[tensor_img_gen == 0] = (color + 30) % 255 # set background to a different class\n",
    "        background_set_classif = permuted_net(tensor_img_gen).argmax()\n",
    "        \n",
    "        # edge set test (set to color since thats the best results)\n",
    "        tensor_avg_img[...] = 0\n",
    "        tensor_avg_img[0,0, 0:edge_width] = color\n",
    "        tensor_avg_img[0,0, -edge_width:] = color\n",
    "        tensor_avg_img[0,0,:, 0:edge_width] = color\n",
    "        tensor_avg_img[0,0,:, -edge_width:] = color\n",
    "        edge_set_classif = permuted_net(tensor_avg_img).argmax()\n",
    "        \n",
    "        total_answered += 1\n",
    "        right_base += lbl.argmax() == base_classif\n",
    "        right_background_set += lbl.argmax() == background_set_classif\n",
    "        right_edge_set += lbl.argmax() == edge_set_classif\n",
    "        right_calibrated += lbl.argmax() == calibrated_classif\n",
    "        right_naive += lbl.argmax() == naive_classif\n",
    "        right_color_set += lbl.argmax() == color_set_classif\n",
    "    print(f\"Calibrated got {right_calibrated/total_answered:.2%} correct\")\n",
    "    print(f\"Naive got {right_naive/total_answered:.2%} correct\")\n",
    "    print(f\"Color setting got {right_color_set/total_answered:.2%} correct\")\n",
    "    print(f\"Edge setting got {right_edge_set/total_answered:.2%} correct\")\n",
    "    print(f\"Background setting got {right_background_set/total_answered:.2%} correct\")\n",
    "    print(f\"Base got {right_base/total_answered:.2%} correct\")\n",
    "    \n",
    "result = averaging_test(valid_set, 100_000)\n",
    "# PCA map to see edge behaviour (average a bunch of them?)\n",
    "# color set edge test\n",
    "# background only set test? (do it maliciously) (see how badly it hurts performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0790b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_by_color(dataset, sample=100_000):\n",
    "    points = []\n",
    "    avg_area = np.pi/3*(dataset.radius[1]**2+dataset.radius[0]**2+dataset.radius[0]*dataset.radius[1])\n",
    "    pct_area = avg_area / (dataset.size**2)\n",
    "    print(f\"Targets are on average {pct_area:.1%} of the image\")\n",
    "    other_points = []\n",
    "    total_answered = 0\n",
    "    right_calibrated = 0\n",
    "    right_naive = 0\n",
    "    right_really_naive = 0\n",
    "    for _ in tqdm(range(sample)):\n",
    "        img_gen, lbl, color, *_ = dataset.generate_one()\n",
    "        #prediction = np.minimum(img_gen/pct_area, 255)\n",
    "        foreground_mask = np.where(img_gen>2)\n",
    "        other_space = img_gen[(img_gen > 2) & (img_gen != color)].sum() / foreground_mask[0].shape[0]\n",
    "        #print(len(foreground_mask[0]), img_gen[(img_gen > 2) & (img_gen != color)].size)\n",
    "        # model: avg = color*(1-pct) + 128*pct\n",
    "        # calculate pct by figuring out the average sum of non-target non-background pixels\n",
    "        # divided by the size of the non-background area => gives you 128*pct\n",
    "        \n",
    "        prediction = (img_gen[foreground_mask].mean() - 36.9)/(1-36.9/128)\n",
    "        if np.isnan(prediction) or np.isnan(other_space):\n",
    "            continue\n",
    "        total_answered += 1\n",
    "        right_calibrated += lbl.argmax() == color_classifier(prediction)\n",
    "        right_naive += lbl.argmax() == color_classifier(img_gen[foreground_mask].mean())\n",
    "        right_really_naive += lbl.argmax() == color_classifier(img_gen.mean()/pct_area)\n",
    "        points.append((color, prediction))\n",
    "        other_points.append((color, other_space))\n",
    "    print(f\"Calibrated got {right_calibrated/total_answered:.2%} correct\")\n",
    "    print(f\"Naive got {right_naive/total_answered:.2%} correct\")\n",
    "    print(f\"Really naive got {right_really_naive/total_answered:.2%} correct\")\n",
    "\n",
    "    return np.asarray(points), np.asarray(other_points)\n",
    "result = error_by_color(valid_set, sample=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cacc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(result[1][:,0], result[1][:,1], s=0.05)\n",
    "plt.plot(np.arange(255), c=\"r\")\n",
    "result[1][:,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff6084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(result[0][:,0], result[0][:,1], s=0.05)\n",
    "plt.plot(np.arange(255), c=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da112e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_net = AllActivations(permuted_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7210c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5_123_456)\n",
    "test_img, lbl, color, size, *_  = valid_set.generate_one()\n",
    "print(color)\n",
    "plt.imshow(test_img, cmap=\"gray\")\n",
    "tensor_test_img = tensorize(test_img, device=device)\n",
    "# with p=0.8 (ignore this, switched away from this approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e88b017",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5_123_456)\n",
    "test_img, lbl, color, size, pos  = valid_set.generate_one()\n",
    "print(color)\n",
    "plt.imshow(test_img, cmap=\"gray\")\n",
    "# with p = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5_123_456)\n",
    "test_img, lbl, color, size, *pos  = valid_set.generate_one()\n",
    "print(color)\n",
    "plt.imshow(test_img, cmap=\"gray\")\n",
    "tensor_test_img = tensorize(test_img, device=device)\n",
    "# with p = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9785a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_net.eval()\n",
    "interp_net(tensor_test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "block = 1\n",
    "\n",
    "#uniform_inpt = torch.full((1,16,32,32), 100.0).to(device)\n",
    "#plt.imshow(tiny_net.conv_blocks[0].conv2.weight[c, in_c].detach().cpu().numpy(), cmap=\"bwr\")\n",
    "conv_maps = tiny_net.conv_blocks[block].conv2.weight[c, :]\n",
    "#imshow_centered_colorbar(conv_maps[7].detach().cpu().numpy(), cmap=\"bwr\")\n",
    "conv_scale = conv_maps.max(axis=-1).values.max(axis=-1).values\n",
    "conv_shift = tiny_net.conv_blocks[block].conv2.bias[c]\n",
    "bn_scale = tiny_net.conv_blocks[block].batch_norm2.weight[c]\n",
    "bn_shift = tiny_net.conv_blocks[block].batch_norm2.bias[c]\n",
    "bn_var = tiny_net.conv_blocks[block].batch_norm2.running_var[c]\n",
    "bn_mean = tiny_net.conv_blocks[block].batch_norm2.running_mean[c]\n",
    "print(conv_shift, bn_scale, bn_shift, bn_var, bn_mean)\n",
    "#(c*conv_scale + conv_shift - bn_mean) / torch.sqrt(bn_var) * bn_scale + bn_shift\n",
    "slope = (conv_scale/torch.sqrt(bn_var)*bn_scale).detach().cpu().numpy()\n",
    "bias = ((conv_shift - bn_mean)/torch.sqrt(bn_var)*bn_scale + bn_shift).detach().cpu().numpy()\n",
    "\n",
    "lines = np.asarray([profile_plots[f\"conv_blocks.{block}.act_func1_{x}\"][0] for x in range(6)])\n",
    "\n",
    "uniform_scaling = slope.dot(lines) + bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09b7313",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.maximum(uniform_scaling, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4287d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_net.eval()\n",
    "profile_plots,_ = activation_color_profile(AllActivations(noise_net), valid_loader, valid_set, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83342d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_noise_net.eval()\n",
    "low_profile_plots,_ = activation_color_profile(AllActivations(low_noise_net), valid_loader, valid_set, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105936aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_noise_net.eval()\n",
    "tiny_profile_plots,_ = activation_color_profile(AllActivations(tiny_noise_net), valid_loader, valid_set, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d20747",
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_net.eval()\n",
    "permuted_plots,_ = activation_color_profile(AllActivations(permuted_net), valid_loader, valid_set, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e96bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "show_profile_plot(low_profile_plots[\"conv_blocks.1.act_func2_4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657cba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_profile_plot(profile_plots[\"conv_blocks.1.act_func2_4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_weights(interp_net, \"conv_blocks.1.act_func1\", color_profile=tiny_profile_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95846d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_weights(interp_net, \"conv_blocks.1.act_func2\", color_profile=tiny_profile_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09141873",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(interp_net, \"conv_blocks.1.act_func2\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32957346",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_fc_conv(interp_net, color_profile=tiny_profile_plots, fixed_height=True, full_gridspec=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e26be1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_net.fully_connected[0].fully_connected.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc0b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_fc_conv(interp_net, color_profile=profile_plots, fixed_height=False, full_gridspec=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab19196",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "feature_gram, projected_weights = visualizations.fc_conv_feature_angles(noise_net, \n",
    "                            \"fully_connected.0.act_func\", num_embed=3, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5824af",
   "metadata": {},
   "source": [
    "# Permuted Pixels (no squares) networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4db5959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e388b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_img[valid_set.random_permutes[10][...,0][::-1, ::-1], valid_set.random_permutes[10][...,1][::-1, ::-1]], cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf19642",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(orig_img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a3e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5_13_46)\n",
    "test_img, lbl, color, size, pos, noise, orig_img  = valid_set.generate_one()\n",
    "print(color)\n",
    "\n",
    "plt.figure(figsize=(12,16))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_img, cmap=\"gray\")\n",
    "plt.subplot(1,2,2)\n",
    "denoised_img = np.where(test_img == color, color, 0)\n",
    "plt.imshow(denoised_img, cmap=\"gray\")\n",
    "\n",
    "tensor_test_img = tensorize(test_img, device=device)\n",
    "denoised_tensor_img = tensorize(denoised_img, device=device)\n",
    "\n",
    "interp_net = AllActivations(permuted_net)\n",
    "interp_net.eval()\n",
    "interp_net(tensor_test_img)\n",
    "\n",
    "de_interp_net = AllActivations(permuted_net)\n",
    "de_interp_net.eval()\n",
    "de_interp_net(denoised_tensor_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b05a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_weights(interp_net, \"conv_blocks.0.act_func1\", color_profile=permuted_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21100dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(interp_net, \"conv_blocks.0.act_func1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8383a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_weights(interp_net, \"conv_blocks.0.act_func2\", color_profile=permuted_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8496bee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(interp_net, \"conv_blocks.0.act_func2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(de_interp_net, \"conv_blocks.0.act_func2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95391ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(interp_net.model.conv_blocks[1].conv1.bias)\n",
    "show_conv_weights(interp_net, \"conv_blocks.1.act_func1\", color_profile=permuted_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7de39ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(interp_net, \"conv_blocks.1.act_func1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5f0912",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(de_interp_net, \"conv_blocks.1.act_func1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140be7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_weights(interp_net, \"conv_blocks.1.act_func2\", color_profile=permuted_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24567e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(interp_net, \"conv_blocks.1.act_func2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5587be58",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(de_interp_net, \"conv_blocks.1.act_func2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421bcda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_fc_conv(interp_net, color_profile=permuted_plots, fixed_height=True, full_gridspec=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e881748",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(interp_net, \"conv_blocks.1.act_func1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a54c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(interp_net, \"conv_blocks.1.act_func2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c97db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(de_interp_net, \"conv_blocks.1.act_func2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e21661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_mapper = get_weight(interp_net, \"fully_connected.0.fully_connected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ed68e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_large_net.final_img_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483523e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_weights(interp_net, \"conv_blocks.0.act_func2\", color_profile=permuted_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c1d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(interp_net, \"conv_blocks.0.act_func2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c4db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_weights(interp_net, \"conv_blocks.1.act_func1\", color_profile=permuted_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fa4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(interp_net, \"conv_blocks.1.act_func1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5310ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_weights(interp_net, \"conv_blocks.1.act_func2\", color_profile=permuted_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36fa0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(interp_net, \"conv_blocks.1.act_func2\")\n",
    "# uniform image of average and pass into network\n",
    "# pasting images onto each other\n",
    "# send in images that have only target pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7456f481",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_fc_conv(interp_net, color_profile=permuted_plots, fixed_height=True, full_gridspec=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe3a100",
   "metadata": {},
   "source": [
    "# PCA Direction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015647ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_scales = [3,5,7,9,13,15]\n",
    "if 0: \n",
    "    %store -r noise_back_pca_directions_1_stride noise_back_pca_directions_s_stride\n",
    "else:\n",
    "    noise_back_pca_directions_1_stride = find_pca_directions(valid_set, 4096, default_scales, 1)\n",
    "    noise_back_pca_directions_s_stride = find_pca_directions(valid_set, 4096, default_scales, default_scales)\n",
    "    %store noise_back_pca_directions_1_stride noise_back_pca_directions_s_stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9758c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pca_directions(noise_back_pca_directions_s_stride, \"Strides=scales\", default_scales, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef2b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1_2123, 1_40_124, 1_508_559, 1_5_019_258, 1_2_429_852, 9032, 5832, 12, 5014, 92, 42, 52, \n",
    "         52_934, 935_152, 1_000_000, 1_000_001, 27, 24, 512, 999_105]  # 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5312b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_map_s_strides, _, grad_maps, explain_imgs = generate_many_pca(permuted_net, seeds, \n",
    "                noise_back_pca_directions_1_stride, default_scales, valid_set, component=0, \n",
    "                batch_size=512, strides=3, skip_1_stride=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90178548",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_grid_figure([explain_imgs, pca_map_s_strides, grad_maps], transpose=True, titles=[\"Image\", \"Strides=3\", \"Gradient\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
