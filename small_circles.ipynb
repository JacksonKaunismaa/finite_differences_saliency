{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "from color_regions import *\n",
    "from network import *\n",
    "from visualizations import *\n",
    "from utils import *\n",
    "from hooks import *\n",
    "from config_objects import *\n",
    "from training import *\n",
    "\n",
    "# set up autoreloading of shared code\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport color_regions,network,visualizations,utils,hooks,config_objects,training\n",
    "%aimport\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4eadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = (0, 250_000) # size of training set\n",
    "valid_indices = (1_250_000, 1_275_000)\n",
    "test_indices = (3_260_000, 3_560_000)\n",
    "\n",
    "critical_color_values = list(range(0,241,30))\n",
    "\n",
    "dset_config = ColorDatasetConfig(task_difficulty=\"hard\",\n",
    "                                 noise_size=(1,9),\n",
    "                                 num_classes=3,\n",
    "                                 num_objects=1,  # => permuted\n",
    "                                 radius=(1/8., 1/7.),\n",
    "                                 device=device,\n",
    "                                 batch_size=128)\n",
    "\n",
    "# copies the config each time\n",
    "train_set = ColorDatasetGenerator(train_indices, dset_config)\n",
    "valid_set = ColorDatasetGenerator(valid_indices, dset_config)\n",
    "test_set = ColorDatasetGenerator(test_indices, dset_config)\n",
    "# train_set.cfg.infinite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bbf7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"hard\" task\n",
    "color_probe = np.linspace(0, 255, 255)\n",
    "color_class = [color_classifier(x) for x in color_probe]\n",
    "plt.plot(color_probe, color_class)\n",
    "plt.xlabel(\"Color\")\n",
    "plt.yticks([0, 1, 2])\n",
    "plt.ylabel(\"Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7aebda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_config = ExperimentConfig(layer_sizes=[[2, 3, 4], [6, 3, 4]], \n",
    "                                    learn_rate=0.01, weight_decay=2e-03, \n",
    "                                    gain=0.05, epochs=50)\n",
    "tiny_net = ResNet(\"tiny_net_small_circles.dict\", tiny_config, dset_config).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "tiny_optim = torch.optim.Adam(tiny_net.parameters())\n",
    "print(tiny_net.num_params())\n",
    "tiny_net.load_model_state_dict(optim=tiny_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e17f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "unstrided_config = ExperimentConfig(layer_sizes=[[16, 3, 1], [32, 3, 1]], \n",
    "                                    learn_rate=0.01, weight_decay=2e-03, \n",
    "                                    gain=0.05, epochs=50)\n",
    "\n",
    "# unstrided_net = ResNet(\"models/corrected_unstrided_small_circles.dict\", unstrided_config, dset_config).to(device)\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "# unstrided_optim = torch.optim.Adam(unstrided_net.parameters())\n",
    "# print(unstrided_net.num_params())\n",
    "# unstrided_net.load_model_state_dict(optim=unstrided_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e659295",
   "metadata": {},
   "outputs": [],
   "source": [
    "unstrided_net = ResNet(\"small_circles_unpermuted/0.0001_8.858667904100833e-07_True_medium_size_0.1_25.dict\",\n",
    "                      unstrided_config, dset_config)\n",
    "unstrided_net.load_model_state_dict()\n",
    "unstrided_net.to(dset_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31bc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(tiny_net, nn.CrossEntropyLoss(), test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a194d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(unstrided_net, nn.CrossEntropyLoss(), test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd529ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "old_config = copy.copy(unstrided_config)\n",
    "old_config.global_avg_pooling = False\n",
    "old_unstrided_net = ResNet(\"models/unstrided_small_circles.dict\",\n",
    "                      unstrided_config, dset_config)\n",
    "print(old_unstrided_net)\n",
    "old_unstrided_net.load_model_state_dict()\n",
    "old_unstrided_net.to(dset_config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6706d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unstrided_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936cf267",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train(unstrided_net, unstrided_optim, loss_func, 40, train_loader, valid_loader, device=device,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538eb04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(unstrided_net, loss_func, valid_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(tiny_net, loss_func, valid_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba6c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train(tiny_net, tiny_optim, loss_func, 1000, train_loader, valid_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da112e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_net = AllActivations(tiny_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7210c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5_123_456)\n",
    "test_img, lbl, color, *size, pos  = valid_set.generate_one()\n",
    "print(color)\n",
    "plt.imshow(test_img, cmap=\"gray\")\n",
    "tensor_test_img = tensorize(test_img, device=device)\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9785a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_net.eval()\n",
    "interp_net(tensor_test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7d2fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_conv = interp_net._features[\"conv_blocks.0.conv1\"].detach().cpu().numpy().squeeze()\n",
    "first_conv_weights = dict(tiny_net.named_modules())[\"conv_blocks.0.conv1\"].weight.detach().cpu().numpy().squeeze()\n",
    "print(dict(tiny_net.named_modules())[\"conv_blocks.0.conv1\"].bias)\n",
    "fig = plt.figure(figsize=(4*2, 5*2))\n",
    "plt.subplot(3,2,1)\n",
    "imshow_centered_colorbar(test_img, \"bwr\", \"original_image\")\n",
    "plt.subplot(3,2,3)\n",
    "imshow_centered_colorbar(first_conv[0], \"bwr\", \"output conv1_0.0\")\n",
    "plt.subplot(3,2,4)\n",
    "imshow_centered_colorbar(first_conv[1], \"bwr\", \"output conv1_0.1\")\n",
    "plt.subplot(3,2,5)\n",
    "imshow_centered_colorbar(first_conv_weights[0], \"bwr\", \"weights of conv1_0.0\")\n",
    "plt.subplot(3,2,6)\n",
    "imshow_centered_colorbar(first_conv_weights[1], \"bwr\", \"weights of conv1_0.1\")\n",
    "# => first layer basically just computes a compressed version of original, twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e446cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn1_params = dict(tiny_net.named_modules())[\"conv_blocks.0.batch_norm1\"]\n",
    "print(bn1_params.weight, bn1_params.bias)\n",
    "print(bn1_params.running_mean, bn1_params.running_var)\n",
    "first_batchnorms = interp_net._features[\"conv_blocks.0.batch_norm1\"].detach().cpu().numpy().squeeze()\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "plt.subplot(2,2,1)\n",
    "imshow_centered_colorbar(first_conv[0], \"bwr\", \"output conv1_0.0\")\n",
    "plt.subplot(2,2,3)  # conv{1,2}_{layer_num}.{channel_index}\n",
    "imshow_centered_colorbar(first_conv[1], \"bwr\", \"output of conv1_0.1\")\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "imshow_centered_colorbar(first_batchnorms[0], \"bwr\", \"output batchnorm1_0.0\")\n",
    "plt.subplot(2,2,4)\n",
    "imshow_centered_colorbar(first_batchnorms[1], \"bwr\", \"output batchnorm1_0.1\")\n",
    "# conv of circle (which we just preserve its shape with our conv1) must exceed\n",
    "# the bias else it gets zero-ed out => gives us 1 boundary on the color. \n",
    "# eg. look at channel 1. we multiply the raw value by 7.5, and then subtract 270\n",
    "# (note that the bias on channel 1 is basically 0), and divide by 553\n",
    "# then multiply by 1, and subtract 0.8176 => any color value above -23 will be > 0\n",
    "# for channel 0, it turns out any color value above +25 will be > 0 => already\n",
    "# separating on that first non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8fa12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_conv = interp_net._features[\"conv_blocks.0.conv2\"].detach().cpu().numpy().squeeze()\n",
    "second_conv_weights = dict(tiny_net.named_modules())[\"conv_blocks.0.conv2\"].weight.detach().cpu().numpy().squeeze()\n",
    "fig = plt.figure(figsize=(6, 7))\n",
    "\n",
    "for m in range(2):\n",
    "    plt.subplot(3,2,m+1)\n",
    "    imshow_centered_colorbar(second_conv[m], \"bwr\", f\"out conv2_1.{m}\")\n",
    "    plt.subplot(3,2,m+3)\n",
    "    imshow_centered_colorbar(second_conv_weights[m][0], \"bwr\", f\"w 2_0.0->{m}\")\n",
    "    plt.subplot(3,2,m+5)\n",
    "    imshow_centered_colorbar(second_conv_weights[m][1], \"bwr\", f\"w 2_0.1->{m}\")\n",
    "# both paths have a \"just recompute/compress the image (identity mapping learned?)\", though\n",
    "# 1 shifts it up a bit (not sure how relevant this is, but you can actually see it in the image)\n",
    "# very curve detector-like filters as well in both paths\n",
    "# so channel 0 is upper-right curves, unsure what the bright pixel in lower left of w_2_0.0 is\n",
    "# but the other path doesn't have it, so maybe not important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "block = 1\n",
    "\n",
    "#uniform_inpt = torch.full((1,16,32,32), 100.0).to(device)\n",
    "#plt.imshow(tiny_net.conv_blocks[0].conv2.weight[c, in_c].detach().cpu().numpy(), cmap=\"bwr\")\n",
    "conv_maps = tiny_net.conv_blocks[block].conv2.weight[c, :]\n",
    "#imshow_centered_colorbar(conv_maps[7].detach().cpu().numpy(), cmap=\"bwr\")\n",
    "conv_scale = conv_maps.max(axis=-1).values.max(axis=-1).values\n",
    "conv_shift = tiny_net.conv_blocks[block].conv2.bias[c]\n",
    "bn_scale = tiny_net.conv_blocks[block].batch_norm2.weight[c]\n",
    "bn_shift = tiny_net.conv_blocks[block].batch_norm2.bias[c]\n",
    "bn_var = tiny_net.conv_blocks[block].batch_norm2.running_var[c]\n",
    "bn_mean = tiny_net.conv_blocks[block].batch_norm2.running_mean[c]\n",
    "print(conv_shift, bn_scale, bn_shift, bn_var, bn_mean)\n",
    "#(c*conv_scale + conv_shift - bn_mean) / torch.sqrt(bn_var) * bn_scale + bn_shift\n",
    "slope = (conv_scale/torch.sqrt(bn_var)*bn_scale).detach().cpu().numpy()\n",
    "bias = ((conv_shift - bn_mean)/torch.sqrt(bn_var)*bn_scale + bn_shift).detach().cpu().numpy()\n",
    "\n",
    "lines = np.asarray([profile_plots[f\"conv_blocks.{block}.act_func1_{x}\"][0] for x in range(6)])\n",
    "\n",
    "uniform_scaling = slope.dot(lines) + bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4287d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unstrided_net.eval()\n",
    "profile_plots,_ = activation_color_profile(AllActivations(unstrided_net), valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf18114",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_net.eval()\n",
    "profile_plots,_ = activation_color_profile(AllActivations(tiny_net), valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194df3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_weights(tiny_net, \"conv_blocks.0.act_func1\", color_profile=profile_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c0ab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_weights(interp_net, \"conv_blocks.0.act_func1\", color_profile=profile_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31359ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_weights(interp_net, \"conv_blocks.0.act_func2\", color_profile=profile_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_weights(interp_net, \"conv_blocks.1.act_func1\", color_profile=profile_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "show_profile_plots(profile_plots, \"conv_blocks.1.act_func2\", size_mul=0.7, \n",
    "                   fixed_height=False, rm_border=False, hide_ticks=False)\n",
    "plt.savefig(\"mid_conv_intensity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95846d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_weights(interp_net, \"conv_blocks.1.act_func2\", color_profile=profile_plots, size_mul=2.67, fixed_height=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09141873",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_conv_layer(interp_net, \"conv_blocks.1.act_func2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c7b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_conv = interp_net._features[\"conv_blocks.1.conv1\"].detach().cpu().numpy().squeeze()\n",
    "second_conv_weights = dict(tiny_net.named_modules())[\"conv_blocks.1.conv1\"].weight.detach().cpu().numpy().squeeze()\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "\n",
    "for m in range(6):\n",
    "    plt.subplot(3,6,m+1)\n",
    "    imshow_centered_colorbar(second_conv[m], \"bwr\", f\"out conv1_1.{m}\")\n",
    "    plt.subplot(3,6,m+7)\n",
    "    imshow_centered_colorbar(second_conv_weights[m][0], \"bwr\", f\"w 1_1.0->{m}\")\n",
    "    plt.subplot(3,6,m+13)\n",
    "    imshow_centered_colorbar(second_conv_weights[m][1], \"bwr\", f\"w 1_1.1->{m}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cda2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn3_params = dict(tiny_net.named_modules())[\"conv_blocks.1.batch_norm1\"]\n",
    "print(bn3_params.weight, bn3_params.bias)\n",
    "print(bn3_params.running_mean, bn3_params.running_var)\n",
    "third_batchnorms = interp_net._features[\"conv_blocks.1.batch_norm1\"].detach().cpu().numpy().squeeze()\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "for m in range(6):\n",
    "    plt.subplot(2,6,m+1)\n",
    "    imshow_centered_colorbar(second_conv[m], \"bwr\", f\"output conv1_1.{m}\")\n",
    "    plt.subplot(2,6,m+7)\n",
    "    imshow_centered_colorbar(third_batchnorms[m], \"bwr\", f\"output batchnorm1_1.{m}\")\n",
    "# it appears the only important channel at this point is 2. channels 0,1 looks like it was\n",
    "# close to being important, but failed some color check. channel 5 I don't really\n",
    "# understand since it appears to have picked up some signal that wasnt there before?\n",
    "# (I suppose the mean is negative, and the scale is larger than 1 so it would expand any\n",
    "# slight differences that existed but weren't visible?). Channel 4 I think is also trying\n",
    "# to be a circle finder (upper right?), but failed color check as well. Channel 3 is also\n",
    "# looking like it just barely failed the color check. Actually, looking at channel 1 again, \n",
    "# its output after a ReLU I expect would look exactly like channel 4 right now, so\n",
    "# channel 4 is definitely a \"failed color check\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b0304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_conv = interp_net._features[\"conv_blocks.1.conv2\"].detach().cpu().numpy().squeeze()\n",
    "second_conv_weights = dict(tiny_net.named_modules())[\"conv_blocks.1.conv2\"].weight.detach().cpu().numpy().squeeze()\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "for m in range(6):\n",
    "    plt.subplot(7,6,m+1)\n",
    "    imshow_centered_colorbar(second_conv[m], \"bwr\", f\"out conv2_1.{m}\")\n",
    "    for k in range(6):\n",
    "        plt.subplot(7,6,m+1+(k+1)*6)\n",
    "        imshow_centered_colorbar(second_conv_weights[m][k], \"bwr\", f\"w 2_1.{k}->{m}\",\n",
    "                                colorbar=True)\n",
    "    # Channel 0 is basically saying \"cancel out everything except for channel 4 in prev layer\"\n",
    "    # So it should basically copy its value (which it does). Channel 2 is similar, though it \n",
    "    # appears to copy from channel 1, and 4 a bit. At the end of it, channel 4 ends\n",
    "    # up being the most active, since it has that strong positive edge detector with \n",
    "    # channel 2 in the previous layer. Channel 1 also does decently well, but its circle\n",
    "    # has been thoroughly zeroed out, and only an \"artifact-like\" row of brightness \n",
    "    # remains at the top edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e98608",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn4_params = dict(tiny_net.named_modules())[\"conv_blocks.1.batch_norm2\"]\n",
    "print(bn4_params.weight, bn4_params.bias)\n",
    "print(bn4_params.running_mean, bn4_params.running_var)\n",
    "fourth_batchnorms = interp_net._features[\"conv_blocks.1.batch_norm2\"].detach().cpu().numpy().squeeze()\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "for m in range(6):\n",
    "    plt.subplot(2,6,m+1)\n",
    "    imshow_centered_colorbar(second_conv[m], \"bwr\", f\"output conv2_1.{m}\")\n",
    "    plt.subplot(2,6,m+7)\n",
    "    imshow_centered_colorbar(fourth_batchnorms[m], \"bwr\", f\"output batchnorm2_1.{m}\")\n",
    "# so again, we see somewhat of a \"direction reversal\" in channel5 (pretty much because of \n",
    "# the positive bias (compared to the other biases, which are all negative), but channel 4 mostly\n",
    "# seems to be the winner here. The \"artifact\" bright top row of channel 1 is mostly negated, \n",
    "# (we actually see those weird rows in multiple conv maps here, could be an artifact\n",
    "# of the padding/striding method maybe?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d9ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_weights = dict(tiny_net.named_modules())[\"fully_connected.0.fully_connected\"].weight.detach().cpu().numpy().squeeze()\n",
    "fig = plt.figure(figsize=(14, 7))\n",
    "\n",
    "for m in range(6):\n",
    "    plt.subplot(4,6,m+1)\n",
    "    imshow_centered_colorbar(fourth_batchnorms[m], \"bwr\", f\"out batchnorm2_1.{m}\") # no ReLU\n",
    "    for fc_m in range(3):\n",
    "        fc_shaped = fc_weights[fc_m].reshape(6,8,8)[m]\n",
    "        result = (np.where(fourth_batchnorms[m]>0, fourth_batchnorms[m], 0)*fc_shaped).sum()\n",
    "        plt.subplot(4,6,m+1+(fc_m+1)*6)\n",
    "        imshow_centered_colorbar(fc_shaped, \"bwr\", f\"{result:.2f}\")\n",
    "    # the stupid edge lines actually seem to be getting used somehow (see bottom row, which\n",
    "    # is used to predict class 2). Some of these maps are just \"find a circle-ish thing in \n",
    "    # the center\". Probably makes sense that the \"best\" place to put your circle checker is \n",
    "    # right in the middle, because most circles are at least overlapping the middle, due\n",
    "    # to the data generation process. Some of these maps appear to do nothing, eg.\n",
    "    # the map for predicting class 1 ignores channel 4. Although maybe there is some\n",
    "    # \"antipodal\" symmetry between class 1 channel 4 and class 0 channel 4 => channel 4 \n",
    "    # gives a lot of info for class 1??, though im not sure why you would only highlight\n",
    "    # one pixel inside them (we see the same pattern used in channel 2, and actually in a lot of\n",
    "    # the channels) => channel 0 is like \"positive evidence for class 1, negative evidence for\n",
    "    # class 0\"\n",
    "    \n",
    "    # note that its actually the same classes that are in superpositon:\n",
    "    # eg. for class 0,1 we have superposition in channel 0, channel 4\n",
    "    #     for class 0,2 we have superpositon in channel 1,2,3,5\n",
    "    \n",
    "    # also we arguably have a \"1-map\" type thing occuring in many of the channels. For example,\n",
    "    # in channel 4, it sort of looks like that for class 0 and class 2, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32957346",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_fc_conv(interp_net, color_profile=profile_plots, fixed_height=True, full_gridspec=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e26be1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_net.fully_connected[0].fully_connected.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc0b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.figure(figsize=(8,8))\n",
    "show_fc_conv(interp_net, color_profile=profile_plots, fixed_height=True, full_gridspec=True)\n",
    "plt.savefig(\"intensity_profile.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab19196",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "feature_gram, projected_weights = visualizations.fc_conv_feature_angles(tiny_net, \n",
    "                            \"fully_connected.0.act_func\", num_embed=3, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6792e271",
   "metadata": {},
   "source": [
    "# Which pixels do we care about more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d353528",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def patch_pixel_response():\n",
    "    img, lbl, *_, (noise_size, noise_clr, noise_loc) = valid_set.generate_one()\n",
    "    mask = np.where((noise_clr > 5) & (noise_size > 1))\n",
    "    offsets = np.random.randint(0, noise_size[mask,None], \n",
    "                                (len(noise_size[mask]),2))\n",
    "    selection = noise_loc[mask] + offsets\n",
    "    unif = 2*valid_set.radius[1], valid_set.size-2*valid_set.radius[1]\n",
    "    interior_mask = np.where((unif[0] <= selection[:,0]) & (selection[:,0] <= unif[1]) & \n",
    "                             (unif[0] <= selection[:,1]) & (selection[:,1] <= unif[1]))\n",
    "    selection = selection[interior_mask]\n",
    "\n",
    "    num_locs = len(selection)\n",
    "    stacked_img = np.repeat(img[None,...], num_locs, axis=0)\n",
    "    tensor_img = tensorize(stacked_img, device=device)\n",
    "    #colors = np.arange(255)\n",
    "    maxs = torch.full((num_locs,), -torch.inf).to(device)\n",
    "    mins = torch.full((num_locs,), torch.inf).to(device)\n",
    "    zeros = torch.zeros((num_locs,1), requires_grad=True).to(device)\n",
    "    #for clr in colors:\n",
    "    tensor_img[np.arange(num_locs), :, selection[:,0], selection[:,1]] += zeros\n",
    "    response = tiny_net(tensor_img, logits=True)[:,lbl.argmax()].sum()\n",
    "    grads = torch.autograd.grad(response, zeros)[0]\n",
    "    sizes = noise_size[mask][interior_mask]\n",
    "    return sizes, abs(grads)\n",
    "\n",
    "def avg_patch_response(runs=1000):\n",
    "    tiny_net.eval()\n",
    "    samples = [[] for _ in range(valid_set.noise_size[1] - valid_set.noise_size[0] - 1)]\n",
    "    \n",
    "    for _ in tqdm(range(runs)):\n",
    "        sizes,diffs = patch_pixel_response()\n",
    "        for sz,diff in zip(sizes, diffs.cpu().numpy()):\n",
    "            samples[sz-2].append(diff)  # -2 because we ignore sizes of 0 and 1\n",
    "    return samples\n",
    "result4 = avg_patch_response(runs=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7331f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(result):\n",
    "    x.extend(result3[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af39e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_error_mean = [np.std(x)/np.sqrt(len(x)) for x in result]\n",
    "means = [np.mean(x) for x in result]\n",
    "plt.errorbar(list(range(2,9)), means, std_error_mean, capsize=2)\n",
    "plt.xlabel(\"Region Size\")\n",
    "plt.ylabel(\"Average network response diff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2411077",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_error_mean = [np.std(x)/np.sqrt(len(x)) for x in result4]\n",
    "means = [np.mean(x) for x in result4]\n",
    "plt.errorbar(list(range(2,9)), means, std_error_mean, capsize=2)\n",
    "plt.xlabel(\"Region Size\")\n",
    "plt.ylabel(\"Average absolute gradient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e2dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "both_pixels_response(tiny_net, valid_set, 1, 5,# img_id=5125,\n",
    "                     one_class=True, outer=False, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0894bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = visualizations.region_importance(tiny_net, valid_set, batch_size=512, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341368b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = visualizations.region_importance(tiny_net, valid_set, batch_size=512, \n",
    "                                           device=device, runs=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfc38e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations.both_pixels_response(tiny_net, valid_set, 1, 3, one_class=True, \n",
    "                                    img_id=987_650, outer=True, device=device, batch_size=526)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83e51cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations.both_pixels_response(tiny_net, valid_set, 650, 10, one_class=True, \n",
    "                                    img_id=987_650, outer=False, device=device, batch_size=526)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902d3d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizations.plot_region_importance(*result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755abb20",
   "metadata": {},
   "source": [
    "# PCA Saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a30729",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_scales = [3,5,7,9,13,15]\n",
    "if 1: \n",
    "    %store -r small_pca_directions_1_stride small_pca_directions_s_stride\n",
    "else:\n",
    "    small_pca_directions_1_stride = find_pca_directions(valid_set, 16384, default_scales, 1)\n",
    "    small_pca_directions_s_stride = find_pca_directions(valid_set, 16384, default_scales, default_scales)\n",
    "    %store small_pca_directions_1_stride small_pca_directions_s_stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eaba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1_2123, 1_40_124, 1_508_559, 1_5_019_258, 1_2_429_852, 9032, 5832, 12, 5014, 92, 42, 52, \n",
    "         52_934, 935_152, 1_000_000, 1_000_001, 27, 24, 512, 999_105]  # 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1691c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1_2123, 1_40_124, 9032, 1_5_019_258]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab5b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_map_s_strides, _, grad_maps, explain_imgs = generate_many_pca(tiny_net, seeds, \n",
    "                small_pca_directions_1_stride, default_scales, valid_set, component=0, \n",
    "                batch_size=512, strides=3, skip_1_stride=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfecf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_grid_figure([explain_imgs, pca_map_s_strides, grad_maps], transpose=True, titles=[\"Image\", \"Strides=3\", \"Gradient\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041b73a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_map_s_strides, _, grad_maps, explain_imgs = generate_many_pca(unstrided_net, seeds, \n",
    "                small_pca_directions_1_stride, default_scales, valid_set, component=0, \n",
    "                batch_size=128, strides=3, skip_1_stride=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47136ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_grid_figure([explain_imgs, pca_map_s_strides, grad_maps], transpose=True, titles=[\"Image\", \"Strides=3\", \"Gradient\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7c8344",
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_net = GuidedBackprop(unstrided_net)\n",
    "guided_pca_map_s_strides, _, guided_grad_maps, explain_imgs = generate_many_pca(guided_net, seeds, \n",
    "                small_pca_directions_1_stride, default_scales, valid_set, component=0, \n",
    "                batch_size=128, strides=3, skip_1_stride=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529a0a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_grid_figure([explain_imgs, guided_pca_map_s_strides, guided_grad_maps], transpose=True, titles=[\"Image\", \"PCA Strides=3\", \"Guided Backprop\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e4197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cherry_picked = [0, 1, 5, 3]\n",
    "# cexplain_imgs = [explain_imgs[c] for c in cherry_picked]\n",
    "# cguided_pca_map_s_strides = [guided_pca_map_s_strides[c] for c in cherry_picked]\n",
    "# cguided_grad_maps = [guided_grad_maps[c] for c in cherry_picked]\n",
    "plt_grid_figure([explain_imgs, guided_pca_map_s_strides, guided_grad_maps], transpose=True, colorbar=False)\n",
    "#plt.rcParams.update({'font.size': 25})\n",
    "plt.savefig(\"saliency.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f069dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_grid_figure([[explain_imgs[0]], [guided_pca_map_s_strides[0]]], transpose=True, colorbar=False)\n",
    "#plt.rcParams.update({'font.size': 25})\n",
    "plt.savefig(\"saliency.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
