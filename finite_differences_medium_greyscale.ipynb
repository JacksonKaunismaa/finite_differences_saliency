{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "import color_regions, network, visualizations, utils\n",
    "from color_regions import *\n",
    "from network import *\n",
    "from visualizations import *\n",
    "from utils import *\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6613c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up autoreloading of shared code\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport color_regions,network,visualizations,utils\n",
    "%aimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f19fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "prev_time = 0\n",
    "gamma = 0.99\n",
    "stats = {}  # tracks ewma running average\n",
    "def benchmark(point=None, profile=True, verbose=True, cuda=True): # not thread safe at all\n",
    "    global prev_time\n",
    "    if not profile:\n",
    "        return\n",
    "    if cuda:\n",
    "        torch.cuda.synchronize()\n",
    "    time_now = time.perf_counter()\n",
    "    if point is not None:\n",
    "        point = f\"{sys._getframe().f_back.f_code.co_name}-{point}\"\n",
    "        time_taken = time_now - prev_time\n",
    "        if point not in stats:\n",
    "            stats[point] = time_taken\n",
    "        stats[point] = stats[point]*gamma + time_taken*(1-gamma)\n",
    "        if verbose:\n",
    "            print(f\"took {time_taken} to reach {point}, ewma={stats[point]}\")\n",
    "    prev_time = time_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4eadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "     [transforms.ToTensor()])#,\n",
    "    #transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "batch_size = 32  # seems to be the fastest batch size\n",
    "train_indices = (0, 200_000) # size of training set\n",
    "valid_indices = (1_250_000, 1_260_000)\n",
    "test_indices = (260_000, 460_000)\n",
    "\n",
    "def color_classifier(color):\n",
    "    if color <= 100:  # medium difficulty (width = 75)\n",
    "        return 0\n",
    "    if 100 < color <= 150:  # hard difficulty (width = 50)\n",
    "        return 1\n",
    "    if 150 < color <= 200:  # hard difficulty (width = 50)\n",
    "        return 2\n",
    "    if 200 < color:  # hard difficulty (width = 50)\n",
    "        return 1\n",
    "critical_color_values = [100, 150, 200]\n",
    "\n",
    "def set_loader_helper(indices):\n",
    "    data_set = ColorDatasetGenerator(color_classifier=color_classifier,\n",
    "                                    image_indices=indices,\n",
    "                                    transform=transform,\n",
    "                                    color_range=(25, 250),\n",
    "                                    noise_size=(1,7),\n",
    "                                    num_classes=3,\n",
    "                                    size=128,\n",
    "                                    radius=(128//6, 128//3))\n",
    "    loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size, \n",
    "                                          shuffle=True, num_workers=4, pin_memory=True)\n",
    "    return data_set, loader\n",
    "train_set, train_loader = set_loader_helper(train_indices)\n",
    "valid_set, valid_loader = set_loader_helper(valid_indices)\n",
    "test_set, test_loader = set_loader_helper(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d39721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"medium\" task\n",
    "color_probe = np.linspace(0, 255, 255)\n",
    "color_class = [color_classifier(x) for x in color_probe]\n",
    "plt.plot(color_probe, color_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ff8c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_net = ResNet([[16, 3, 2],  # num_channels (input and output), kernel_size, stride\n",
    "                    [32, 3, 2],\n",
    "                    [64, 3, 2]], 3, [128, 128, 1], \n",
    "                   \"small_net_noise_medium_grey.dict\", fc_layers=[32]).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "small_optim = torch.optim.Adam(small_net.parameters())\n",
    "print(small_net.num_params())\n",
    "small_net.load_model_state_dict(optim=small_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea99043",
   "metadata": {},
   "outputs": [],
   "source": [
    "unstrided_net = ResNet([[16, 3, 1],  # num_channels (input and output), kernel_size, stride\n",
    "                        [32, 3, 1],\n",
    "                        [64, 3, 2]], 3, [128, 128, 1], \n",
    "                   \"unstrided_net_noise_medium_grey.dict\", fc_layers=[32]).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "unstrided_optim = torch.optim.Adam(unstrided_net.parameters())\n",
    "print(unstrided_net.num_params())\n",
    "unstrided_net.load_model_state_dict(optim=unstrided_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46fa421",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train(unstrided_net, unstrided_optim, loss_func, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40afa409",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train(small_net, small_optim, loss_func, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c63984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_distribution(small_net, valid_loader, valid_set,\n",
    "                  critical_values=critical_color_values, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17f7269",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_graph(small_net, valid_set, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de51df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to further test the \"using 1 image => bad batchnorm estimates\" lets do the same test\n",
    "# but instead we will average over a sample of responses\n",
    "small_net.eval() # very important!\n",
    "stack_size = 32\n",
    "sampled_indices = 1_250_000 + np.random.choice(1000, stack_size, replace=False)\n",
    "total_images = stack_size * 255\n",
    "correct_num = 0\n",
    "with torch.no_grad():\n",
    "    counterfactual_color_values = np.linspace(0, 255, 255)\n",
    "    responses = []\n",
    "    for color in tqdm(counterfactual_color_values):\n",
    "        stacked_generated_img = []\n",
    "        for sampled_index in sampled_indices:\n",
    "            np.random.seed(sampled_index)\n",
    "            generated_img, lbl, *__ = valid_set.generate_one(set_color=color)\n",
    "            stacked_generated_img.append(generated_img)\n",
    "        stacked_generated_img = np.array(stacked_generated_img).transpose(0, 3, 1, 2)\n",
    "        generated_img = torch.tensor(stacked_generated_img).to(device).float()\n",
    "        response = small_net(generated_img, logits=True)\n",
    "        stacked_lbl = torch.tensor(np.repeat(np.expand_dims(lbl, 0), stack_size, axis=0)).to(device)\n",
    "        correct_num += correct(response, stacked_lbl).sum()\n",
    "        responses.append(np.squeeze(response.cpu().numpy()).mean(axis=0))\n",
    "print(correct_num/total_images, \"total accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8a4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = np.arcsinh(np.array(responses))  # this graph is quite robust to changes in batch size\n",
    "for output_logit in range(responses.shape[1]): # if we do .eval(), but varies if we do .train()\n",
    "    plt.plot(counterfactual_color_values, responses[:, output_logit], label=f\"class {output_logit}\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Color value\")\n",
    "plt.ylabel(\"Network output logit\")\n",
    "plt.vlines([100, 150], np.min(responses), np.max(responses), linewidth=0.8,\n",
    "           colors=\"r\", label=\"decision boundary\", # probably because .train() in this case actually gives biased estimates because all the colors are the same\n",
    "           linestyles=\"dashed\")  # logit graphs look bad if doing .train(), and accuracy is lower?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a01fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_net.eval()   # ---> without this line, it fails, especially with small colors\n",
    "with torch.no_grad():  # => batchnorm updates are very inaccurate if just one image\n",
    "    idx = 1_250_026   # => network expects batchnorm updates to basically be exactly in the \"middle\" 127\n",
    "    print(valid_set[idx])  # => fix the logit response graph by requiring it to be in eval mode\n",
    "    print(torch.softmax(res_net(torch.unsqueeze(valid_set[idx][\"image\"], 0).to(device).float()), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc75f90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test the above hypothesis, if we just stack the same image a bunch of times, and do .train()\n",
    "# the estimates should still be bad because batchnorm estimates would be just as bad as with \n",
    "# a single image in the batch\n",
    "res_net.train()\n",
    "idx = 1_250_024  # => hypothesis seems to be confirmed\n",
    "test_image = valid_set[idx][\"image\"].numpy()\n",
    "stacked_test = np.repeat(np.expand_dims(test_image, 0), 32, axis=0)\n",
    "print(valid_set[idx])\n",
    "print(torch.softmax(res_net(torch.tensor(stacked_test).to(device).float()), 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493f1f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(generated_img.cpu().numpy()), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_net.train()\n",
    "with torch.no_grad():\n",
    "    for i, sample in enumerate(valid_loader):\n",
    "        imgs = sample[\"image\"].to(device).float()\n",
    "        labels = sample[\"label\"].to(device).float()\n",
    "        print(sample[\"color\"])\n",
    "        outputs = res_net(imgs)\n",
    "        print(loss_func(outputs, labels).item())\n",
    "        print(torch.argmax(labels,dim=1), torch.argmax(outputs, dim=1))\n",
    "        print(correct(outputs, labels).sum().item())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362c7276",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(500_001)\n",
    "explain_img, lbl, *_ = valid_set.generate_one()\n",
    "heat_map = finite_differences_map(small_net, valid_set, lbl.argmax(), explain_img, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725c5459",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(explain_img, cmap=\"gray\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(heat_map, cmap=\"bwr\", interpolation=\"bilinear\")\n",
    "plt.colorbar()\n",
    "# generated with strides = 2 everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc1e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_grid_figure([explain_img, heat_map], transpose=False, colorbar=True)\n",
    "# generated with strides = 1, strides = 8 for lats layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9c62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = [20_000, 25_000, 30_000, 600_000, 600_001]\n",
    "heat_maps = []\n",
    "explain_imgs = []\n",
    "for i, image_id in enumerate(image_ids):\n",
    "    np.random.seed(image_id)\n",
    "    explain_img_i, target_i, *__ = valid_set.generate_one()\n",
    "    heat_map_i = finite_differences_map(small_net, valid_set, target_i.argmax(), explain_img_i)\n",
    "    heat_maps.append(heat_map_i)\n",
    "    explain_imgs.append(explain_img_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ff450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_grid_figure([explain_imgs, heat_maps], transpose=True, colorbar=True)\n",
    "# generated with strides=2 everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bdc89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_grid_figure([explain_imgs, heat_maps], transpose=True, colorbar=True)\n",
    "# generated with strides = 1, strides = 8 for lats layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f8060b",
   "metadata": {},
   "source": [
    "What if we run the same experiment, but cheat with a prior on pixel values that we know *should* be informative to the output logit, namely values closest to the decision boundary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f1e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfair_prior = np.array([90, 110, 140, 160])  #  close to the critical values of 100, 150\n",
    "unfair_heat_maps = []\n",
    "plt.figure(figsize=(12, 5*len(image_ids)))\n",
    "for i, image_id in enumerate(image_ids):\n",
    "    np.random.seed(image_id)\n",
    "    explain_img_i, target_i, __ = valid_set.generate_one()\n",
    "    unfair_map_i = finite_differences_map(res_net, valid_set, target_i.argmax(), explain_img_i, unfairness=\"unfair\", values_prior=unfair_prior)\n",
    "    unfair_heat_maps.append(unfair_map_i)\n",
    "    plt.subplot(len(image_ids), 3, 3*i+1)\n",
    "    plt.imshow(explain_img_i, cmap=\"gray\")\n",
    "    plt.subplot(len(image_ids), 3, 3*i+2)\n",
    "    heat_max = np.max(abs(unfair_map_i))\n",
    "    plt.imshow(unfair_map_i, cmap=\"bwr\", vmax=heat_max, vmin=-heat_max)\n",
    "    plt.colorbar(shrink=0.5)\n",
    "    plt.subplot(len(image_ids), 3, 3*i+3)\n",
    "    heat_max = np.max(abs(heat_maps[i]))\n",
    "    plt.imshow(heat_maps[i], cmap=\"bwr\", vmax=heat_max, vmin=-heat_max)\n",
    "    plt.colorbar(shrink=0.5)\n",
    "plt.show()  # => very similar results, but with a 4x speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094020d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regenerate unfair FD maps\n",
    "unfair_prior = np.array([90, 110, 140, 160])  #  close to the critical values of 100, 150\n",
    "image_ids = [20_000, 25_000, 30_000, 600_000, 600_001, 227_662, 998_102, 106_758]\n",
    "unfair_heat_maps = []\n",
    "#plt.figure(figsize=(12, 5*len(image_ids)))\n",
    "for i, image_id in enumerate(image_ids):\n",
    "    np.random.seed(image_id)\n",
    "    explain_img_i, target_i, __ = valid_set.generate_one()\n",
    "    unfair_map_i = finite_differences_map(res_net, valid_set, target_i.argmax(), explain_img_i, unfairness=\"unfair\", values_prior=unfair_prior)\n",
    "    unfair_heat_maps.append(unfair_map_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f292125",
   "metadata": {},
   "source": [
    "We can do even better by taking the \"closest value in a different class\" for our prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5440d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_unfair_heat_maps = []\n",
    "plt.figure(figsize=(20, 5*len(image_ids)))\n",
    "for i, image_id in enumerate(image_ids):\n",
    "    np.random.seed(image_id)\n",
    "    explain_img_i, target_i, color_i = valid_set.generate_one()\n",
    "    very_unfair_map_i = finite_differences_map(res_net, valid_set, target_i.argmax(), explain_img_i, unfairness=\"very unfair\", values_prior=[100, 150])\n",
    "    very_unfair_heat_maps.append(very_unfair_map_i)\n",
    "    \n",
    "    plt.subplot(len(image_ids), 5, 5*i+1)\n",
    "    if i == 0:\n",
    "        plt.title(\"Image\")\n",
    "    plt.imshow(explain_img_i, cmap=\"gray\")\n",
    "    \n",
    "    plt.subplot(len(image_ids), 5, 5*i+2)\n",
    "    if i == 0:\n",
    "        plt.title(\"Very unfair FD map\")\n",
    "    heat_max = np.max(abs(very_unfair_map_i))\n",
    "    plt.imshow(very_unfair_map_i, cmap=\"bwr\", vmax=heat_max, vmin=-heat_max)\n",
    "    plt.colorbar(shrink=0.5)\n",
    "    \n",
    "    plt.subplot(len(image_ids), 5, 5*i+3)\n",
    "    if i == 0:\n",
    "        plt.title(\"Unfair FD map\")\n",
    "    heat_max = np.max(abs(unfair_heat_maps[i]))\n",
    "    plt.imshow(unfair_heat_maps[i], cmap=\"bwr\", vmax=heat_max, vmin=-heat_max)\n",
    "    plt.colorbar(shrink=0.5)\n",
    "    \n",
    "    plt.subplot(len(image_ids), 5, 5*i+4)\n",
    "    if i == 0:\n",
    "        plt.title(\"FD map\")\n",
    "    heat_max = np.max(abs(heat_maps[i]))\n",
    "    plt.imshow(heat_maps[i], cmap=\"bwr\", vmax=heat_max, vmin=-heat_max)\n",
    "    plt.colorbar(shrink=0.5)\n",
    "    \n",
    "    plt.subplot(len(image_ids), 5, 5*i+5)\n",
    "    if i == 0:\n",
    "        plt.title(\"Location in color space\")\n",
    "    plt.plot(color_probe, color_class) \n",
    "    plt.vlines([color_i], 0, valid_set.num_classes-1, linewidth=0.8,\n",
    "           colors=\"r\", label=\"color value\",\n",
    "           linestyles=\"dashed\")\n",
    "plt.show()  # => somewhat similar results (see image 2), but with an overall ~11x speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075e168",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_heat_maps = []\n",
    "plt.figure(figsize=(20, 5*len(image_ids)))\n",
    "for i, image_id in tqdm(enumerate(image_ids)):\n",
    "    np.random.seed(image_id)\n",
    "    explain_img_i, target_i, color_i = valid_set.generate_one()\n",
    "    batched_explain_img_i = torch.tensor(np.expand_dims(explain_img_i, 0).transpose(0, 3, 1, 2), requires_grad=True).to(device).float()\n",
    "    output_logit_i = res_net(batched_explain_img_i)[0, target_i.argmax()]\n",
    "    \n",
    "    img_grad_i = torch.autograd.grad(output_logit_i, batched_explain_img_i)[0].squeeze().cpu().numpy()\n",
    "    grad_times_input_i = img_grad_i * np.squeeze(explain_img_i)\n",
    "    \n",
    "    plt.subplot(len(image_ids), 5, 5*i+1)\n",
    "    if i == 0:\n",
    "        plt.title(\"Image\")\n",
    "    plt.imshow(explain_img_i, cmap=\"gray\")\n",
    "    \n",
    "    plt.subplot(len(image_ids), 5, 5*i+2)\n",
    "    if i == 0:\n",
    "        plt.title(\"Input*Gradient explanation\")\n",
    "    heat_max = np.max(abs(grad_times_input_i))\n",
    "    plt.imshow(grad_times_input_i, cmap=\"bwr\", vmax=heat_max, vmin=-heat_max)\n",
    "    plt.colorbar(shrink=0.5)\n",
    "    \n",
    "    plt.subplot(len(image_ids), 5, 5*i+3)\n",
    "    if i == 0:\n",
    "        plt.title(\"Gradient explanation\")\n",
    "    heat_max = np.max(abs(img_grad_i))\n",
    "    plt.imshow(img_grad_i, cmap=\"bwr\", vmax=heat_max, vmin=-heat_max)\n",
    "    plt.colorbar(shrink=0.5)\n",
    "    \n",
    "    plt.subplot(len(image_ids), 5, 5*i+4)\n",
    "    if i == 0:\n",
    "        plt.title(\"FD explanation (unfair)\")\n",
    "    heat_max = np.max(abs(unfair_heat_maps[i]))\n",
    "    plt.imshow(unfair_heat_maps[i], cmap=\"bwr\", vmax=heat_max, vmin=-heat_max)\n",
    "    plt.colorbar(shrink=0.5)\n",
    "    \n",
    "    plt.subplot(len(image_ids), 5, 5*i+5)\n",
    "    if i == 0:\n",
    "        plt.title(\"Location in color space\")\n",
    "    plt.plot(color_probe, color_class) \n",
    "    plt.vlines([color_i], 0, valid_set.num_classes-1, linewidth=0.8,\n",
    "           colors=\"r\", label=\"color value\",\n",
    "           linestyles=\"dashed\")\n",
    "plt.show()\n",
    "# gradient should be zero, so double check computations, fix scale on cmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656cd5d0",
   "metadata": {},
   "source": [
    "# PCA Direction Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c03e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2,2)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        print(\"\\tBefore ReLUs\", x)\n",
    "        return self.relu(x)\n",
    "dummy_net = DummyNet()\n",
    "dummy_net.linear._parameters[\"weight\"].data = torch.nn.Parameter(torch.tensor([[1., 0], [0, 1]]))\n",
    "dummy_net.linear._parameters[\"bias\"].data = torch.nn.Parameter(torch.tensor([200., 200]))\n",
    "print(\"Network parameters\", dummy_net.linear._parameters)\n",
    "print(\"WITHOUT GUIDED BACKPROP\")\n",
    "inpt = torch.tensor([1., -1.], requires_grad=True)\n",
    "result = dummy_net(inpt)\n",
    "print(\"\\tNetwork output\", result)\n",
    "print(\"\\t'Loss'\", -result[0]+result[1])\n",
    "print(\"\\tResulting gradients\", torch.autograd.grad(-result[0]+result[1], inpt))\n",
    "print(\"WITH GUIDED BACKPROP\")\n",
    "guided_dummy = GuidedBackprop(dummy_net)\n",
    "result = guided_dummy(inpt, preserve_hooks=False)\n",
    "print(\"\\tNetwork output\", result)\n",
    "print(\"\\t'Loss'\", -result[0]+result[1])\n",
    "print(\"\\tResulting gradients\", torch.autograd.grad(-result[0]+result[1], inpt))\n",
    "print(\"GUIDED BACKPROP AGAIN (should auto-clean now)\")\n",
    "inpt = torch.tensor([1., -1.], requires_grad=True)\n",
    "result = dummy_net(inpt)\n",
    "print(\"\\tNetwork output\", result)\n",
    "print(\"\\t'Loss'\", -result[0]+result[1])\n",
    "print(\"\\tResulting gradients\", torch.autograd.grad(-result[0]+result[1], inpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d630c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_scales = [3,5,7,9,13,15]\n",
    "pca_directions_1_stride = find_pca_directions(valid_set, 1024, default_scales, 1)\n",
    "pca_directions_s_stride = find_pca_directions(valid_set, 1024, default_scales, default_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca58f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6*4, 12))\n",
    "for i, res in enumerate(pca_directions_s_stride):\n",
    "    compressed_results = np.concatenate(np.concatenate(res, 1), 1)\n",
    "    plt.subplot(1,len(pca_directions_s_stride),i+1)\n",
    "    if i == 0:\n",
    "        plt.title(\"Strided windows\")\n",
    "    plt.imshow(compressed_results, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92731f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6*4, 12))\n",
    "for i, res in enumerate(pca_directions_1_stride):\n",
    "    compressed_results = np.concatenate(np.concatenate(res, 1), 1)\n",
    "    plt.subplot(1,len(pca_directions_1_stride),i+1)\n",
    "    if i == 0:\n",
    "        plt.title(\"Stride=1\")\n",
    "    plt.imshow(compressed_results, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe52ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(200_010)\n",
    "generated_img, label, *__ = valid_set.generate_one()\n",
    "pca_map_strided = pca_direction_grids(small_net, valid_set, label.argmax(), generated_img, \n",
    "                                      pca_direction_grids=pca_directions_s_stride)\n",
    "pca_map_1_stride = pca_direction_grids(small_net, valid_set, label.argmax(), generated_img, \n",
    "                                      pca_direction_grids=pca_directions_1_stride, strides=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac78412",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_grid_figure([generated_img, result])  # => with strides == scales\n",
    "# I believe it was generated on \n",
    "# small_net = ResNet([[16, 3, 2],  # num_channels (input and output), kernel_size, stride\n",
    "#                     [32, 3, 2],\n",
    "#                     [64, 3, 2]], 3, [128, 128, 1], \n",
    "#                    \"small_net_noise_hard_grey.dict\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f115175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_grid_figure([generated_img, result])  # => with strides == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5b467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1_2123, 1_40_124, 1_508_559, 1_5_019_258, 1_2_429_852, 9032, 5832, 12, 5014, 92, 42, 52, \n",
    "         52_934, 935_152, 1_000_000, 1_000_001, 27, 24, 512, 999_105]  # 20 \n",
    "def generate_many_pca(net):\n",
    "    _pca_map_s_strides = []\n",
    "    _pca_map_1_strides = []\n",
    "    _grad_maps = []\n",
    "    _explain_imgs = []\n",
    "    for seed in seeds:\n",
    "        np.random.seed(seed)\n",
    "        generated_img, label, *__ = valid_set.generate_one()\n",
    "        tensored_img = torch.tensor(generated_img.transpose(2,0,1), requires_grad=True).unsqueeze(0).float().to(device)\n",
    "        grad_map = torch.autograd.grad(net(tensored_img)[0,label.argmax()], tensored_img)[0]\n",
    "        pca_map_strided = pca_direction_grids(net, valid_set, label.argmax(), generated_img, \n",
    "                                              pca_direction_grids=pca_directions_s_stride)\n",
    "        pca_map_1_stride = pca_direction_grids(net, valid_set, label.argmax(), generated_img, \n",
    "                                          pca_direction_grids=pca_directions_1_stride, strides=1)\n",
    "        _explain_imgs.append(generated_img)\n",
    "        _grad_maps.append(grad_map.detach().cpu().squeeze(0).numpy().transpose(1,2,0))\n",
    "        _pca_map_s_strides.append(pca_map_strided.copy())\n",
    "        _pca_map_1_strides.append(pca_map_1_stride.copy())\n",
    "    return _pca_map_s_strides, _pca_map_1_strides, _grad_maps, _explain_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1124c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_map_s_strides, pca_map_1_strides, grad_maps, explain_imgs = generate_many_pca(unstrided_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a3df7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_net = GuidedBackprop(unstrided_net)\n",
    "guided_pca_map_s_strides, guided_pca_map_1_strides, guided_grad_maps, explain_imgs = generate_many_pca(guided_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c70afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_grid_figure([explain_imgs, guided_pca_map_s_strides, guided_pca_map_1_strides, guided_grad_maps, pca_map_s_strides, pca_map_1_strides, grad_maps], transpose=True, titles=[\"Image\", \"Guided Strides=scale\", \"Guided strides=1\", \"Guided Gradient\", \"Strides=scale\", \"strides=1\", \"Gradient\"])\n",
    "# on the strides=2 final layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e677a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer_weight = unstrided_net._modules[\"fully_connected\"][-1].fully_connected.weight.detach().cpu().numpy()\n",
    "imshow_centered_colorbar(last_layer_weight, cmap=\"bwr\", title=\"Last Layer FC weights\", colorbar=False)\n",
    "plt.vlines([1, 4, 17], ymin=0, ymax=2.5)\n",
    "\n",
    "last_layer_bias = unstrided_net._modules[\"fully_connected\"][-1].fully_connected.bias.detach().cpu().numpy()\n",
    "print(last_layer_bias)\n",
    "# in column 17, all weights are negative, so guided backprop means we immediately zero everything out\n",
    "# would also happend for columns 3(?), 10, 13, 16, 17, 18, 20, 30(?), 31\n",
    "# basically 9/32 = 28% of all images will be completely zero, if the assumption of\n",
    "# \"only 1 non-zero logit in the final hidden layer\" holds true\n",
    "# relevant columns have been highlighted\n",
    "\n",
    "# column 1 mostly means class 0, very strongly not class 2\n",
    "# column 4 mostly means class 1, equally strongly not class 0 and 2\n",
    "# column 17 means not class 0, not class 1, barely class 2 => problematic\n",
    "\n",
    "# since class 2 is \"default class\" (largest bias), the negative weights in column 17 are fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6*4, 6*8))\n",
    "first_fc = unstrided_net._modules[\"fully_connected\"][0].fully_connected.weight.detach().cpu().numpy()\n",
    "# only bother visualizing outputs 1, 4, and 17 (and add 13,12,15,0,9,27 to compare)\n",
    "relevant_outputs = range(32)#[1, 4, 17, 13, 12, 15, 0, 9, 27]\n",
    "for i, output_col in enumerate(relevant_outputs):\n",
    "    fc_weights = np.concatenate(np.concatenate(first_fc[output_col].reshape(8, 8, 63, 63),1),1)\n",
    "    plt.subplot(8,4,i+1)\n",
    "    imshow_centered_colorbar(fc_weights, cmap=\"bwr\", title=f\"FC weights of {output_col}\")\n",
    "# seemingly \"empty\" maps in the actual useful columns is just very low norm compared to the\n",
    "# other weights (~0.2, whereas other weight maps range up to ~2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746fed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6*4, 12))\n",
    "final_conv_map = debug_net._features[\"conv_blocks.2.batch_norm2\"].detach().cpu().numpy()[0].reshape(8,8,63,63)\n",
    "conv_max = abs(final_conv_map).max()\n",
    "\n",
    "compressed_results = np.concatenate(np.concatenate(final_conv_map, 1), 1)\n",
    "plt.imshow(compressed_results, cmap=\"bwr\", vmin=-conv_max, vmax=conv_max)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9add34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6*4, 12))\n",
    "final_relu = debug_net._features[\"conv_blocks.2.act_func2\"].detach().cpu().numpy()[0].reshape(8,8,63,63)\n",
    "relu_max = abs(final_relu).max()\n",
    "\n",
    "compressed_results = np.tanh(np.concatenate(np.concatenate(final_relu, 1), 1))\n",
    "plt.imshow(compressed_results, cmap=\"bwr\", vmin=-1, vmax=1)\n",
    "plt.colorbar()\n",
    "# with tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seeds[9])\n",
    "working_img, label, color, *_____ = valid_set.generate_one()\n",
    "print(color, label.argmax())\n",
    "tensored_img = torch.tensor(working_img.transpose(2,0,1), requires_grad=True).unsqueeze(0).float().to(device)\n",
    "guided_net = GuidedBackprop(unstrided_net)\n",
    "grad_map = torch.autograd.grad(guided_net(tensored_img)[0,label.argmax()], tensored_img)[0]\n",
    "debug_net = AllActivations(unstrided_net)\n",
    "debug_net(tensored_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb5d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def final_activation_histogram(net):\n",
    "    net.eval()\n",
    "    nonzero_histogram = torch.zeros(32).to(device)\n",
    "    pattern_counts = defaultdict(lambda: defaultdict(int))\n",
    "    debug_net = AllActivations(net)\n",
    "    for i, sample in tqdm(enumerate(test_loader)):\n",
    "        imgs = sample[\"image\"].to(device).float()\n",
    "        colors = sample[\"color\"]\n",
    "        debug_net(imgs)\n",
    "        final_layer = debug_net._features[\"fully_connected.0.act_func\"]\n",
    "        nonzero = torch.where(final_layer > 0, 1, 0).detach().cpu().numpy()\n",
    "        for row, color in zip(nonzero, colors):\n",
    "            pattern = str(row)\n",
    "            pattern_counts[int(color)][pattern] += 1\n",
    "    return pattern_counts\n",
    "color_distrib = final_activation_histogram(unstrided_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3c66ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq = set()\n",
    "for x in color_distrib.values():\n",
    "    uniq = uniq.union(set(x.keys()))\n",
    "pattern_to_names = {}\n",
    "for pattern in uniq:\n",
    "    pattern_to_names[pattern] = pattern[1+2*1] + pattern[1+2*4] + pattern[1+2*17]\n",
    "pattern_to_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9364bf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_totals = defaultdict(int)\n",
    "for color, values in color_distrib.items():\n",
    "    for pattern, count in values.items():\n",
    "        pattern_totals[pattern] += count\n",
    "pattern_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ff0345",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pattern in uniq:\n",
    "    amounts = np.zeros((255,))\n",
    "    for color, distrib in color_distrib.items():\n",
    "        amounts[color] = distrib[pattern]\n",
    "    plt.plot(amounts, label=pattern_to_names[pattern])\n",
    "color_probe = np.linspace(0, 255, 255)\n",
    "color_class = [color_classifier(x)*400 for x in color_probe]\n",
    "plt.plot(color_probe, color_class, label=\"classes\", linestyle=\"dotted\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb7e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_generate(img, lbl, net, alpha, lr, runs):\n",
    "    # alpha is maximum norm that the adversarial can be\n",
    "    adversarial_direction = np.random.uniform(-alpha, alpha, size=(1, valid_set.size, valid_set.size))\n",
    "    adversarial_direction = adversarial_direction/np.linalg.norm(adversarial_direction)*alpha\n",
    "    \n",
    "    # pick arbitrary target\n",
    "    bad_lbl = (lbl.argmax() + 1) % valid_set.num_classes\n",
    "    target = torch.tensor(np.zeros_like(lbl)).to(device).unsqueeze(0).float()\n",
    "    target[0,2] = 1.\n",
    "    print(target, lbl)\n",
    "    \n",
    "    tensor_img = torch.tensor(img.transpose(2,0,1)).unsqueeze(0).to(device).float()\n",
    "    tensor_adv_dir = torch.tensor(adversarial_direction, requires_grad=True).unsqueeze(0).to(device).float()\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    for i in range(runs):\n",
    "        curr_img = tensor_img + tensor_adv_dir\n",
    "        curr_net_out = net(curr_img)\n",
    "        curr_loss = loss_func(curr_net_out, target)# + 5e-10*torch.linalg.norm(tensor_adv_dir)\n",
    "        grad_dir = torch.autograd.grad(curr_loss, tensor_adv_dir)[0]\n",
    "        tensor_adv_dir -= lr*grad_dir\n",
    "        tensor_adv_dir = torch.clamp(tensor_adv_dir, min=-alpha, max=alpha)\n",
    "\n",
    "        if i % (runs//5) == (runs//5-1):\n",
    "            print(curr_loss.item(), curr_net_out)\n",
    "    return tensor_img + tensor_adv_dir\n",
    "np.random.seed(58)\n",
    "generated_img, gen_label, color, *_ = valid_set.generate_one()\n",
    "print(color)\n",
    "adv_example = adversarial_generate(generated_img, gen_label, unstrided_net, 6.4, 1e2, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a138c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 230 color -> alpha very close to 5, works on 1e2, 500 (seed 55)\n",
    "# 136 color -> alpha of 3 (seed 54)\n",
    "# 50 color -> alpha of 9-10 (seed 53)\n",
    "# 181 color (in class 2 already) -> adv_dir is basically random noise, weird cyclic structure to it (seed 52)\n",
    "# 82 color -> alpha of 3.8 (high) (seed 51)\n",
    "# 201 color -> alpha of 0-1 (high) (seed 50)\n",
    "# 110 color -> alpha of 2-3 (low) (seed 56)\n",
    "# 232 color -> alpha close to 5 (seed 57)\n",
    "# 60 color -> alpha 6-7 (mid) (seed 58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71399de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "plt.imshow(generated_img, cmap=\"gray\")\n",
    "plt.subplot(1,3,2)\n",
    "np_adv = adv_example.detach().cpu().numpy().squeeze()\n",
    "plt.imshow(np_adv, cmap=\"gray\")\n",
    "plt.subplot(1,3,3)\n",
    "imshow_centered_colorbar(generated_img.squeeze()-np_adv, cmap=\"bwr\", title=\"Adv direction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cec4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_net = AllActivations(unstrided_net)\n",
    "debug_net(adv_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7c3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_net._features[\"fully_connected.0.act_func\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09307ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_grid_figure([explain_imgs, pca_map_s_strides, pca_map_1_strides], transpose=True, titles=[\"Image\", \"Strides=scale\", \"strides=1\"])\n",
    "# add comparison to regular gradient\n",
    "# smaller circles = bad?\n",
    "# manually test if the gradient changes make sense\n",
    "# model lerans weird stuff about the noise\n",
    "# interpertation of its algo is interesting\n",
    "\n",
    "# make texture dataset and test the methods on it\n",
    "# texture generation: emerging conv?\n",
    "# heuristic = lots of code\n",
    "# dataset\n",
    "# test on natual images eventually\n",
    "\n",
    "# show it works when edeges important too (guided backprop first)\n",
    "# could do saliency checks\n",
    "\n",
    "# fourier transform could work for texture, if its the whole image (to get window size)\n",
    "# do fft on quadrants of image to guess at scale, look at max fourier coeff =>\n",
    "# should give rough idea of window size (top k coeffs?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296b8ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_grid_figure([explain_imgs, pca_map_s_strides, pca_map_1_strides, grad_maps], transpose=True, titles=[\"Image\", \"Strides=scale\", \"strides=1\", \"Gradient\"])\n",
    "# on the strides=8 final layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74688f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_grid_figure([explain_imgs, pca_map_s_strides, pca_map_1_strides, grad_maps], transpose=True, titles=[\"Image\", \"Strides=scale\", \"strides=1\", \"Gradient\"])\n",
    "# on the strides=2 final layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c36226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# surely this is the best way to do this :)\n",
    "x = np.arange(400).reshape(5,5,4,4)\n",
    "from itertools import permutations\n",
    "for transp_1 in permutations([0,1,2,3]):\n",
    "    for axis1 in range(3):\n",
    "        for transp_2 in permutations([0,1,2]):\n",
    "            for axis2 in range(2):\n",
    "                for transp_3 in permutations([0,1]):\n",
    "                    try:\n",
    "                        t1 = x.transpose(*transp_1)\n",
    "                        t2 = np.concatenate(t1, axis1)\n",
    "                        t3 = t2.transpose(*transp_2)\n",
    "                        t4 = np.concatenate(t3, axis2)\n",
    "                        t5 = t4.transpose(*transp_3)\n",
    "                        if t5.shape == (20,20):\n",
    "                            if all(t5[0,:4] == np.arange(4)) and t5[0,4] == 16:\n",
    "                                pass\n",
    "                                #print(transp_1, axis1, transp_2, axis2, transp_3)\n",
    "                    except:\n",
    "                        continue\n",
    "np.concatenate(np.concatenate(x,1),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e91693",
   "metadata": {},
   "source": [
    "So the question then becomes, how do we search for useful reference images/pixel values in general? We want the distance to be close to the image (small denominator), but also lead to large differences in output logits. This is dangerously close to finding adversarial directions, so we need to make sure we stay in the data manifold => need to establish some sort of distance metric, and potentially a way of detecting whether we are in manifold or not, so we can project into manifold space if needed. This also allows us to switch to a DeepLIFT style type thing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86a46e",
   "metadata": {},
   "source": [
    "# Model Optimization Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f3a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_net.save_model_state_dict(optim=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a9a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_img, _, __ = valid_set.generate_one()\n",
    "generated_img = torch.tensor(generated_img.transpose(2,0,1)).to(device).unsqueeze(0).float()\n",
    "for _ in range(1000):\n",
    "    small_net.forward(generated_img, profile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd852210",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum(stats.values())  # --> gave 3x speed! (Fast and Accurate Model scaling?)\n",
    "for k,v in stats.items():    # --> the 3x speedup caused underfitting though, so switched to 2x\n",
    "    print(k,(100.*v/total))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
