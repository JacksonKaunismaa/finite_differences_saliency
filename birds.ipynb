{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import os.path as osp\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from torchvision.models import EfficientNet_B0_Weights, efficientnet_b0\n",
    "import matplotlib.pyplot as plt\n",
    "import glob as glob\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import scipy\n",
    "import cv2\n",
    "\n",
    "from colorviz.conv_color import visualizations, utils, hooks\n",
    "from colorviz.conv_color.config_objects import ImageDatasetCfg, ExperimentConfig\n",
    "from colorviz.birds_dataset.data import ImageDataset\n",
    "from colorviz.birds_dataset import network\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "path = \"/scratch/ssd004/scratch/jackk/birds_data\"\n",
    "model = keras.models.load_model(osp.join(path, 'EfficientNetB0-525-(224 X 224)- 98.97.h5'), custom_objects={'F1_score':'F1_score'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "te_dset = tf.keras.utils.image_dataset_from_directory(osp.join(path, \"test\"), image_size=(224, 224))\n",
    "va_dset = tf.keras.utils.image_dataset_from_directory(osp.join(path, \"valid\"), image_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# rotate, crop and scale back up\n",
    "def get_f1(dset, scale=20):\n",
    "    full_labels = tf.zeros([0], dtype=tf.int32)\n",
    "    full_preds = tf.zeros([0], dtype=tf.int64)\n",
    "    for b in tqdm(dset):\n",
    "        noise = tf.random.uniform(b[0].shape, minval=-scale, maxval=scale)\n",
    "        preds = model(b[0] + noise)\n",
    "        full_labels = tf.concat([full_labels, b[1]], 0)\n",
    "        full_preds = tf.concat([full_preds, tf.math.argmax(preds, axis=-1)], 0)\n",
    "    full_labels = full_labels.numpy()\n",
    "    full_preds = full_preds.numpy()\n",
    "    return sklearn.metrics.f1_score(full_preds, full_labels, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(get_f1(te_dset, scale=50))\n",
    "# print(get_f1(va_dset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cfg = ImageDatasetCfg(batch_size=512,\n",
    "                            num_workers=4,\n",
    "                            data_dir=\"/scratch/ssd004/scratch/jackk/birds_data\",\n",
    "                            device=\"cuda:0\")\n",
    "transform = EfficientNet_B0_Weights.IMAGENET1K_V1.transforms()\n",
    "\n",
    "dsets = {split: ImageDataset(split, transform, data_cfg, ddp=False) for split in [\"train\", \"valid\", \"test\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def pca_direction_grids(model, dataset, target_class, img, scales, pca_direction_grids,\n",
    "                        strides=None, gaussian=False, component=0, batch_size=32):\n",
    "    # begin by computing d_output_d_alphas\n",
    "    model.eval()\n",
    "    im_size = img.shape[0]\n",
    "    if strides is None:\n",
    "        strides = scales\n",
    "    if isinstance(strides, int):\n",
    "        strides = [strides]*len(pca_direction_grids)\n",
    "\n",
    "    d_out_d_alpha_grids = []\n",
    "    interpolators = []\n",
    "    indices_grid = np.mgrid[0:im_size, 0:im_size]\n",
    "\n",
    "    stacked_img = np.repeat(np.expand_dims(img, 0), batch_size, axis=0)\n",
    "    stacked_img = np.transpose(stacked_img, (0, 3, 1, 2)).astype(np.float32) # NCHW format\n",
    "    img_tensor = dataset.implicit_normalization(torch.tensor(stacked_img).to(dataset.cfg.device))\n",
    "\n",
    "    for s, (scale, stride) in enumerate(zip(scales, strides)):\n",
    "        # centers are [scale//2, ..., im_size-scale//2-1], num_windows = im_size-scale+1\n",
    "        # the -1 on the upper limit center c.f. the \"last index\" being im_size-1\n",
    "        # the num_windows is correct because `(im_size-scale//2-1) - (scale//2) = (im_size-2*(scale-1)/2-1) = im_size-scale`\n",
    "        # and num elements of the array is last-first+1\n",
    "        index_windows = np.lib.stride_tricks.sliding_window_view(indices_grid, (scale,scale), axis=(1,2))\n",
    "\n",
    "        xs = np.mgrid[0:im_size-scale:stride, 0:im_size-scale:stride]  # indexes into pca_direction_grids\n",
    "        num_grid = xs.shape[1]\n",
    "        #print(xs, num_grid)\n",
    "        d_out_d_alpha_grid = np.zeros((num_grid, num_grid))\n",
    "\n",
    "        strided_indices = xs.transpose(1,2,0).reshape(-1, 2)  # ie should always pass strides=1 pca_directions into this\n",
    "        unstrided_indices = np.mgrid[:num_grid, :num_grid].transpose(1,2,0).reshape(-1, 2)\n",
    "        for k in tqdm(range(0, num_grid*num_grid, batch_size)):\n",
    "            actual_batch_size = min(batch_size, num_grid*num_grid-k)\n",
    "            batch_locs = strided_indices[k: k+actual_batch_size]\n",
    "            batch_unstrided_locs = unstrided_indices[k: k+actual_batch_size]  # for indexing into a dense grid (num_grid, num_grid)\n",
    "\n",
    "            pca_directions = pca_direction_grids[s][batch_locs[:,0], batch_locs[:,1], component]\n",
    "            batch_window_indices = index_windows[:, batch_locs[:,0], batch_locs[:,1], ...]\n",
    "\n",
    "            # do d_output_d_alpha computation\n",
    "            alpha = torch.zeros((actual_batch_size,1,1,1), requires_grad=True).to(dataset.cfg.device)\n",
    "            direction_tensor = dataset.implicit_normalization(torch.tensor(pca_directions).to(dataset.cfg.device).float())\n",
    "            img_tensor[np.arange(actual_batch_size)[:,None,None], :, batch_window_indices[0], batch_window_indices[1]] += alpha*direction_tensor\n",
    "            output = model(img_tensor)  # sum since gradient will be back-proped as vector of 1`s\n",
    "\n",
    "            d_out_d_alpha = torch.autograd.grad(output[:,target_class].sum(), alpha)[0].squeeze()\n",
    "            model.zero_grad()\n",
    "            d_out_d_alpha_grid[batch_unstrided_locs[:,0], batch_unstrided_locs[:,1]] = d_out_d_alpha.detach().cpu().numpy()\n",
    "\n",
    "        d_out_d_alpha_grids.append(d_out_d_alpha_grid.copy())\n",
    "        # add scale//2 because centers of windows are actually offset by scale//2, and don't directly correspond to indices into\n",
    "        # pca_direction_grid space\n",
    "        interpolators.append(RegularGridInterpolator((xs[1,0]+scale//2, xs[1,0]+scale//2), d_out_d_alpha_grid,\n",
    "                                                     bounds_error=False, fill_value=None))\n",
    "\n",
    "    # now, per pixel, interpolate what the d_output_d_alpha value would be if the window\n",
    "    # were centered at that pixel, then take the max over all possible scales\n",
    "    #print(d_out_d_alpha_grids[-1])\n",
    "    saliency_map = np.zeros_like(img).astype(np.float32)\n",
    "    scale_wins = [0] * len(scales)\n",
    "    for i in tqdm(range(im_size)):\n",
    "        for j in range(im_size):\n",
    "            best_d_out_d_alpha = 0\n",
    "            best_scale = -1\n",
    "            for s in range(len(scales)):\n",
    "                interp_value = interpolators[s]([i,j])\n",
    "                if abs(interp_value) >= abs(best_d_out_d_alpha):\n",
    "                    best_d_out_d_alpha = interp_value\n",
    "                    best_scale = s\n",
    "            saliency_map[i,j] = best_d_out_d_alpha\n",
    "            scale_wins[best_scale] += 1\n",
    "    print(scale_wins)\n",
    "    return saliency_map  # try jacobian with respect to window itself (isnt this just the gradient?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class_ = 52\n",
    "class_imgs  = []\n",
    "for img in dsets['train']:\n",
    "    if img['label'] == class_:\n",
    "        class_imgs.append(img['image'].numpy())\n",
    "class_imgs = np.asarray(class_imgs)\n",
    "avg_img = class_imgs.mean(axis=0)\n",
    "plt.imshow(avg_img.transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class_ = 51\n",
    "class_imgs  = []\n",
    "for img in dsets['train']:\n",
    "    if img['label'] == class_:\n",
    "        class_imgs.append(img['image'].numpy())\n",
    "class_imgs = np.asarray(class_imgs)\n",
    "avg_img = class_imgs.mean(axis=0)\n",
    "plt.imshow(avg_img.transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cfg = ExperimentConfig(epochs=30,\n",
    "                            lr_max=4e-3,\n",
    "                            step_size=7,\n",
    "                            weight_decay=1e-4,\n",
    "                            lr_decay=0.1,\n",
    "                            full=False,\n",
    "                            fc_layers=[2_000])\n",
    "\n",
    "net = network.OurEfficientNet(efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1),\n",
    "                        \"/scratch/ssd004/scratch/jackk/birds_data/efficientnet_birds.dict\", \n",
    "                        exp_cfg, dsets['train'].num_classes)\n",
    "net.load_model_state_dict()\n",
    "net.to(\"cuda:0\")\n",
    "net = hooks.GuidedBackprop(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer():\n",
    "    def __init__(self, means, scales):\n",
    "        self.means = np.asarray(means)[None, None, None, :]\n",
    "        self.scales = np.asarray(scales)[None, None, None, :]\n",
    "\n",
    "    def fwd(self, imgs):\n",
    "        if imgs.ndim < self.means.ndim:  # eg. if imgs is (b, 224, 224), apply red channel transformation only\n",
    "            return (imgs - self.means[..., 0]) / self.scales[...,0]\n",
    "        return (imgs - self.means) / self.scales\n",
    "\n",
    "    def rev(self, imgs):\n",
    "        if imgs.ndim < self.means.ndim:\n",
    "            return np.clip(imgs * self.scales[...,0] + self.means[...,0], 0, 1)\n",
    "        return np.clip(imgs * self.scales + self.means, 0, 1)\n",
    "        \n",
    "normer = Normalizer(means=[0.485, 0.456, 0.406], scales=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_grid = []\n",
    "titles = []\n",
    "map_names = glob.glob(\"bird_pca_split_channels/guided*\")\n",
    "for name in tqdm(map_names):\n",
    "    fname = osp.basename(name)\n",
    "    data_idx = int(fname.split(\"-\")[-1])\n",
    "    comp = fname.split(\"_\")[2]\n",
    "    image, label_idx = dsets['valid'][data_idx].values()\n",
    "    \n",
    "    image = image.numpy().transpose(1,2,0)\n",
    "    label = dsets['valid'].idx_to_class_name[label_idx]\n",
    "    titles.append(f\"{comp=} {label=}\")\n",
    "    with open(name, \"rb\") as p:\n",
    "        saliency_map = pickle.load(p)\n",
    "    img_grid.append([normer.rev(image), saliency_map])\n",
    "# what the saliency maps look like with the right PCA directions (SiLUs are suppressed), no cropping\n",
    "\n",
    "# try doing channels individually for PCA maps, see if it affects the results (DONE, though probably has a bug)\n",
    "# do visualization that combines the saliency map directly with the image (DONE, though blending could be better)\n",
    "# try doing PCA directions in HSV space (or in only H space)\n",
    "# investigate why the border artifacts occur?\n",
    "# investigate why \"squares\" appear? (only 1 particualr pixel that has a very high d_out_d_alpha for some reason\n",
    "# figure out why the saliency maps are identical in all channels, when pca maps are different along channels? (probably a bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salienced_imgs = [visualizations.combine_saliency_and_img(img, saliency, method=\"bone\", alpha=0.7) for img,saliency in img_grid]\n",
    "utils.image_grid(salienced_imgs, titles=titles, force_linear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsv_version = matplotlib.colors.rgb_to_hsv(img_grid[0][0])\n",
    "alpha = 0.4\n",
    "hsv_version[...,2] = hsv_version[...,2]*alpha + abs(img_grid[0][1][...,0])*(1-alpha)\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img_grid[0][0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(matplotlib.colors.hsv_to_rgb(hsv_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img_grid[0][0])\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(visualizations.combine_saliency_and_img(img_grid[0][0], img_grid[0][1], method=\"bone\", alpha=0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[abs(x[1][...,2] - x[1][...,0]).max() for x in img_grid]  # uh oh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipyflow)",
   "language": "python",
   "name": "ipyflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
