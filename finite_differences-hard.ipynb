{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from color_regions import ColorDatasetGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f19fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "prev_time = 0\n",
    "gamma = 0.99\n",
    "stats = {}  # tracks ewma running average\n",
    "def benchmark(point=None, profile=True, verbose=True, cuda=True): # not thread safe at all\n",
    "    global prev_time\n",
    "    if not profile:\n",
    "        return\n",
    "    if cuda:\n",
    "        torch.cuda.synchronize()\n",
    "    time_now = time.perf_counter()\n",
    "    if point is not None:\n",
    "        point = f\"{sys._getframe().f_back.f_code.co_name}-{point}\"\n",
    "        time_taken = time_now - prev_time\n",
    "        if point not in stats:\n",
    "            stats[point] = time_taken\n",
    "        stats[point] = stats[point]*gamma + time_taken*(1-gamma)\n",
    "        if verbose:\n",
    "            print(f\"took {time_taken} to reach {point}, ewma={stats[point]}\")\n",
    "    prev_time = time_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4eadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "     [transforms.ToTensor()])#,\n",
    "    #transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "batch_size = 512 # seems to be the fastest batch size\n",
    "train_indices = (0, 250_000) # size of training set\n",
    "valid_indices = (1_250_000, 1_270_000)\n",
    "test_indices = (2_260_000, 2_270_000)\n",
    "\n",
    "def color_classifier(color):  \n",
    "    if color <= 30:  # => 3 classes\n",
    "        return 0\n",
    "    if 30 < color <= 60:  # => 90/255 is 0, 90/255 is 1, 75/255 is 2\n",
    "        return 1\n",
    "    if 60 < color <= 90:\n",
    "        return 2\n",
    "    if 90 < color <= 120:\n",
    "        return 1\n",
    "    if 120 < color <= 150:\n",
    "        return 0\n",
    "    if 150 < color <= 180:\n",
    "        return 1\n",
    "    if 180 < color <= 210:\n",
    "        return 2\n",
    "    if 210 < color <= 240:\n",
    "        return 0\n",
    "    if 240 < color:\n",
    "        return 2\n",
    "critical_color_values = list(range(0,241,30))\n",
    "\n",
    "def set_loader_helper(indices):\n",
    "    data_set = ColorDatasetGenerator(color_classifier=color_classifier,\n",
    "                                    image_indices=indices,\n",
    "                                    transform=transform,\n",
    "                                    color_range=(5, 255),\n",
    "                                    noise_size=(1,9),\n",
    "                                    num_classes=3,\n",
    "                                    size=128,\n",
    "                                    radius=(128//6, 128//3))\n",
    "    loader = torch.utils.data.DataLoader(data_set, batch_size=batch_size, \n",
    "                                          shuffle=True, num_workers=6, pin_memory=True)\n",
    "    return data_set, loader\n",
    "train_set, train_loader = set_loader_helper(train_indices)\n",
    "valid_set, valid_loader = set_loader_helper(valid_indices)\n",
    "test_set, test_loader = set_loader_helper(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bbf7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"hard\" task\n",
    "color_probe = np.linspace(0, 255, 255)\n",
    "color_class = [color_classifier(x) for x in color_probe]\n",
    "plt.plot(color_probe, color_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bd6ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, conv_layers, num_classes, img_shape, path, fc_layers=[1000], groups=1):\n",
    "        super().__init__()\n",
    "        self.conv_layers1 = []  # entry into residual block \n",
    "        self.conv_layers2 = []  # https://arxiv.org/pdf/1512.03385.pdf Figure 3\n",
    "        self.batch_norms1 = []\n",
    "        self.batch_norms2 = []\n",
    "        self.is_resid = []\n",
    "        self.path = path\n",
    "        self.num_classes = num_classes\n",
    "        channels = img_shape[-1]\n",
    "        img_size = img_shape[0]\n",
    "        for l in conv_layers:  # (out_channels, kernel_size, stride) is each l\n",
    "            if l[2] > 1: # stride\n",
    "                pad_type = \"valid\"\n",
    "                img_size = (img_size-l[1])//l[2] + 1 # https://arxiv.org/pdf/1603.07285.pdf\n",
    "            else:\n",
    "                pad_type = \"same\"\n",
    "            if isinstance(l[0], float):\n",
    "                l[0] = int(l[0])\n",
    "                l[0] -= l[0] % groups # ensure divisble by groups\n",
    "            self.is_resid.append(l[2] == 1 and channels == l[0])\n",
    "            self.conv_layers1.append(nn.Conv2d(channels, l[0], l[1], stride=l[2], padding=pad_type, groups=groups))\n",
    "            channels = l[0]\n",
    "            self.final_num_logits = channels * img_size * img_size \n",
    "            self.batch_norms1.append(nn.BatchNorm2d(channels)) # cant use track_running_stats=False since\n",
    "            self.batch_norms2.append(nn.BatchNorm2d(channels)) # it causes poor performance for inference with batch size=1 (or probably with the same image repeated a bunch of times)\n",
    "            self.conv_layers2.append(nn.Conv2d(channels, channels, l[1], stride=1, padding=\"same\", groups=groups))\n",
    "        self.conv_layers1 = nn.ModuleList(self.conv_layers1)\n",
    "        self.conv_layers2 = nn.ModuleList(self.conv_layers2)\n",
    "        self.batch_norms1 = nn.ModuleList(self.batch_norms1)\n",
    "        self.batch_norms2 = nn.ModuleList(self.batch_norms2)\n",
    "\n",
    "        fully_connected = []\n",
    "        fc_layers.insert(0, self.final_num_logits)\n",
    "        fc_layers.append(num_classes)\n",
    "        for fc_prev, fc_next in zip(fc_layers, fc_layers[1:]):\n",
    "            fully_connected.append(nn.Linear(fc_prev, fc_next))\n",
    "        self.fully_connected = nn.ModuleList(fully_connected)\n",
    "\n",
    "    def forward(self, x, logits=False):\n",
    "        network_iter = zip(self.conv_layers1, self.conv_layers2, self.batch_norms1, self.batch_norms2, self.is_resid)\n",
    "        for i, (conv1, conv2, batch_norm1, batch_norm2, is_resid) in enumerate(network_iter):                \n",
    "            x_conv1 = F.relu(batch_norm1(conv1(x)))\n",
    "            x_conv2 = F.relu(batch_norm2(conv2(x_conv1)))\n",
    "            if is_resid:\n",
    "                x = x + x_conv2  # residual block\n",
    "            else:\n",
    "                x = x_conv2  # dimension increasing block            \n",
    "        x = torch.flatten(x, 1)\n",
    "        for i, fc_layer in enumerate(self.fully_connected):\n",
    "            x = fc_layer(x)\n",
    "            if i != len(self.fully_connected) - 1: # dont ReLU the last one\n",
    "                x = F.relu(x)            \n",
    "        if self.num_classes == 1 and not logits:  # always allow returning logits\n",
    "            x = torch.sigmoid(x)\n",
    "        return x    \n",
    "\n",
    "    def num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    def save_model_state_dict(self, path=None, optim=None):\n",
    "        if path is None:\n",
    "            path = self.path\n",
    "        if optim is not None:\n",
    "            save_dict = {}\n",
    "            save_dict[\"model\"] = self.state_dict()\n",
    "            save_dict[\"optim\"] = optim.state_dict()\n",
    "        else:\n",
    "            save_dict = self.state_dict()\n",
    "        torch.save(save_dict, path)\n",
    "    \n",
    "    def load_model_state_dict(self, path=None, optim=None):\n",
    "        if path is None:\n",
    "            path = self.path\n",
    "        if not os.path.exists(path):\n",
    "            return\n",
    "        load_dict = torch.load(path)\n",
    "        if \"model\" in load_dict:\n",
    "            if optim is not None:\n",
    "                optim.load_state_dict(load_dict[\"optim\"]) \n",
    "            self.load_state_dict(load_dict[\"model\"])\n",
    "        else:\n",
    "            self.load_state_dict(load_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a139b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProfileExecution(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.handles = []\n",
    "        \n",
    "        for list_name, mod_list in self.model.named_children():\n",
    "            for mod_name, mod in mod_list.named_children():\n",
    "                name = f\"{list_name}_{mod_name}\"\n",
    "                self.handles.append(mod.register_forward_hook(self.benchmark_hook(name)))\n",
    "    \n",
    "    def benchmark_hook(self, name):\n",
    "        def fn(layer, inpt, outpt):\n",
    "            benchmark(name, verbose=False)\n",
    "        return fn\n",
    "    \n",
    "    def clean_up(self):\n",
    "        for handle in self.handles:\n",
    "            handle.remove()\n",
    "\n",
    "    def forward(self, *args):\n",
    "        benchmark()\n",
    "        return self.model(*args)\n",
    "\n",
    "class AllActivations(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self._features = {}\n",
    "        self.handles = []\n",
    "\n",
    "        for list_name, mod_list in self.model.named_children():\n",
    "            for mod_name, mod in mod_list.named_children():\n",
    "                name = f\"{list_name}_{mod_name}\"\n",
    "                self.handles.append(mod.register_forward_hook(self.save_activations_hook(name)))\n",
    "\n",
    "    def save_activations_hook(self, name):\n",
    "        def fn(layer, inpt, output):\n",
    "            self._features[name] = output\n",
    "        return fn\n",
    "\n",
    "    def clean_up(self):\n",
    "        for hand in self.handles:\n",
    "            handle.remove()\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self.model(*args)\n",
    "        return self._features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067e476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(pred_logits, labels):\n",
    "    if labels.shape[1] != 1:\n",
    "        pred_probabilities = F.softmax(pred_logits, dim=1)\n",
    "        classifications = torch.argmax(pred_probabilities, dim=1)\n",
    "        labels_argmax = torch.argmax(labels, dim=1)\n",
    "    else:\n",
    "        classifications = pred_logits.int()\n",
    "        labels_argmax = labels\n",
    "    correct = (labels_argmax == classifications)\n",
    "    return correct\n",
    "\n",
    "def train(net, optimizer, loss, epochs):\n",
    "    va_losses = []\n",
    "    tr_losses = []\n",
    "    va_accuracies = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_tr_loss = 0.0\n",
    "        net.train()\n",
    "        for i, sample in tqdm(enumerate(train_loader)):\n",
    "            imgs = sample[\"image\"].to(device, non_blocking=False).float()\n",
    "            labels = sample[\"label\"].to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(imgs)\n",
    "            batch_loss = loss(outputs, labels)\n",
    "            epoch_tr_loss += batch_loss.item()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_va_loss = 0.0\n",
    "        epoch_va_correct = 0\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, sample in enumerate(valid_loader):\n",
    "                imgs = sample[\"image\"].to(device).float()\n",
    "                labels = sample[\"label\"].to(device).float()\n",
    "                outputs = net(imgs)\n",
    "                epoch_va_loss += loss(outputs, labels).item()\n",
    "                epoch_va_correct += correct(outputs, labels).sum().item()\n",
    "        epoch_va_accuracy = epoch_va_correct/(valid_indices[1] - valid_indices[0])\n",
    "        print(f'Epoch {epoch + 1}: va_loss: {epoch_va_loss}, va_accuracy: {epoch_va_accuracy}, tr_loss: {epoch_tr_loss}')\n",
    "        if not va_losses or epoch_va_loss < min(va_losses):\n",
    "            net.save_model_state_dict(optim=optimizer)\n",
    "        va_losses.append(epoch_va_loss)\n",
    "        tr_losses.append(epoch_tr_loss)\n",
    "        va_accuracies.append(epoch_va_accuracy)\n",
    "    return va_losses, va_accuracies, tr_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f349501",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_net = ResNet([[16, 3, 2],  # num_channels (input and output), kernel_size, stride\n",
    "                    [32, 3, 2],\n",
    "                    [64, 3, 2]], 3, [128, 128, 1], \n",
    "                   \"small_net_noise_hard_grey.dict\", fc_layers=[32]).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "small_optim = torch.optim.Adam(small_net.parameters())\n",
    "print(small_net.num_params())\n",
    "small_net.load_model_state_dict(optim=small_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e6e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train(small_net, small_optim, loss_func, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af8ffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_net = ResNet([[2, 3, 4],  # num_channels (input and output), kernel_size, stride\n",
    "                   #[4, 3, 2],\n",
    "                   [6, 3, 4]], 3, [128, 128, 1], \n",
    "                   \"tiny_net_noise_hard_grey.dict\", fc_layers=[]).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "tiny_optim = torch.optim.Adam(tiny_net.parameters())\n",
    "print(tiny_net.num_params())\n",
    "tiny_net.load_model_state_dict(optim=tiny_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670bd759",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train(tiny_net, tiny_optim, loss_func, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82dfbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def rate_distribution(net, loader, dataset, buckets=100):\n",
    "    net.eval()\n",
    "    total = np.zeros((buckets))\n",
    "    num_correct = np.zeros((buckets))\n",
    "    num_possible_colors = dataset.color_range[1] - dataset.color_range[0]\n",
    "    for sample in tqdm(loader):\n",
    "        imgs = sample[\"image\"].to(device).float()\n",
    "        labels = sample[\"label\"].to(device).float()\n",
    "        actual_colors = sample[\"color\"]\n",
    "        color_indices = (buckets * (actual_colors - dataset.color_range[0]) / num_possible_colors).int().numpy()\n",
    "        outputs = net(imgs)\n",
    "        correct_preds = correct(outputs, labels).cpu().numpy()\n",
    "        for i, color_idx in enumerate(color_indices):\n",
    "            total[color_idx] += 1  \n",
    "            num_correct[color_idx] += correct_preds[i]\n",
    "    return num_correct, total\n",
    "_num_correct, _total = rate_distribution(small_net, valid_loader, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d70fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(num_correct, total, dataset, critical_values=[150-0.5], buckets=100):\n",
    "    num_wrong = total - num_correct\n",
    "    width = 0.4\n",
    "    labels = [int(x) for i, x in enumerate(np.linspace(*dataset.color_range, buckets))]\n",
    "    plt.bar(labels, num_correct, width, label=\"correct amount\")\n",
    "    plt.bar(labels, num_wrong, width, bottom=num_correct, label=\"wrong amount\")\n",
    "    plt.vlines(critical_values, 5, -5, \n",
    "               linewidth=0.8, colors=\"r\", label=\"decision boundary\",\n",
    "               linestyles=\"dashed\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Color value\")\n",
    "    plt.show()\n",
    "make_graph(_num_correct, _total, valid_set,   # with .eval(), looks good\n",
    "           critical_values=[x-0.5 for x in critical_color_values]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_net.eval() # very important!\n",
    "with torch.no_grad():\n",
    "    test_index = 987_652  # results seem pretty dependent on image, especially in small-color regimes\n",
    "    counterfactual_color_values = np.linspace(0, 255, 255) # probably because bad batchnorm estimates\n",
    "    responses = []\n",
    "    for color in counterfactual_color_values:\n",
    "        np.random.seed(test_index)\n",
    "        generated_img, lbl, __ = valid_set.generate_one(set_color=color)\n",
    "        generated_img = np.expand_dims(generated_img, 0).transpose(0, 3, 1, 2)\n",
    "        generated_img = torch.tensor(generated_img).to(device).float()\n",
    "        response = small_net(torch.tensor(generated_img).to(device).float(), logits=True).cpu().numpy()\n",
    "        responses.append(np.squeeze(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e16a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_responses(resp, colors, title):\n",
    "    resp = np.arcsinh(np.array(resp))\n",
    "    for output_logit in range(resp.shape[1]):\n",
    "        plt.plot(colors, resp[:, output_logit], label=f\"class {output_logit}\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Color value\")\n",
    "    plt.ylabel(\"Network output logit\")\n",
    "    plt.title(title)\n",
    "    plt.vlines(critical_color_values, np.min(resp), np.max(resp), linewidth=0.8,\n",
    "               colors=\"r\", label=\"decision boundary\",\n",
    "               linestyles=\"dashed\") # with .eval() works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17f7269",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_responses(responses, counterfactual_color_values, \"\")\n",
    "#i ncreas esize of training set and roughness (mean squared grad across img or pixelvalues) should go down, meght be overfitting (double deep descent)\n",
    "#  -> could be causing decrease in quality of FD maps\n",
    "# to be expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def finite_differences(model, dataset, target_class, stacked_img, locations, channel, unfairness, values_prior):\n",
    "    num_iters = 20 # sample 20 values evenly spaced\n",
    "    cuda_stacked_img = torch.tensor(stacked_img).to(device)\n",
    "    if dataset.num_classes == 2:\n",
    "        class_multiplier = 1 if target_class == 1 else -1 \n",
    "        baseline_activations = class_multiplier*model(cuda_stacked_img, logits=True)\n",
    "    else:\n",
    "        baseline_activations = model(cuda_stacked_img)[:, target_class]\n",
    "    largest_slope = np.zeros(stacked_img.shape[0])  # directional finite difference?\n",
    "    slices = np.index_exp[np.arange(stacked_img.shape[0]), channel, locations[:, 0], locations[:, 1]]\n",
    "    if values_prior is None:\n",
    "        values_prior = np.linspace(5, 250, stacked_img.shape[0]) # uniform distribution assumption\n",
    "    elif isinstance(values_prior, list):\n",
    "        values_prior = np.expand_dims(np.asarray(values_prior), 1)\n",
    "    num_loops = 1 if unfairness == \"very unfair\" else len(values_prior)\n",
    "    for i in range(num_loops):\n",
    "        shift_img = stacked_img.copy()\n",
    "        # shifting method\n",
    "        if unfairness in [\"fair\", \"unfair\"]:\n",
    "            shift_img[slices] = values_prior[i]+0.01  # add tiny offset to \"guarantee\" non-zero shift\n",
    "        elif unfairness in [\"very unfair\"]:\n",
    "            critical_value_dists = shift_img[slices] - values_prior\n",
    "            closest = np.argmin(abs(critical_value_dists), axis=0) # find closest class boundary\n",
    "            shift_img[slices] = 0.01 + np.choose(closest, values_prior) - 10*np.sign(np.choose(closest, critical_value_dists))\n",
    "        \n",
    "        actual_diffs = shift_img[slices] - stacked_img[slices]  \n",
    "        img_norm = torch.tensor(shift_img).to(device) # best is no normalization anyway\n",
    "        if dataset.num_classes == 2:\n",
    "            activations = class_multiplier*model(img_norm, logits=True)\n",
    "        else:\n",
    "            activations = model(img_norm)[:, target_class]\n",
    "        activation_diff = (activations - baseline_activations).cpu().numpy().squeeze()\n",
    "        finite_difference = np.clip(activation_diff/actual_diffs, -30, 30) # take absolute slope\n",
    "        largest_slope = np.where(abs(finite_difference) > abs(largest_slope), finite_difference, largest_slope)\n",
    "    return largest_slope      \n",
    "\n",
    "def finite_differences_map(model, dataset, target_class, img, unfairness=\"fair\", values_prior=None):\n",
    "    # generate a saliency map using finite differences method (iterate over colors)\n",
    "    model.eval()\n",
    "    batch_size = 32  # check batch_size num pixel positions in parallel\n",
    "    im_size = dataset.size\n",
    "    #img = img.astype(np.float32)/255. # normalization handled later\n",
    "    values_x = np.repeat(np.arange(im_size), im_size)\n",
    "    values_y = np.tile(np.arange(im_size), im_size)\n",
    "    indices = np.stack((values_x, values_y), axis=1)\n",
    "    stacked_img = np.repeat(np.expand_dims(img, 0), batch_size, axis=0)\n",
    "    stacked_img = np.transpose(stacked_img, (0, 3, 1, 2)).astype(np.float32) # NCHW format\n",
    "    img_heat_map = np.zeros_like(img)\n",
    "    for channel in range(dataset.channels):\n",
    "        for k in tqdm(range(0, im_size*im_size, batch_size)):\n",
    "            actual_batch_size = min(batch_size, im_size*im_size-k+batch_size)\n",
    "            locations = indices[k:k+batch_size]\n",
    "            largest_slopes = finite_differences(model, dataset, target_class, stacked_img, locations, channel, unfairness, values_prior)\n",
    "            img_heat_map[locations[:,0], locations[:,1], channel] = largest_slopes\n",
    "    return img_heat_map#.sum(axis=2)  # linear approximation aggregation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d84dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(500_001)\n",
    "explain_img, label, *_ = valid_set.generate_one()\n",
    "heat_map = finite_differences_map(small_net, valid_set, label.argmax(), explain_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d95ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_grid_figure(grid, titles=None, colorbar=True, cmap=None, transpose=False, hspace=-0.4):      \n",
    "    np_grid = np.array(grid).squeeze()\n",
    "    if len(np_grid.shape) != 4:\n",
    "        np_grid = np.expand_dims(np_grid, 0)\n",
    "    if transpose:\n",
    "        np_grid = np_grid.T\n",
    "        \n",
    "    if cmap is None:\n",
    "        cmap = \"bwr\"\n",
    "    nrows, ncols = np_grid.shape[0], np_grid.shape[1]\n",
    "    im_size = np_grid.shape[2]\n",
    "    fig = plt.figure(figsize=(4/128*im_size*ncols, 5/128*im_size*nrows))\n",
    "    gridspec = fig.add_gridspec(nrows, ncols, hspace=hspace)\n",
    "    axes = gridspec.subplots(sharex=\"col\", sharey=\"row\")\n",
    "    if len(axes.shape) == 1:\n",
    "        axes = np.expand_dims(axes, 0)\n",
    "    print(np_grid.shape, nrows, ncols)\n",
    "    for i, row in enumerate(np_grid):\n",
    "        for j, img in enumerate(row):\n",
    "            if j == 0: # assume explain_img is the first thing\n",
    "                im = axes[i,j].imshow(img, cmap=\"gray\")\n",
    "            else:\n",
    "                img_max = np.max(abs(img))\n",
    "                if cmap != \"gray\":\n",
    "                    im = axes[i,j].imshow(img, cmap=cmap, interpolation=\"nearest\", vmax=img_max, vmin=-img_max)\n",
    "                else:\n",
    "                    axes[i,j].imshow(img, cmap=cmap)\n",
    "                if colorbar:\n",
    "                    plt.colorbar(im, pad=0, fraction=0.048)\n",
    "            if titles and i == 0:\n",
    "                axes[i,j].set_title(titles[j])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725c5459",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_grid_figure([explain_img, heat_map], titles=[\"Image\", \"FD Map\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9c62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = [20_000, 25_000, 30_000, 600_000, 600_001]\n",
    "heat_maps = []\n",
    "explain_imgs = []\n",
    "for i, image_id in enumerate(image_ids):\n",
    "    np.random.seed(image_id)\n",
    "    explain_img_i, target_i, *__ = valid_set.generate_one()\n",
    "    heat_map_i = finite_differences_map(small_net, valid_set, target_i.argmax(), explain_img_i)\n",
    "    heat_maps.append(heat_map_i)\n",
    "    explain_imgs.append(explain_img_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3081340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_grid_figure([explain_imgs, heat_maps], transpose=True, colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcda00b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_pixels_response(num_pixels, one_class=True):\n",
    "    small_net.eval() # very important!\n",
    "    test_index = 987_652  # results seem pretty dependent on image, especially in small-color regimes\n",
    "    np.random.seed(test_index)\n",
    "    generated_img, lbl, color, size, pos  = valid_set.generate_one()\n",
    "    generated_img = np.expand_dims(generated_img, 0).transpose(0, 3, 1, 2)\n",
    "    \n",
    "    np.random.seed(int(time.time()/np.pi))\n",
    "    selected_pixels = np.random.randint(0, valid_set.size, (num_pixels, 2))\n",
    "    num_inside = 0\n",
    "    for p in selected_pixels:\n",
    "        if np.linalg.norm(p-pos) < size:\n",
    "            num_inside += 1\n",
    "    print(f\"Percent of random inside circle: {num_inside/num_pixels*100.}\")\n",
    "    \n",
    "    counterfactual_color_values = np.linspace(0, 255, 255) # probably because bad batchnorm estimates\n",
    "    responses = []\n",
    "    with torch.no_grad():\n",
    "        for color in counterfactual_color_values:\n",
    "            generated_img[0, 0, selected_pixels[:,0], selected_pixels[:,1]] = color\n",
    "            tensor_img = torch.tensor(generated_img).to(device).float()\n",
    "            response = small_net(torch.tensor(generated_img).to(device).float(), logits=True).cpu().numpy()\n",
    "            if one_class:\n",
    "                responses.append(np.expand_dims(np.squeeze(response[:,lbl.argmax()]), 0))\n",
    "            else:\n",
    "                responses.append(np.squeeze(response))\n",
    "    plot_responses(responses, counterfactual_color_values, \"Randomly selected pixels\")\n",
    "    \n",
    "def circle_pixels_response(num_pixels, one_class=True):\n",
    "    small_net.eval() # very important!\n",
    "    test_index = 987_652 \n",
    "    np.random.seed(test_index)  # generate image\n",
    "    generated_img, lbl, color, size, pos = valid_set.generate_one()\n",
    "    generated_img = np.expand_dims(generated_img, 0).transpose(0, 3, 1, 2)\n",
    "    \n",
    "    np.random.seed(int(time.time()/np.pi))\n",
    "    angle = np.random.uniform(0,2*np.pi, num_pixels)\n",
    "    radii = np.random.uniform(0, size, num_pixels)\n",
    "    selected_pixels = np.zeros((num_pixels, 2))\n",
    "    selected_pixels[:,0] = pos[0][0] + np.cos(angle)*radii\n",
    "    selected_pixels[:,1] = pos[0][1] + np.sin(angle)*radii\n",
    "    selected_pixels = np.round(selected_pixels).astype(np.int64)\n",
    "    \n",
    "    counterfactual_color_values = np.linspace(0, 255, 255) # probably because bad batchnorm estimates\n",
    "    responses = []\n",
    "    with torch.no_grad():\n",
    "        for color in counterfactual_color_values:\n",
    "            generated_img[0, 0, selected_pixels[:,0], selected_pixels[:,1]] = color\n",
    "            tensor_img = torch.tensor(generated_img).to(device).float()\n",
    "            response = small_net(torch.tensor(generated_img).to(device).float(), logits=True).cpu().numpy()\n",
    "            if one_class:\n",
    "                responses.append(np.expand_dims(np.squeeze(response[:,lbl.argmax()]), 0))\n",
    "            else:\n",
    "                responses.append(np.squeeze(response))\n",
    "    plot_responses(responses, counterfactual_color_values, \"Pixels inside circle\")\n",
    "    \n",
    "def both_pixels_response(num_pixels, one_class):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    circle_pixels_response(num_pixels, one_class=one_class)\n",
    "    plt.subplot(1,2,2)\n",
    "    random_pixels_response(num_pixels, one_class=one_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4e5772",
   "metadata": {},
   "outputs": [],
   "source": [
    "both_pixels_response(1, True)\n",
    "# deeper layers = more ability to do step function\n",
    "# conv layer = not be able to do step function\n",
    "# different pxiles have different jobs, display graph of a bunch of pixels at once\n",
    "# if theory is correct, we would have a bunch of clutter, but the average would converge to the \n",
    "# decision boundary\n",
    "# mask out everything but 1 pixel, and train on that too\n",
    "# vector value might be ok\n",
    "# want it to be beyond edge detection( add color detection)\n",
    "\n",
    "# interesting result is that even if you force the network to do non-linear decesion boundary\n",
    "# seemingly, it doesnt work on a pixel-by-pixel basis. Could try to make a dataset where\n",
    "# that really does matter but at that point eh. \n",
    "# Want to get a better understanding of how this actually happens. Like what is the network\n",
    "# learning, or what can we hypothesize about how nets in general function. Seems to be taking\n",
    "# the \"average value\" somehow and using that for its decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f8060b",
   "metadata": {},
   "source": [
    "What if we run the same experiment, but cheat with a prior on pixel values that we know *should* be informative to the output logit, namely values closest to the decision boundary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d72b360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, conv_layers, img_shape, path, fc_layers=[], embed_size=128):\n",
    "        super().__init__()\n",
    "        self.path = path  # for saving and loading\n",
    "        \n",
    "        enc_layers1 = []  \n",
    "        enc_layers2 = []\n",
    "        enc_maxpools = []\n",
    "        enc_batchnorms1 = []\n",
    "        enc_batchnorms2 = []\n",
    "        \n",
    "        dec_layers1 = []  \n",
    "        dec_layers2 = []\n",
    "        dec_maxpools = []  # should be maxunpools\n",
    "        dec_batchnorms1 = []\n",
    "        dec_batchnorms2 = []\n",
    "        \n",
    "        is_resid = []        \n",
    "        channels = img_shape[-1]\n",
    "        img_size = img_shape[0]\n",
    "        for l in conv_layers:  # (out_channels, kernel_size, stride) is each l\n",
    "            is_resid.append(l[2] == 1 and channels == l[0])\n",
    "            \n",
    "            if l[2] > 1:\n",
    "                img_size = (img_size-l[2])//l[2]+1\n",
    "                print(img_size)\n",
    "                enc_maxpools.append(nn.MaxPool2d(l[2], return_indices=True))\n",
    "                dec_maxpools.insert(0, nn.MaxUnpool2d(l[2]))\n",
    "            else:\n",
    "                enc_maxpools.append(None)\n",
    "                dec_maxpools.insert(0, None)\n",
    "                \n",
    "            enc_layers1.append(nn.Conv2d(channels, l[0], l[1], padding=\"same\"))\n",
    "            enc_batchnorms1.append(nn.BatchNorm2d(l[0])) \n",
    "\n",
    "            dec_layers2.insert(0,nn.ConvTranspose2d(l[0], channels, l[1], padding=l[1]//2))\n",
    "            dec_batchnorms2.insert(0,nn.BatchNorm2d(channels)) \n",
    "\n",
    "            channels = l[0]\n",
    "            \n",
    "            enc_layers2.append(nn.Conv2d(channels, channels, l[1], padding=\"same\"))\n",
    "            enc_batchnorms2.append(nn.BatchNorm2d(channels))\n",
    "\n",
    "            dec_layers1.insert(0,nn.ConvTranspose2d(channels, channels, l[1], padding=l[1]//2))\n",
    "            dec_batchnorms1.insert(0,nn.BatchNorm2d(channels)) \n",
    "            \n",
    "        self.final_flat_shape = channels*img_size*img_size\n",
    "        self.final_img_shape = [channels, img_size, img_size]\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        enc_fully_connected = []\n",
    "        dec_fully_connected = []\n",
    "        extended_fc_layers = fc_layers.copy()\n",
    "        extended_fc_layers.insert(0, self.final_flat_shape)\n",
    "        extended_fc_layers.append(embed_size)\n",
    "        for fc_prev, fc_next in zip(extended_fc_layers, extended_fc_layers[1:]):\n",
    "            enc_fully_connected.append(nn.Linear(fc_prev, fc_next))\n",
    "            dec_fully_connected.insert(0,nn.Linear(fc_next, fc_prev))\n",
    "            \n",
    "        self.enc_layers1 = nn.ModuleList(enc_layers1)\n",
    "        self.enc_layers2 = nn.ModuleList(enc_layers2)\n",
    "        self.enc_maxpools = nn.ModuleList(enc_maxpools)\n",
    "        self.enc_batchnorms1 = nn.ModuleList(enc_batchnorms1)\n",
    "        self.enc_batchnorms2 = nn.ModuleList(enc_batchnorms2)\n",
    "        self.enc_fully_connected = nn.ModuleList(enc_fully_connected)\n",
    "        \n",
    "        self.dec_layers1 = nn.ModuleList(dec_layers1)\n",
    "        self.dec_layers2 = nn.ModuleList(dec_layers2)\n",
    "        self.dec_maxpools = nn.ModuleList(dec_maxpools)\n",
    "        self.dec_batchnorms1 = nn.ModuleList(dec_batchnorms1)\n",
    "        self.dec_batchnorms2 = nn.ModuleList(dec_batchnorms2)\n",
    "        self.dec_fully_connected = nn.ModuleList(dec_fully_connected)\n",
    "\n",
    "        self.enc_is_resid = is_resid\n",
    "        self.dec_is_resid = reversed(is_resid)\n",
    "        \n",
    "        iter_names = [\"layers1\", \"layers2\", \"maxpools\", \"batchnorms1\", \"batchnorms2\", \"is_resid\"]\n",
    "        self.enc_iter = list(zip(*[getattr(self, \"enc_\"+name) for name in iter_names]))\n",
    "        self.dec_iter = list(zip(*[getattr(self, \"dec_\"+name) for name in iter_names]))\n",
    "        \n",
    "    def net_block(self, x, indices, block_name):\n",
    "        conv_iter = getattr(self, block_name + \"_iter\")\n",
    "        fc_iter = getattr(self, block_name + \"_fully_connected\")\n",
    "\n",
    "        if block_name == \"dec\":\n",
    "            for i, fc_layer in enumerate(fc_iter):\n",
    "                x = F.relu(fc_layer(x))\n",
    "            x = torch.reshape(x, (-1,*self.final_img_shape))\n",
    "#         print(x.shape, block_name, \"pre-conv\", type(conv_iter), next(conv_iter))\n",
    "        #print([t.shape if t is not None else t for t in indices])\n",
    "        for i, (conv1, conv2, maxpool, batch_norm1, batch_norm2, is_resid) in enumerate(conv_iter):\n",
    "            x_conv1 = F.relu(batch_norm1(conv1(x)))\n",
    "#             if block_name == \"dec\":\n",
    "#                 print(\"DEC\", i, type(indices[i]), maxpool)\n",
    "            if maxpool is not None:\n",
    "                if block_name == \"enc\":\n",
    "                    x_conv1, indices_layer = maxpool(x_conv1)\n",
    "                    #print(x.shape, conv1, indices_layer.shape)\n",
    "                    indices.insert(0,indices_layer)\n",
    "                else:\n",
    "                    #print(x.shape, maxpool, i, type(indices[i-1]), type(indices[i]), type(indices[i+1]))\n",
    "                    x_conv1 = maxpool(x_conv1, indices[i])\n",
    "            else:\n",
    "                if block_name == \"enc\":\n",
    "                    indices.insert(0,None)\n",
    "                    #print(x.shape, conv1)\n",
    "                    \n",
    "            if block_name == \"dec\" and i == len(conv_iter)-1:\n",
    "                x_conv2 = F.relu(conv2(x_conv1))\n",
    "            else:\n",
    "                x_conv2 = F.relu(batch_norm2(conv2(x_conv1)))\n",
    "                \n",
    "            if is_resid:\n",
    "                x = x + x_conv2  # residual block\n",
    "            else:\n",
    "                x = x_conv2  # dimension increasing block\n",
    "          #  print(x.shape, block_name, \"conv_mid\", i)\n",
    "#         print(x.shape, block_name, \"post_conv\")\n",
    "        if block_name == \"enc\":\n",
    "            x = torch.flatten(x, 1)\n",
    "            for i, fc_layer in enumerate(fc_iter):\n",
    "                x = fc_layer(x)\n",
    "                if i != len(fc_iter) - 1:\n",
    "                    x = F.relu(x)\n",
    "        return x    \n",
    "    \n",
    "    def encode(self, x, indices): # due to the symmetry of decoding/encoding, we can do this nicely\n",
    "        return self.net_block(x, indices, \"enc\")\n",
    "    \n",
    "    def decode(self, z, indices): # => this implementation means the output is ReLU'd\n",
    "        return self.net_block(z, indices, \"dec\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        save_indices = []\n",
    "        #print(save_indices, x.shape, \"pre anything\")\n",
    "        z = self.encode(x, save_indices)  # write to save_indices\n",
    "        return self.decode(z, save_indices) # read from save_indices\n",
    "    \n",
    "    def num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    def save_model_state_dict(self, path=None, optim=None):\n",
    "        if path is None:\n",
    "            path = self.path\n",
    "        if optim is not None:\n",
    "            save_dict = {}\n",
    "            save_dict[\"model\"] = self.state_dict()\n",
    "            save_dict[\"optim\"] = optim.state_dict()\n",
    "        else:\n",
    "            save_dict = self.state_dict()\n",
    "        torch.save(save_dict, path)\n",
    "    \n",
    "    def load_model_state_dict(self, path=None, optim=None):\n",
    "        if path is None:\n",
    "            path = self.path\n",
    "        if not os.path.exists(path):\n",
    "            return\n",
    "        load_dict = torch.load(path)\n",
    "        if \"model\" in load_dict:\n",
    "            if optim is not None:\n",
    "                optim.load_state_dict(load_dict[\"optim\"]) \n",
    "            self.load_state_dict(load_dict[\"model\"])\n",
    "        else:\n",
    "            self.load_state_dict(load_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85239a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoenc_train(net, optimizer, loss, epochs):\n",
    "    va_losses = []\n",
    "    tr_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_tr_loss = 0.0\n",
    "        net.train()\n",
    "        for i, sample in tqdm(enumerate(train_loader)):\n",
    "            imgs = sample[\"image\"].to(device, non_blocking=False).float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(imgs)  # should be close to the image\n",
    "            batch_loss = loss(outputs, imgs)  # L2 loss of reconstruction\n",
    "            epoch_tr_loss += batch_loss.item()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_va_loss = 0.0\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, sample in enumerate(valid_loader):\n",
    "                imgs = sample[\"image\"].to(device).float()\n",
    "                outputs = net(imgs)\n",
    "                epoch_va_loss += loss(outputs, imgs).item()\n",
    "        print(f'Epoch {epoch + 1}: va_loss: {epoch_va_loss}, tr_loss: {epoch_tr_loss}')\n",
    "        if not va_losses or epoch_va_loss < min(va_losses):\n",
    "            net.save_model_state_dict(optim=optimizer)\n",
    "        va_losses.append(epoch_va_loss)\n",
    "        tr_losses.append(epoch_tr_loss)\n",
    "    return va_losses, tr_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a37661",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_enc = AutoEncoder([[16, 7, 1],  # num_channels (input and output), kernel_size, max_pool kernel\n",
    "                        [32, 3, 2],  # make sure to change the batch size before working with this\n",
    "                        [32, 3, 2],\n",
    "                        [64, 3, 2],\n",
    "                        [64, 3, 2],\n",
    "                        [128, 3, 2],\n",
    "                        [128, 3, 1]], [128, 128, 1], \"auto_enc_greyscale_no_norm.dict\",\n",
    "                        fc_layers=[], embed_size=256).to(device)\n",
    "auto_enc_loss = nn.MSELoss()\n",
    "auto_enc_optim = torch.optim.Adam(auto_enc.parameters())\n",
    "print(auto_enc.num_params())\n",
    "auto_enc.load_model_state_dict(optim=auto_enc_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136c7b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = autoenc_train(auto_enc, auto_enc_optim, auto_enc_loss, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca4c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple sanity check\n",
    "auto_enc.eval()\n",
    "generated_img = valid_set.generate_one()[0]\n",
    "tensor_img = torch.tensor(np.expand_dims(generated_img,0).transpose(0,3,1,2)).to(device).float()\n",
    "reconstruction = np.expand_dims(auto_enc(tensor_img).detach().cpu().numpy().squeeze(),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reconstruction.mean(), generated_img.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2887066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_grid_figure([generated_img.squeeze(), reconstruction.squeeze()], colorbar=False, cmap=\"gray\", transpose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a22fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_indicator(outpts, inputs):  # assume values are equally spaced\n",
    "    outpt_diffs = outpts[:-1] - outpts[1:]\n",
    "    input_diffs = inputs[:-1] - inputs[1:]\n",
    "    deriv = outpt_diffs/input_diffs\n",
    "    avg_abs_grad = np.where(abs(deriv) < 1e-3, 1, 0).mean()  # for linear funcs, output_range = m*x_range\n",
    "    grad_range = np.max(abs(deriv)) - np.min(abs(deriv))\n",
    "    output_range = np.max(outpts) - np.min(outpts) # avg_square_grad = m**2/x_range\n",
    "    return output_range, avg_abs_grad, grad_range/output_range\n",
    "    \n",
    "def random_polynomial(num_pts, degree, pts_range, inpts):\n",
    "    input_pts = np.random.uniform(inpts.min(), inpts.max(), num_pts)\n",
    "    input_pts = np.concatenate((input_pts, [0,255]))\n",
    "    output_pts = np.random.uniform(*pts_range, num_pts)\n",
    "    output_pts = np.concatenate((output_pts, np.random.uniform(*pts_range, 2)))\n",
    "    return np.poly1d(np.polyfit(input_pts, output_pts, degree))(inpts)\n",
    "\n",
    "def random_indicator(jumps_range, inputs):\n",
    "    num_jumps = np.random.randint(*jumps_range)\n",
    "    jumps = np.sort(np.random.uniform(inputs.min(), inputs.max(), num_jumps))\n",
    "    noise = np.random.normal(loc=0, scale=0.01, size=(len(inputs)))\n",
    "    return np.digitize(inputs, jumps) % 2 + noise\n",
    "\n",
    "input_range = np.linspace(0,255,255)\n",
    "plt.subplot(1,2,1)\n",
    "poly_results = random_polynomial(4, 3, (-4, 4), input_range)\n",
    "plt.plot(input_range, poly_results)\n",
    "plt.subplot(1,2,2)\n",
    "indic_results = random_indicator((1,5), input_range)\n",
    "plt.plot(input_range, indic_results)\n",
    "print(is_indicator(poly_results, input_range))\n",
    "print(is_indicator(indic_results, input_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70b9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_image(encoder_net, img, dataset, target_class, sample_size=1024):\n",
    "    sample = []\n",
    "    model.eval()\n",
    "    while len(sample) != sample_size:\n",
    "        sampled_img, sampled_label, *_ =  dataset.generate_one()\n",
    "        if sampled_label.argmax() != target_class:  # only consider sample from other classes\n",
    "            sample.append(sampled_img)\n",
    "    im_size = img.shape[0]\n",
    "    sample = np.array(sample).transpose(0,3,1,2)\n",
    "    sample_tensor = torch.tensor(sample).to(device).float()\n",
    "    print(sample_tensor.shape)\n",
    "    batch_size = 32\n",
    "    encoding_vectors = []\n",
    "    encoding_indices = []\n",
    "    for k in range(0, sample_size, batch_size):\n",
    "        selected = sample_tensor[k:min(k+batch_size, sample_size)]\n",
    "        encodings, indices = encoder_net.encode(selected)\n",
    "        encoding_vectors.append(encodings.detach().cpu().numpy())\n",
    "        encoding_indices.append(indices.detach().cpu().numpy())\n",
    "    img_tensor = torch.tensor(img.transpose(2,0,1)).unsqueeze(0).to(device)\n",
    "    img_enc, img_indices = encoder_net.encode(img_tensor)\n",
    "    img_enc = img_enc.detach().cpu().numpy()\n",
    "    encoding_vectors = np.array(encoding_vectors)\n",
    "    diffs = np.linalg.norm(encoding_vectors - img_enc, axis=1)\n",
    "    min_diff = np.argmin(diffs)\n",
    "    closest_img = sample[min_diff]\n",
    "    # now, do a binary search on alpha values for the closest value that is the other class\n",
    "    # interpolation on \"indices\" can be done with ???\n",
    "    # convolutional VAE \n",
    "    # paperswithcode\n",
    "    alpha_bin_search(img, closest_img, classifier, auto_encoder, img_enc, closest_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86a46e",
   "metadata": {},
   "source": [
    "# Model Optimization Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a9a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_net.eval()\n",
    "generated_img = torch.tensor(valid_set.generate_one()[0].transpose(2,0,1)).unsqueeze(0).to(device).float()\n",
    "profile_model = ProfileExecution(small_net)\n",
    "for _ in tqdm(range(1000)):\n",
    "    profile_model.forward(generated_img)\n",
    "profile_model.clean_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd852210",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum(stats.values())  # --> gave 3x speed! (Fast and Accurate Model scaling?)\n",
    "for k,v in sorted(stats.items(), key=lambda x: x[0]):    # --> the 3x speedup caused underfitting though, so switched to 2x\n",
    "    print(k,(100.*v/total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
